{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Riverbed Tools for content datamining and NLP at scale. Motivation Given a set of content including video, image and/or text content in human language, or code, we would like to: - create interesting features for the content, including augmenting the content. - cluster, explore, search and visualize the content. - label and create classifiers for the content. - search, filter, store and share the content to user and to other AI models The content may be domain specific and may change over time. If a change is significant, we would like to be notified of the changes and/or automatically re-run some or all of the above.","title":"Home"},{"location":"#welcome-to-riverbed","text":"Tools for content datamining and NLP at scale.","title":"Welcome to Riverbed"},{"location":"#motivation","text":"Given a set of content including video, image and/or text content in human language, or code, we would like to: - create interesting features for the content, including augmenting the content. - cluster, explore, search and visualize the content. - label and create classifiers for the content. - search, filter, store and share the content to user and to other AI models The content may be domain specific and may change over time. If a change is significant, we would like to be notified of the changes and/or automatically re-run some or all of the above.","title":"Motivation"},{"location":"about/","text":"About Riverbed This is more detail for riverbed.","title":"About"},{"location":"about/#about-riverbed","text":"This is more detail for riverbed.","title":"About Riverbed"},{"location":"reference/SUMMARY/","text":"banned_words char_manager cjk filtering flagged_words kenlm_manager langid_manager pdf_and_ocr pii_manager searcher_indexer simhash stopwords translation utils","title":"SUMMARY"},{"location":"reference/banned_words/","text":"","title":"banned_words"},{"location":"reference/char_manager/","text":"","title":"char_manager"},{"location":"reference/cjk/","text":"","title":"cjk"},{"location":"reference/filtering/","text":"Copyright, 2021-2022 Ontocord, LLC, All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"filtering"},{"location":"reference/flagged_words/","text":"","title":"flagged_words"},{"location":"reference/kenlm_manager/","text":"Copyright, 2021-2022 Ontocord, LLC, and other authors of Muliwai, All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. KenlmModel Source code in src/kenlm_manager.py class KenlmModel: digit_re: re.Pattern = re.compile(r\"\\d\") unicode_punct: Dict[str, str] = { \"\uff0c\": \",\", \"\u3002\": \".\", \"\u3001\": \",\", \"\u201e\": '\"', \"\u201d\": '\"', \"\u201c\": '\"', \"\u00ab\": '\"', \"\u00bb\": '\"', \"\uff11\": '\"', \"\u300d\": '\"', \"\u300c\": '\"', \"\u300a\": '\"', \"\u300b\": '\"', \"\u00b4\": \"'\", \"\u2236\": \":\", \"\uff1a\": \":\", \"\uff1f\": \"?\", \"\uff01\": \"!\", \"\uff08\": \"(\", \"\uff09\": \")\", \"\uff1b\": \";\", \"\u2013\": \"-\", \"\u2014\": \" - \", \"\uff0e\": \". \", \"\uff5e\": \"~\", \"\u2019\": \"'\", \"\u2026\": \"...\", \"\u2501\": \"-\", \"\u3008\": \"<\", \"\u3009\": \">\", \"\u3010\": \"[\", \"\u3011\": \"]\", \"\uff05\": \"%\", \"\u25ba\": \"-\", } unicode_punct_re = re.compile(f\"[{''.join(unicode_punct.keys())}]\") non_printing_chars_re = re.compile( f\"[{''.join(map(chr, list(range(0, 32)) + list(range(127, 160))))}]\" ) kenlm_model_dir = None sentence_piece_model_dir = None # TODO: we are not doing the sacremoses tokenizer to get put spaces between escaped chars # but consider whether we should do this for the ccnet models # https://github.com/facebookresearch/cc_net/blob/bda555bd1cf1ee2e0b925363e62a61cd46c8b60d/cc_net/tokenizer.py#L23 # does it make a difference? def __init__( self, model_name: str=None, language: str=None, lowercase: bool = False, remove_accents: bool = False, normalize_numbers: bool = True, punctuation: int = 1, do_normalize_spacing_for_tok: bool = False, tokenizer=None, model_path: str=None, ): print (model_name) self.model_name = model_name if model_path is not None: self.model = kenlm.Model(model_path) elif \"riverbed\" in model_name: self.model = kenlm.Model(os.path.join(self.model_name, f\"arpa.bin\")) tokenizer = mt5_tokenizer else: self.model = kenlm.Model(os.path.join(self.model_name, f\"{language}.arpa.bin\")) if tokenizer is None: self.tokenizer = SentencePiece(os.path.join(self.model_name, f\"{language}.sp.model\")) else: self.tokenizer = tokenizer self.do_normalize_spacing_for_tok = do_normalize_spacing_for_tok self.accent = remove_accents self.lowercase = lowercase self.numbers = normalize_numbers self.punct = punctuation self.language = language @classmethod def from_pretrained( cls, model_name: str, language: str='*', ): load_kenlm_model( language = language, pretrained_models =[model_name], ) return cls( model_name = model_name, language = language, lowercase = \"edugp\" not in model_name, remove_accents = language in {\"en\", \"my\"}, do_normalize_spacing_for_tok = \"edugp\" not in model_name, ) def pp(self, log_score, length): return 10.0 ** (-log_score / length) # Tokenize (after normalizing): See https://github.com/facebookresearch/cc_net/blob/bda555bd1cf1ee2e0b925363e62a61cd46c8b60d/cc_net/mine.py#L352 for full pipeline def get_perplexity(self, doc: str): doc = self.normalize( doc, accent=self.accent, lowercase=self.lowercase, numbers=self.numbers, punct=self.punct, tokenizer=self.tokenizer, do_tokenize=True, do_normalize_spacing_for_tok=self.do_normalize_spacing_for_tok ) doc_log_score, doc_length = 0, 0 for line in doc.split(\"\\n\"): log_score = self.model.score(line) length = len(line.split()) + 1 doc_log_score += log_score doc_length += length return round(self.pp(doc_log_score, doc_length), 1) @staticmethod def normalize( line: str, accent: bool = False, lowercase: bool = True, numbers: bool = True, punct: int = 1, do_tokenize: bool = True, tokenizer = None, do_normalize_spacing_for_tok: bool = True ) -> str: line = line.strip() if not line: return line if lowercase: line = line.lower() if accent: line = KenlmModel.strip_accents(line) if numbers: line = KenlmModel.digit_re.sub(\"0\", line) if punct == 1: line = KenlmModel.replace_unicode_punct(line) elif punct == 2: line = KenlmModel.remove_unicode_punct(line) line = KenlmModel.remove_non_printing_char(line) if do_tokenize: assert tokenizer is not None if do_normalize_spacing_for_tok: line = KenlmModel.normalize_spacing_for_tok(line) line = tokenizer.tokenize(line) line = \" \".join(\" \".join(line).replace(mt5_underscore, \" \").split()) for w in punc_char: line = line.replace(\" \"+w, w) return line @staticmethod def normalize_spacing_for_tok(text: str, language: str = \"en\") -> str: res = ( text.replace(\"\\r\", \"\") # remove extra spaces .replace(\"(\", \" (\") .replace(\")\", \") \") .replace(\" +\", \" \") ) res = re.sub(r\"\\) ([\\.\\!\\:\\?\\;\\,])\", r\"\\)\\1\", res) res = res.replace(\"( \", \"(\").replace(\" )\", \")\") res = re.sub(r\"(\\d) \\%\", r\"\\1\\%\", res) res = res.replace(\" :\", \":\").replace(\" ;\", \";\") res = res.replace(\"`\", \"'\").replace(\"''\", ' \" ') res = ( res.replace(\"\u201e\", '\"') .replace(\"\u201c\", '\"') .replace(\"\u201d\", '\"') .replace(\"\u2013\", \"-\") .replace(\"\u2014\", \" - \") .replace(\" +\", \" \") .replace(\"\u00b4\", \"'\") .replace(\"([a-z])\u2018([a-z])\", r\"\\1'\\2/\") .replace(\"([a-z])\u2019([a-z])\", r\"\\1'\\2/\") .replace(\"\u2018\", '\"') .replace(\"\u201a\", '\"') .replace(\"\u2019\", '\"') .replace(\"''\", '\"') .replace(\"\u00b4\u00b4\", '\"') .replace(\"\u2026\", \"...\") # French quotes .replace(\" \u00ab \", ' \"') .replace(\"\u00ab \", '\"') .replace(\"\u00ab\", '\"') .replace(\" \u00bb \", '\" ') .replace(\" \u00bb\", '\"') .replace(\"\u00bb\", '\"') # handle pseudo-spaces .replace(\" %\", \"%\") .replace(\"n\u00ba \", \"n\u00ba \") .replace(\" :\", \":\") .replace(\" \u00baC\", \" \u00baC\") .replace(\" cm\", \" cm\") .replace(\" ?\", \"?\") .replace(\" !\", \"!\") .replace(\" ;\", \";\") .replace(\", \", \", \") .replace(\" +\", \" \") .replace(\"\uff0e\", \". \") ) # English \"quotation,\" followed by comma, style if language == \"en\": res = re.sub(r\"\\\"([,\\.]+)\", r\"\\1\\\"\", res) # Czech is confused elif language == \"cs\" or language == \"cz\": pass # German/Spanish/French \"quotation\", followed by comma, style else: res = res.replace(',\"', '\",') res = re.sub( r\"(\\.+)\\\"(\\s*[^<])\", r\"\\\"\\1\\2\", res ) # don't fix period at end of sentence if ( language == \"de\" or language == \"es\" or language == \"cz\" or language == \"cs\" or language == \"fr\" ): res = re.sub(r\"(\\d) (\\d)\", r\"\\1,\\2\", res) else: res = re.sub(r\"(\\d) (\\d)\", r\"\\1.\\2\", res) return res @staticmethod def strip_accents(line: str) -> str: \"\"\"Strips accents from a piece of text.\"\"\" nfd = unicodedata.normalize(\"NFD\", line) output = [c for c in nfd if unicodedata.category(c) != \"Mn\"] if len(output) == line: return line return \"\".join(output) @staticmethod def replace_unicode_punct(text: str) -> str: return \"\".join(KenlmModel.unicode_punct.get(c, c) for c in text) @staticmethod def remove_unicode_punct(text: str) -> str: \"\"\"More aggressive version of replace_unicode_punct but also faster.\"\"\" return KenlmModel.unicode_punct_re.sub(\"\", text) @staticmethod def remove_non_printing_char(text: str) -> str: return KenlmModel.non_printing_chars_re.sub(\"\", text) def check_common_name(self, name: str, return_score: bool = False): \"\"\" Check if a name is a common name. :param name: Name to check. :param return_score: If True, return the score of the name and cutoff threshold of the pattern. :return: True if name is a common name, False otherwise. \"\"\" public_patterns = public_figure_kenlm_cutoff_map.get(self.language, public_figure_kenlm_cutoff_map.get('en')) model_name = self.model_name.split(\"/\")[-1] for pattern in public_patterns.get(model_name, public_patterns.get('wikipedia')): test_name = pattern['pattern'].format(name) score = self.get_perplexity(test_name) if score < pattern['cutoff']: if return_score: return True, score, pattern['cutoff'] return True if return_score: return False, 0.0, 0.0 return False check_common_name(name, return_score=False) Check if a name is a common name. :param name: Name to check. :param return_score: If True, return the score of the name and cutoff threshold of the pattern. :return: True if name is a common name, False otherwise. Source code in src/kenlm_manager.py def check_common_name(self, name: str, return_score: bool = False): \"\"\" Check if a name is a common name. :param name: Name to check. :param return_score: If True, return the score of the name and cutoff threshold of the pattern. :return: True if name is a common name, False otherwise. \"\"\" public_patterns = public_figure_kenlm_cutoff_map.get(self.language, public_figure_kenlm_cutoff_map.get('en')) model_name = self.model_name.split(\"/\")[-1] for pattern in public_patterns.get(model_name, public_patterns.get('wikipedia')): test_name = pattern['pattern'].format(name) score = self.get_perplexity(test_name) if score < pattern['cutoff']: if return_score: return True, score, pattern['cutoff'] return True if return_score: return False, 0.0, 0.0 return False remove_unicode_punct(text) staticmethod More aggressive version of replace_unicode_punct but also faster. Source code in src/kenlm_manager.py @staticmethod def remove_unicode_punct(text: str) -> str: \"\"\"More aggressive version of replace_unicode_punct but also faster.\"\"\" return KenlmModel.unicode_punct_re.sub(\"\", text) strip_accents(line) staticmethod Strips accents from a piece of text. Source code in src/kenlm_manager.py @staticmethod def strip_accents(line: str) -> str: \"\"\"Strips accents from a piece of text.\"\"\" nfd = unicodedata.normalize(\"NFD\", line) output = [c for c in nfd if unicodedata.category(c) != \"Mn\"] if len(output) == line: return line return \"\".join(output) check_for_common_name(language='en', pretrained_models=['wikipedia'], name=None, verbose=False, kenlm_models=None, return_score=False) Check if a name is a public figure or a very common name Source code in src/kenlm_manager.py def check_for_common_name( language: str = \"en\", pretrained_models: list = ['wikipedia'], name: str = None, verbose: bool = False, kenlm_models=None, return_score=False, ): \"\"\" Check if a name is a public figure or a very common name \"\"\" # load all kenlm models and cutoff patterns if kenlm_models is None: kenlm_models = load_kenlm_model(language, pretrained_models) public_patterns = public_figure_kenlm_cutoff_map.get(language, public_figure_kenlm_cutoff_map.get('en')) for model_name, model in kenlm_models.items(): for pattern in public_patterns.get(model_name, public_patterns.get('wikipedia')): test_name = pattern['pattern'].format(name) score = model.get_perplexity(test_name) if score < pattern['cutoff']: #if verbose: # print(name, score) if return_score: return True, score, pattern['cutoff'] return True if return_score: return False, 0.0, 0.0 return False load_kenlm_model(language='*', pretrained_models=['ontocord/riverbed_kenlm'], store_model=True, cache_dir=None, default_kenlm_wikipedia='./kenlm_ccnet_wikipedia_models') Load all supported kenlm model for source language. Consider if we want to use an LRU. TODO: Incorporate OSCAR kenlm models. They are quite big, and we still need patterns and cutoffs. Source code in src/kenlm_manager.py def load_kenlm_model( language: str = \"*\", pretrained_models: list = ['ontocord/riverbed_kenlm'], store_model: bool = True, cache_dir: str = None, default_kenlm_wikipedia: str = \"./kenlm_ccnet_wikipedia_models\" ) -> dict: \"\"\" Load all supported kenlm model for source language. Consider if we want to use an LRU. TODO: Incorporate OSCAR kenlm models. They are quite big, and we still need patterns and cutoffs. \"\"\" assert len(pretrained_models) <= len( kenlm_models), 'Total of number kenlm models loads larger than supported kenlm models' all_models = {} model_files = [\"arpa.bin\", \"sp.model\", ] # \"sp.vocab\" # cache to dir if cache_dir is None: cache_dir = os.path.expanduser('~') + \"/.cache\" if language is None: language = '*' # check if pretrain model exist for model_name in pretrained_models: if language in kenlm_models[model_name]: all_models[model_name] = kenlm_models[model_name][language] elif \"wikipedia\" in model_name and os.path.exists(f\"{default_kenlm_wikipedia}/{language}.arpa.bin\"): model = KenlmModel(default_kenlm_wikipedia, language, do_normalize_spacing_for_tok=True) all_models[model_name] = model if store_model: kenlm_models[model_name][language] = model elif \"wikipedia\" in model_name and language in ccnet_langs: download_ccnet_sp_kenlm_models(language, default_kenlm_wikipedia) model = KenlmModel(default_kenlm_wikipedia, language, do_normalize_spacing_for_tok=True) all_models[model_name] = model if store_model: kenlm_models[model_name][language] = model elif model_name not in kenlm_models: warnings.warn(f\"{model_name} pretrained model is not supported!\") else: os.system(f\"mkdir -p {cache_dir}/{model_name}\") found = True if language == '*': if not os.path.exists(f\"{cache_dir}/{model_name}/arpa.bin\"): try: print (f\"{cache_dir}/{model_name}/arpa.bin\") file_url = hf_hub_url(repo_id=model_name, filename=f\"arpa.bin\") file = cached_download(file_url) os.system(f\"ln -s {file} {cache_dir}/{model_name}/arpa.bin\") except: warnings.warn(f'could not find model ontocord/riverbed_kenlm/arpa.bin. will stop searching...') found = False else: for model_file in model_files: if not os.path.exists(f\"{cache_dir}/{model_name}/{language}.{model_file}\"): try: repo_id = \"/\".join(model_name.split(\"/\")[:1]) model_subtype = \"/\".join(model_name.split(\"/\")[1:]) print (f\"loading {model_name}/{language}.{model_file}\") file_url = hf_hub_url(repo_id=repo_id, filename=f\"{model_subtype}/{language}.{model_file}\") file = cached_download(file_url) os.system(f\"ln -s {file} {cache_dir}/{model_name}/{language}.{model_file}\") except: warnings.warn(f'could not find model {language}.{model_file}. will stop searching...') found = False break if found: model = KenlmModel(f\"{cache_dir}/{model_name}\", language) all_models[model_name] = model if store_model: kenlm_models[model_name][language] = model return all_models","title":"kenlm_manager"},{"location":"reference/kenlm_manager/#kenlm_manager.KenlmModel","text":"Source code in src/kenlm_manager.py class KenlmModel: digit_re: re.Pattern = re.compile(r\"\\d\") unicode_punct: Dict[str, str] = { \"\uff0c\": \",\", \"\u3002\": \".\", \"\u3001\": \",\", \"\u201e\": '\"', \"\u201d\": '\"', \"\u201c\": '\"', \"\u00ab\": '\"', \"\u00bb\": '\"', \"\uff11\": '\"', \"\u300d\": '\"', \"\u300c\": '\"', \"\u300a\": '\"', \"\u300b\": '\"', \"\u00b4\": \"'\", \"\u2236\": \":\", \"\uff1a\": \":\", \"\uff1f\": \"?\", \"\uff01\": \"!\", \"\uff08\": \"(\", \"\uff09\": \")\", \"\uff1b\": \";\", \"\u2013\": \"-\", \"\u2014\": \" - \", \"\uff0e\": \". \", \"\uff5e\": \"~\", \"\u2019\": \"'\", \"\u2026\": \"...\", \"\u2501\": \"-\", \"\u3008\": \"<\", \"\u3009\": \">\", \"\u3010\": \"[\", \"\u3011\": \"]\", \"\uff05\": \"%\", \"\u25ba\": \"-\", } unicode_punct_re = re.compile(f\"[{''.join(unicode_punct.keys())}]\") non_printing_chars_re = re.compile( f\"[{''.join(map(chr, list(range(0, 32)) + list(range(127, 160))))}]\" ) kenlm_model_dir = None sentence_piece_model_dir = None # TODO: we are not doing the sacremoses tokenizer to get put spaces between escaped chars # but consider whether we should do this for the ccnet models # https://github.com/facebookresearch/cc_net/blob/bda555bd1cf1ee2e0b925363e62a61cd46c8b60d/cc_net/tokenizer.py#L23 # does it make a difference? def __init__( self, model_name: str=None, language: str=None, lowercase: bool = False, remove_accents: bool = False, normalize_numbers: bool = True, punctuation: int = 1, do_normalize_spacing_for_tok: bool = False, tokenizer=None, model_path: str=None, ): print (model_name) self.model_name = model_name if model_path is not None: self.model = kenlm.Model(model_path) elif \"riverbed\" in model_name: self.model = kenlm.Model(os.path.join(self.model_name, f\"arpa.bin\")) tokenizer = mt5_tokenizer else: self.model = kenlm.Model(os.path.join(self.model_name, f\"{language}.arpa.bin\")) if tokenizer is None: self.tokenizer = SentencePiece(os.path.join(self.model_name, f\"{language}.sp.model\")) else: self.tokenizer = tokenizer self.do_normalize_spacing_for_tok = do_normalize_spacing_for_tok self.accent = remove_accents self.lowercase = lowercase self.numbers = normalize_numbers self.punct = punctuation self.language = language @classmethod def from_pretrained( cls, model_name: str, language: str='*', ): load_kenlm_model( language = language, pretrained_models =[model_name], ) return cls( model_name = model_name, language = language, lowercase = \"edugp\" not in model_name, remove_accents = language in {\"en\", \"my\"}, do_normalize_spacing_for_tok = \"edugp\" not in model_name, ) def pp(self, log_score, length): return 10.0 ** (-log_score / length) # Tokenize (after normalizing): See https://github.com/facebookresearch/cc_net/blob/bda555bd1cf1ee2e0b925363e62a61cd46c8b60d/cc_net/mine.py#L352 for full pipeline def get_perplexity(self, doc: str): doc = self.normalize( doc, accent=self.accent, lowercase=self.lowercase, numbers=self.numbers, punct=self.punct, tokenizer=self.tokenizer, do_tokenize=True, do_normalize_spacing_for_tok=self.do_normalize_spacing_for_tok ) doc_log_score, doc_length = 0, 0 for line in doc.split(\"\\n\"): log_score = self.model.score(line) length = len(line.split()) + 1 doc_log_score += log_score doc_length += length return round(self.pp(doc_log_score, doc_length), 1) @staticmethod def normalize( line: str, accent: bool = False, lowercase: bool = True, numbers: bool = True, punct: int = 1, do_tokenize: bool = True, tokenizer = None, do_normalize_spacing_for_tok: bool = True ) -> str: line = line.strip() if not line: return line if lowercase: line = line.lower() if accent: line = KenlmModel.strip_accents(line) if numbers: line = KenlmModel.digit_re.sub(\"0\", line) if punct == 1: line = KenlmModel.replace_unicode_punct(line) elif punct == 2: line = KenlmModel.remove_unicode_punct(line) line = KenlmModel.remove_non_printing_char(line) if do_tokenize: assert tokenizer is not None if do_normalize_spacing_for_tok: line = KenlmModel.normalize_spacing_for_tok(line) line = tokenizer.tokenize(line) line = \" \".join(\" \".join(line).replace(mt5_underscore, \" \").split()) for w in punc_char: line = line.replace(\" \"+w, w) return line @staticmethod def normalize_spacing_for_tok(text: str, language: str = \"en\") -> str: res = ( text.replace(\"\\r\", \"\") # remove extra spaces .replace(\"(\", \" (\") .replace(\")\", \") \") .replace(\" +\", \" \") ) res = re.sub(r\"\\) ([\\.\\!\\:\\?\\;\\,])\", r\"\\)\\1\", res) res = res.replace(\"( \", \"(\").replace(\" )\", \")\") res = re.sub(r\"(\\d) \\%\", r\"\\1\\%\", res) res = res.replace(\" :\", \":\").replace(\" ;\", \";\") res = res.replace(\"`\", \"'\").replace(\"''\", ' \" ') res = ( res.replace(\"\u201e\", '\"') .replace(\"\u201c\", '\"') .replace(\"\u201d\", '\"') .replace(\"\u2013\", \"-\") .replace(\"\u2014\", \" - \") .replace(\" +\", \" \") .replace(\"\u00b4\", \"'\") .replace(\"([a-z])\u2018([a-z])\", r\"\\1'\\2/\") .replace(\"([a-z])\u2019([a-z])\", r\"\\1'\\2/\") .replace(\"\u2018\", '\"') .replace(\"\u201a\", '\"') .replace(\"\u2019\", '\"') .replace(\"''\", '\"') .replace(\"\u00b4\u00b4\", '\"') .replace(\"\u2026\", \"...\") # French quotes .replace(\" \u00ab \", ' \"') .replace(\"\u00ab \", '\"') .replace(\"\u00ab\", '\"') .replace(\" \u00bb \", '\" ') .replace(\" \u00bb\", '\"') .replace(\"\u00bb\", '\"') # handle pseudo-spaces .replace(\" %\", \"%\") .replace(\"n\u00ba \", \"n\u00ba \") .replace(\" :\", \":\") .replace(\" \u00baC\", \" \u00baC\") .replace(\" cm\", \" cm\") .replace(\" ?\", \"?\") .replace(\" !\", \"!\") .replace(\" ;\", \";\") .replace(\", \", \", \") .replace(\" +\", \" \") .replace(\"\uff0e\", \". \") ) # English \"quotation,\" followed by comma, style if language == \"en\": res = re.sub(r\"\\\"([,\\.]+)\", r\"\\1\\\"\", res) # Czech is confused elif language == \"cs\" or language == \"cz\": pass # German/Spanish/French \"quotation\", followed by comma, style else: res = res.replace(',\"', '\",') res = re.sub( r\"(\\.+)\\\"(\\s*[^<])\", r\"\\\"\\1\\2\", res ) # don't fix period at end of sentence if ( language == \"de\" or language == \"es\" or language == \"cz\" or language == \"cs\" or language == \"fr\" ): res = re.sub(r\"(\\d) (\\d)\", r\"\\1,\\2\", res) else: res = re.sub(r\"(\\d) (\\d)\", r\"\\1.\\2\", res) return res @staticmethod def strip_accents(line: str) -> str: \"\"\"Strips accents from a piece of text.\"\"\" nfd = unicodedata.normalize(\"NFD\", line) output = [c for c in nfd if unicodedata.category(c) != \"Mn\"] if len(output) == line: return line return \"\".join(output) @staticmethod def replace_unicode_punct(text: str) -> str: return \"\".join(KenlmModel.unicode_punct.get(c, c) for c in text) @staticmethod def remove_unicode_punct(text: str) -> str: \"\"\"More aggressive version of replace_unicode_punct but also faster.\"\"\" return KenlmModel.unicode_punct_re.sub(\"\", text) @staticmethod def remove_non_printing_char(text: str) -> str: return KenlmModel.non_printing_chars_re.sub(\"\", text) def check_common_name(self, name: str, return_score: bool = False): \"\"\" Check if a name is a common name. :param name: Name to check. :param return_score: If True, return the score of the name and cutoff threshold of the pattern. :return: True if name is a common name, False otherwise. \"\"\" public_patterns = public_figure_kenlm_cutoff_map.get(self.language, public_figure_kenlm_cutoff_map.get('en')) model_name = self.model_name.split(\"/\")[-1] for pattern in public_patterns.get(model_name, public_patterns.get('wikipedia')): test_name = pattern['pattern'].format(name) score = self.get_perplexity(test_name) if score < pattern['cutoff']: if return_score: return True, score, pattern['cutoff'] return True if return_score: return False, 0.0, 0.0 return False","title":"KenlmModel"},{"location":"reference/kenlm_manager/#kenlm_manager.KenlmModel.check_common_name","text":"Check if a name is a common name. :param name: Name to check. :param return_score: If True, return the score of the name and cutoff threshold of the pattern. :return: True if name is a common name, False otherwise. Source code in src/kenlm_manager.py def check_common_name(self, name: str, return_score: bool = False): \"\"\" Check if a name is a common name. :param name: Name to check. :param return_score: If True, return the score of the name and cutoff threshold of the pattern. :return: True if name is a common name, False otherwise. \"\"\" public_patterns = public_figure_kenlm_cutoff_map.get(self.language, public_figure_kenlm_cutoff_map.get('en')) model_name = self.model_name.split(\"/\")[-1] for pattern in public_patterns.get(model_name, public_patterns.get('wikipedia')): test_name = pattern['pattern'].format(name) score = self.get_perplexity(test_name) if score < pattern['cutoff']: if return_score: return True, score, pattern['cutoff'] return True if return_score: return False, 0.0, 0.0 return False","title":"check_common_name()"},{"location":"reference/kenlm_manager/#kenlm_manager.KenlmModel.remove_unicode_punct","text":"More aggressive version of replace_unicode_punct but also faster. Source code in src/kenlm_manager.py @staticmethod def remove_unicode_punct(text: str) -> str: \"\"\"More aggressive version of replace_unicode_punct but also faster.\"\"\" return KenlmModel.unicode_punct_re.sub(\"\", text)","title":"remove_unicode_punct()"},{"location":"reference/kenlm_manager/#kenlm_manager.KenlmModel.strip_accents","text":"Strips accents from a piece of text. Source code in src/kenlm_manager.py @staticmethod def strip_accents(line: str) -> str: \"\"\"Strips accents from a piece of text.\"\"\" nfd = unicodedata.normalize(\"NFD\", line) output = [c for c in nfd if unicodedata.category(c) != \"Mn\"] if len(output) == line: return line return \"\".join(output)","title":"strip_accents()"},{"location":"reference/kenlm_manager/#kenlm_manager.check_for_common_name","text":"Check if a name is a public figure or a very common name Source code in src/kenlm_manager.py def check_for_common_name( language: str = \"en\", pretrained_models: list = ['wikipedia'], name: str = None, verbose: bool = False, kenlm_models=None, return_score=False, ): \"\"\" Check if a name is a public figure or a very common name \"\"\" # load all kenlm models and cutoff patterns if kenlm_models is None: kenlm_models = load_kenlm_model(language, pretrained_models) public_patterns = public_figure_kenlm_cutoff_map.get(language, public_figure_kenlm_cutoff_map.get('en')) for model_name, model in kenlm_models.items(): for pattern in public_patterns.get(model_name, public_patterns.get('wikipedia')): test_name = pattern['pattern'].format(name) score = model.get_perplexity(test_name) if score < pattern['cutoff']: #if verbose: # print(name, score) if return_score: return True, score, pattern['cutoff'] return True if return_score: return False, 0.0, 0.0 return False","title":"check_for_common_name()"},{"location":"reference/kenlm_manager/#kenlm_manager.load_kenlm_model","text":"Load all supported kenlm model for source language. Consider if we want to use an LRU. TODO: Incorporate OSCAR kenlm models. They are quite big, and we still need patterns and cutoffs. Source code in src/kenlm_manager.py def load_kenlm_model( language: str = \"*\", pretrained_models: list = ['ontocord/riverbed_kenlm'], store_model: bool = True, cache_dir: str = None, default_kenlm_wikipedia: str = \"./kenlm_ccnet_wikipedia_models\" ) -> dict: \"\"\" Load all supported kenlm model for source language. Consider if we want to use an LRU. TODO: Incorporate OSCAR kenlm models. They are quite big, and we still need patterns and cutoffs. \"\"\" assert len(pretrained_models) <= len( kenlm_models), 'Total of number kenlm models loads larger than supported kenlm models' all_models = {} model_files = [\"arpa.bin\", \"sp.model\", ] # \"sp.vocab\" # cache to dir if cache_dir is None: cache_dir = os.path.expanduser('~') + \"/.cache\" if language is None: language = '*' # check if pretrain model exist for model_name in pretrained_models: if language in kenlm_models[model_name]: all_models[model_name] = kenlm_models[model_name][language] elif \"wikipedia\" in model_name and os.path.exists(f\"{default_kenlm_wikipedia}/{language}.arpa.bin\"): model = KenlmModel(default_kenlm_wikipedia, language, do_normalize_spacing_for_tok=True) all_models[model_name] = model if store_model: kenlm_models[model_name][language] = model elif \"wikipedia\" in model_name and language in ccnet_langs: download_ccnet_sp_kenlm_models(language, default_kenlm_wikipedia) model = KenlmModel(default_kenlm_wikipedia, language, do_normalize_spacing_for_tok=True) all_models[model_name] = model if store_model: kenlm_models[model_name][language] = model elif model_name not in kenlm_models: warnings.warn(f\"{model_name} pretrained model is not supported!\") else: os.system(f\"mkdir -p {cache_dir}/{model_name}\") found = True if language == '*': if not os.path.exists(f\"{cache_dir}/{model_name}/arpa.bin\"): try: print (f\"{cache_dir}/{model_name}/arpa.bin\") file_url = hf_hub_url(repo_id=model_name, filename=f\"arpa.bin\") file = cached_download(file_url) os.system(f\"ln -s {file} {cache_dir}/{model_name}/arpa.bin\") except: warnings.warn(f'could not find model ontocord/riverbed_kenlm/arpa.bin. will stop searching...') found = False else: for model_file in model_files: if not os.path.exists(f\"{cache_dir}/{model_name}/{language}.{model_file}\"): try: repo_id = \"/\".join(model_name.split(\"/\")[:1]) model_subtype = \"/\".join(model_name.split(\"/\")[1:]) print (f\"loading {model_name}/{language}.{model_file}\") file_url = hf_hub_url(repo_id=repo_id, filename=f\"{model_subtype}/{language}.{model_file}\") file = cached_download(file_url) os.system(f\"ln -s {file} {cache_dir}/{model_name}/{language}.{model_file}\") except: warnings.warn(f'could not find model {language}.{model_file}. will stop searching...') found = False break if found: model = KenlmModel(f\"{cache_dir}/{model_name}\", language) all_models[model_name] = model if store_model: kenlm_models[model_name][language] = model return all_models","title":"load_kenlm_model()"},{"location":"reference/langid_manager/","text":"Copyright, 2021-2022 Ontocord, LLC, All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. get_lang_groups(src_lang) we use langid because it's pretty fast but it has difficulties in low resource languages langid can sometimes mistake languages that are in the same group. that is ok for our purpose as we mainly use the langid check to confirm the labels from other models. Source code in src/langid_manager.py def get_lang_groups(src_lang): \"\"\" we use langid because it's pretty fast but it has difficulties in low resource languages langid can sometimes mistake languages that are in the same group. that is ok for our purpose as we mainly use the langid check to confirm the labels from other models. \"\"\" lang_groups={src_lang} if src_lang in {'ig', 'sn', 'ny', 'st', 'zu', 'xh', 'rw', 'sw', 'yo', 'so'}: lang_groups = {'ig', 'sn', 'ny', 'st', 'zu', 'xh', 'rw', 'sw', 'yo', 'so'} elif src_lang in {'mr', 'ne', 'hi', }: lang_groups = {'mr', 'ne', 'hi', } elif src_lang in {'fr', 'br'}: lang_groups = {'fr','la', 'br' } elif src_lang in {'pt', }: lang_groups = {'pt','la', 'gl' } elif src_lang in {'eo', 'es', 'oc', 'ca', 'eu', 'an', 'gl' }: lang_groups = {'eo', 'es', 'oc', 'ca', 'eu', 'an', 'gl', 'la' } elif src_lang in {'arz', 'ar', 'fa', 'ur', 'az', 'azb', 'ckb', 'ps' }: lang_groups = {'arz', 'ar', 'fa', 'ur', 'az', 'azb', 'ckb', 'ps' } elif src_lang in {'id', 'ms', }: lang_groups = {'id', 'ms',} elif src_lang in {'as', 'bn', 'bpy'}: lang_groups = {'as', 'bn', 'bpy'} elif src_lang in {'af', 'nl', }: lang_groups = {'af', 'nl',} elif src_lang in {'bo', 'dz', }: lang_groups = {'bo', 'dz',} elif src_lang in {'bs', 'hr', }: lang_groups = {'bs', 'hr',} elif src_lang in {'bxr', 'mn', }: lang_groups = {'bxr', 'mn',} elif src_lang in {'ceb', 'tl', }: lang_groups = {'ceb', 'tl',} elif src_lang in {'cs', 'sk', }: lang_groups = {'cs', 'sk',} elif src_lang in {'da', 'no', }: lang_groups = {'da', 'no',} elif src_lang in {'eml', 'wa', }: lang_groups = {'eml', 'wa',} elif src_lang in {'de', 'lb', 'pl', 'dsb'}: lang_groups = {'de', 'lb', 'pl', 'dsb'} elif src_lang in {'id', 'jv', 'ms', 'tl',}: lang_groups = {'id', 'jv', 'ms', 'tl', } elif src_lang in {'av', 'ru', 'bg', 'ba', 'kk', 'ky', 'uk', 'be', 'ce', 'cv'}: lang_groups = {'av', 'ru', 'bg', 'ba', 'kk', 'ky', 'uk', 'be', 'ce', 'cv'} return lang_groups","title":"langid_manager"},{"location":"reference/langid_manager/#langid_manager.get_lang_groups","text":"we use langid because it's pretty fast but it has difficulties in low resource languages langid can sometimes mistake languages that are in the same group. that is ok for our purpose as we mainly use the langid check to confirm the labels from other models. Source code in src/langid_manager.py def get_lang_groups(src_lang): \"\"\" we use langid because it's pretty fast but it has difficulties in low resource languages langid can sometimes mistake languages that are in the same group. that is ok for our purpose as we mainly use the langid check to confirm the labels from other models. \"\"\" lang_groups={src_lang} if src_lang in {'ig', 'sn', 'ny', 'st', 'zu', 'xh', 'rw', 'sw', 'yo', 'so'}: lang_groups = {'ig', 'sn', 'ny', 'st', 'zu', 'xh', 'rw', 'sw', 'yo', 'so'} elif src_lang in {'mr', 'ne', 'hi', }: lang_groups = {'mr', 'ne', 'hi', } elif src_lang in {'fr', 'br'}: lang_groups = {'fr','la', 'br' } elif src_lang in {'pt', }: lang_groups = {'pt','la', 'gl' } elif src_lang in {'eo', 'es', 'oc', 'ca', 'eu', 'an', 'gl' }: lang_groups = {'eo', 'es', 'oc', 'ca', 'eu', 'an', 'gl', 'la' } elif src_lang in {'arz', 'ar', 'fa', 'ur', 'az', 'azb', 'ckb', 'ps' }: lang_groups = {'arz', 'ar', 'fa', 'ur', 'az', 'azb', 'ckb', 'ps' } elif src_lang in {'id', 'ms', }: lang_groups = {'id', 'ms',} elif src_lang in {'as', 'bn', 'bpy'}: lang_groups = {'as', 'bn', 'bpy'} elif src_lang in {'af', 'nl', }: lang_groups = {'af', 'nl',} elif src_lang in {'bo', 'dz', }: lang_groups = {'bo', 'dz',} elif src_lang in {'bs', 'hr', }: lang_groups = {'bs', 'hr',} elif src_lang in {'bxr', 'mn', }: lang_groups = {'bxr', 'mn',} elif src_lang in {'ceb', 'tl', }: lang_groups = {'ceb', 'tl',} elif src_lang in {'cs', 'sk', }: lang_groups = {'cs', 'sk',} elif src_lang in {'da', 'no', }: lang_groups = {'da', 'no',} elif src_lang in {'eml', 'wa', }: lang_groups = {'eml', 'wa',} elif src_lang in {'de', 'lb', 'pl', 'dsb'}: lang_groups = {'de', 'lb', 'pl', 'dsb'} elif src_lang in {'id', 'jv', 'ms', 'tl',}: lang_groups = {'id', 'jv', 'ms', 'tl', } elif src_lang in {'av', 'ru', 'bg', 'ba', 'kk', 'ky', 'uk', 'be', 'ce', 'cv'}: lang_groups = {'av', 'ru', 'bg', 'ba', 'kk', 'ky', 'uk', 'be', 'ce', 'cv'} return lang_groups","title":"get_lang_groups()"},{"location":"reference/pdf_and_ocr/","text":"","title":"pdf_and_ocr"},{"location":"reference/pii_manager/","text":"lang_2_country = {'am': ['et'], 'ar': ['ae', 'iq', 'dz', 'eg', 'sd', 'aa', 'il', 'ps', 'sa', 'bh', 'km', 'dj', 'er', 'eh', 'jo', 'kw', 'lb', 'ly', 'ma', 'mr', 'om', 'qa', 'so', 'sy', 'td', 'tn', 'ye'], 'ay': ['bo'], 'az': ['az'], 'be': ['by'], 'bg': ['bg'], 'bi': ['vu'], 'bn': ['bd'], 'bs': ['ba'], 'ca': ['ad'], 'ch': ['gu'], 'cs': ['cz'], 'da': ['dk'], 'de': ['at', 'ch', 'de', 'be', 'li', 'lu'], 'dv': ['mv'], 'dz': ['bt'], 'el': ['gr', 'cy'], 'en': ['pk', 'sd', 'au', 'ca', 'gb', 'gh', 'ie', 'in', 'nz', 'us', 'ai', 'as', 'ag', 'bi', 'bs', 'bz', 'bm', 'bb', 'bw', 'cc', 'cm', 'ck', 'cx', 'ky', 'dm', 'er', 'fj', 'fk', 'fm', 'gg', 'gi', 'gm', 'gd', 'gu', 'gy', 'hk', 'im', 'io', 'jm', 'je', 'ke', 'ki', 'kn', 'lr', 'lc', 'ls', 'mg', 'mh', 'mt', 'mp', 'ms', 'mu', 'mw', 'na', 'nf', 'ng', 'nu', 'nr', 'pn', 'ph', 'pw', 'pg', 'pr', 'rw', 'sg', 'sh', 'sb', 'sl', 'ss', 'sz', 'sx', 'sc', 'tc', 'tk', 'to', 'tt', 'tv', 'tz', 'ug', 'um', 'vc', 'vg', 'vi', 'vu', 'ws', 'za', 'zm', 'zw'], 'es': ['ar', 'es', 'mx', 'bo', 'cl', 'co', 'cr', 'cu', 'do', 'ec', 'gq', 'gt', 'hn', 'ni', 'pa', 'pe', 'pr', 'py', 'sv', 'uy', 've'], 'et': ['ee'], 'fa': ['ir', 'af'], 'fi': ['fi'], 'fil': ['ph'], 'fj': ['fj'], 'fo': ['fo'], 'fr': ['dz', 'ca', 'ch', 'fr', 'qc', 'bi', 'be', 'bj', 'bf', 'bl', 'cf', 'ci', 'cm', 'cd', 'cg', 'km', 'dj', 'ga', 'gn', 'gp', 'gq', 'gf', 'ht', 'lu', 'mf', 'ma', 'mc', 'mg', 'ml', 'mq', 'mu', 'yt', 'nc', 'ne', 'pf', 're', 'rw', 'sn', 'pm', 'sc', 'sy', 'td', 'tg', 'tn', 'vu', 'wf'], 'ga': ['ie'], 'gil': ['ki'], 'gn': ['py'], 'gsw': ['ch', 'li'], 'gv': ['im'], 'he': ['il'], 'hi': ['in'], 'hif': ['fj'], 'ho': ['pg'], 'hr': ['hr', 'ba'], 'ht': ['ht'], 'hu': ['hu'], 'hy': ['am'], 'id': ['id'], 'is': ['is'], 'it': ['ch', 'it', 'sm', 'va'], 'ja': ['jp'], 'ka': ['ge'], 'kk': ['kz'], 'kl': ['gl'], 'km': ['kh'], 'ko': ['kr', 'kp'], 'ky': ['kg'], 'lb': ['lu'], 'lo': ['la'], 'lt': ['lt'], 'lv': ['lv'], 'mg': ['mg'], 'mh': ['mh'], 'mi': ['nz'], 'mk': ['mk'], 'mn': ['mn'], 'ms': ['bn', 'my', 'sg'], 'mt': ['mt'], 'my': ['mm'], 'na': ['nr'], 'nb': ['no', 'sj'], 'nd': ['zw'], 'ne': ['np'], 'niu': ['nu'], 'nl': ['nl', 'aw', 'be', 'bq', 'cw', 'sr', 'sx'], 'nn': ['no'], 'ny': ['mw'], 'pap': ['aw', 'cw'], 'pau': ['pw'], 'pl': ['pl'], 'ps': ['af'], 'pt': ['br', 'pt', 'ao', 'cv', 'gw', 'gq', 'mo', 'mz', 'st', 'tl'], 'qu': ['bo', 'ec', 'pe'], 'rn': ['bi'], 'ro': ['ro', 'md'], 'ru': ['ru', 'ua', 'by', 'kz', 'kg'], 'rw': ['rw'], 'sg': ['cf'], 'si': ['lk'], 'sk': ['sk'], 'sl': ['si'], 'sm': ['as', 'ws'], 'sn': ['zw'], 'so': ['so'], 'sq': ['al'], 'sr': ['ba', 'me', 'rs'], 'ss': ['sz'], 'st': ['ls'], 'sv': ['fi', 'se', 'ax'], 'sw': ['ke', 'tz', 'ug'], 'ta': ['lk', 'sg'], 'tet': ['tl'], 'tg': ['tj'], 'th': ['th'], 'ti': ['er'], 'tk': ['tm'], 'tkl': ['tk'], 'tn': ['bw'], 'to': ['to'], 'tpi': ['pg'], 'tr': ['tr', 'cy'], 'tvl': ['tv'], 'ty': ['pf'], 'tzm': ['ma'], 'uk': ['ua'], 'ur': ['pk'], 'uz': ['uz'], 'vi': ['vn'], 'wni': ['km'], 'wo': ['sn'], 'yo': ['ng'], 'zdj': ['km'], 'zh': ['cn', 'tw', 'hk', 'mo', 'sg']} module-attribute Copyright, 2021-2022 Ontocord, LLC, All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. detect_ner_with_regex_and_context(sentence, src_lang, tag_type=None, prioritize_lang_match_over_ignore=True, ignore_stdnum_type={'isil', 'isbn', 'isan', 'imo', 'gs1_128', 'grid', 'figi', 'ean', 'casrn', 'cusip'}, all_regex=None, context_window=20, min_id_length=6, max_id_length=50, precedence={'PHONE': 1, 'IP_ADDRESS': 2, 'DATE': 3, 'TIME': 4, 'LICENSE_PLATE': 5, 'USER': 6, 'AGE': 7, 'ID': 8, 'KEY': 9, 'ADDRESS': 10, 'URL': 11, 'EQUATION': 12}) Output This function returns a list of 4 tuples, representing an NER detection for [(entity, start, end, tag), ...] Input :sentence: any text, including a sentence or a document to tag :src_lang: the language of the sentence :context_window: the contxt window in characters to check for context characters for any rules that requries context :max_id_length: the maximum length of an ID :min_id_length: the minimum length of an ID :tag_type: the type of NER tags we are detecting. If None, then detect everything. :ignore_stdnum_type: the set of stdnum we will consider NOT PII and not match as an ID :prioritize_lang_match_over_ignore: if true, and an ID matches an ingore list, we still keep it as an ID if there was an ID match for this particular src_lang :all_regex: a rulebase of the form {tag: {lang: [(regex, context, block), ...], 'default': [(regex, context, block), ...]}}. context are words that must be found surronding the entity. block are words that must not be found. If all_regex is none, then we use the global regex_rulebase ALGORITHM For each regex, we check the sentence to find a match and a required context, if the context exists in a window. If the regex is an ID or a DATE, test to see if it's a stdnum we know. Stdnum are numbers formatted to specific regions, or generally. If it is a stdnum and NOT a PII type (such as ISBN numbers) skip this ID. UNLESS If the stdnum is ALSO a PII type for the local region of the language, then consider it a matched stdnum. If it's a matched stdnum that is not skipped, save it as an ID. If the ID is not a stdum, check if the ID is a DATE. If it's a DATE using context words in a context window. If it's a DATE then save it as a DATE, else save as ID. Gather all regex matches and sort the list by position, prefering longer matches, and DATEs and ADDRESSES over IDs. For all subsumed IDs and DATEs, remove those subsumed items. Return a list of potentially overlapping NER matched. NOTE: - There may be overlaps in mention spans. - Unlike presidio, we require that a context be met. We don't increase a score if a context is matched. - A regex does not need to match string boundaries or space boundaries. The matching code checks this. We require all entities that is not cjk to have space or special char boundaries or boundaries at end or begining of sentence. - As such, We don't match embedded IDs: e.g., MyIDis555-555-5555 won't match the ID. This is to preven matching extremely nosiy imput that might have patterns of numbers in long strings. Source code in src/pii_manager.py def detect_ner_with_regex_and_context(sentence, src_lang, tag_type= None, prioritize_lang_match_over_ignore=True, \\ ignore_stdnum_type={'isil', 'isbn', 'isan', 'imo', 'gs1_128', 'grid', 'figi', 'ean', 'casrn', 'cusip' }, \\ all_regex=None, context_window=20, min_id_length=6, max_id_length=50, \\ precedence={'PHONE':1, 'IP_ADDRESS':2, 'DATE':3, 'TIME':4, 'LICENSE_PLATE':5, 'USER':6, 'AGE':7, 'ID':8, 'KEY': 9, 'ADDRESS':10, 'URL':11, 'EQUATION': 12}): \"\"\" Output: - This function returns a list of 4 tuples, representing an NER detection for [(entity, start, end, tag), ...] Input: :sentence: any text, including a sentence or a document to tag :src_lang: the language of the sentence :context_window: the contxt window in characters to check for context characters for any rules that requries context :max_id_length: the maximum length of an ID :min_id_length: the minimum length of an ID :tag_type: the type of NER tags we are detecting. If None, then detect everything. :ignore_stdnum_type: the set of stdnum we will consider NOT PII and not match as an ID :prioritize_lang_match_over_ignore: if true, and an ID matches an ingore list, we still keep it as an ID if there was an ID match for this particular src_lang :all_regex: a rulebase of the form {tag: {lang: [(regex, context, block), ...], 'default': [(regex, context, block), ...]}}. context are words that must be found surronding the entity. block are words that must not be found. If all_regex is none, then we use the global regex_rulebase ALGORITHM: For each regex, we check the sentence to find a match and a required context, if the context exists in a window. If the regex is an ID or a DATE, test to see if it's a stdnum we know. Stdnum are numbers formatted to specific regions, or generally. If it is a stdnum and NOT a PII type (such as ISBN numbers) skip this ID. UNLESS If the stdnum is ALSO a PII type for the local region of the language, then consider it a matched stdnum. If it's a matched stdnum that is not skipped, save it as an ID. If the ID is not a stdum, check if the ID is a DATE. If it's a DATE using context words in a context window. If it's a DATE then save it as a DATE, else save as ID. Gather all regex matches and sort the list by position, prefering longer matches, and DATEs and ADDRESSES over IDs. For all subsumed IDs and DATEs, remove those subsumed items. Return a list of potentially overlapping NER matched. NOTE: - There may be overlaps in mention spans. - Unlike presidio, we require that a context be met. We don't increase a score if a context is matched. - A regex does not need to match string boundaries or space boundaries. The matching code checks this. We require all entities that is not cjk to have space or special char boundaries or boundaries at end or begining of sentence. - As such, We don't match embedded IDs: e.g., MyIDis555-555-5555 won't match the ID. This is to preven matching extremely nosiy imput that might have patterns of numbers in long strings. \"\"\" sw = all_stopwords.get(src_lang, {}) # if we are doing 'ID', we would still want to see if we catch an ADDRESS. # ADDRESS may have higher precedence, in which case it might overide an ID match. no_address = False if tag_type is not None and 'ID' in tag_type and 'ADDRESS' not in tag_type: no_address = True tag_type = set(list(tag_type)+['ADDRESS']) # if we are doing 'DATE' we would still want to do ID because they intersect. no_id = False if tag_type is not None and 'DATE' in tag_type and 'ID' not in tag_type: no_id = True tag_type = set(list(tag_type)+['ID']) # if we are doing 'AGE' we would still want to do DATE because they intersect. no_date = False if tag_type is not None and 'AGE' in tag_type and 'DATE' not in tag_type: no_date = True tag_type = set(list(tag_type)+['DATE']) is_cjk = src_lang in {'zh', 'zh-classical', 'zh-min-nan', 'zh-yue', 'ko', 'ja', 'th', 'jv'} if is_cjk: sentence_set = set(sentence.lower()) else: sentence_set = [] #let's do a sanity check. there should be no words beyond 100 chars. #this will really mess up our regexes. for word in sentence.split(\" \"): len_word = len(word) if len_word > 100: sentence = sentence.replace(word, \" \"*len_word) else: sentence_set.append(word.lower()) sentence_set = set([s.strip(rstrip_chars) for s in sentence_set]) all_ner = [] len_sentence = len(sentence) if all_regex is None: all_regex = regex_rulebase if tag_type is None: all_tags_to_check = list(all_regex.keys()) else: all_tags_to_check = list(tag_type) for tag in all_tags_to_check: regex_group = all_regex.get(tag) if not regex_group: continue for regex_context, extra_weight in [(a, 1) for a in regex_group.get(src_lang, [])] + [(a, 0) for a in regex_group.get(\"default\", [])]: if True: regex, context, block = regex_context #if this regex rule requires a context, find if it is satisified in general. this is a quick check. potential_context = False if context: for c1 in context: c1 = c1.lower() for c2 in c1.split(): c2 = c2.strip(rstrip_chars) if c2 in sentence_set: potential_context = True break if potential_context: break if not potential_context: continue #now apply regex for ent in list(set(list(regex.findall(sentence)))): #print (ent) if not isinstance(ent, str): continue ent = ent.strip() #ent = ent.rstrip(rstrip_chars) #ent = ent.lstrip(lstrip_chars) if not ent: continue ent_is_4_digit=False len_ent = len(ent) if len_ent == 4: try: int(ent) ent_is_4_digit=True except: ent_is_4_digit=False sentence2 = sentence delta = 0 #check to see if the ID or DATE is type of stdnum is_stdnum = False if tag in ('ID', 'DATE'): #simple length test ent_no_space = ent.replace(\" \", \"\").replace(\".\", \"\").replace(\"-\", \"\") if len(ent_no_space) > max_id_length and tag == 'ID': continue if len(ent_no_space) < min_id_length and tag == 'ID': continue #check if this is really a non PII stdnum, unless it's specifically an ID for a country using this src_lang. #TODO - complete the country to src_lang dict above. stnum_type = ent_2_stdnum_type(ent, src_lang) #if the stdnum is one of the non PII types, we will ignore it if prioritize_lang_match_over_ignore: is_stdnum = any(a for a in stnum_type if \".\" in a and src_lang in country_2_lang.get(a.split(\".\")[0], [])) if not ent_is_4_digit and not is_stdnum and any(a for a in stnum_type if a in ignore_stdnum_type): #a four digit entity might be a year, so don't skip this ent continue #this is actually an ID of known stdnum and not a DATE if any(a for a in stnum_type if a not in ignore_stdnum_type): tag = 'ID' is_stdnum = True #let's check the FIRST instance of this DATE or ID is really a date; #ideally we should do this for every instance of this ID if tag == 'DATE' or (tag == 'ID' and not is_stdnum): ent, tag = test_is_date(ent, tag, sentence, len_sentence, is_cjk, sentence.index(ent), src_lang, sw) if not ent: continue #do some confirmation for addresses if libpostal is installed. TODO, test if this works for zh. libpostal appears to test for pinyin. if tag == 'ADDRESS' and not parse_address: continue if tag == 'ADDRESS' and parse_address: address = parse_address(ent) if address and not any(ad for ad in address if ad[1] != 'house'): continue # this isn't an address if address and address[0][1] == 'house': address = address[1:] ent_lower = ent.lower() if address[0][0].lower() in ent_lower: ent = ent[ent_lower.index(address[0][0]):].strip(rstrip_chars) #print ('**', ent) if not ent or to_int (ent) is not None: continue # this isn't an address #TODO strip stopwords on either end of an ent for addresses - whether or not libpostal is installed else: pass #print ('problem with address', address) #print ('parse address', ent, '***', address) #now let's check context, block lists and turn all occurances of ent in this sentence into a span mention and also check for context and block words len_ent = len(ent) while True: if not ent or ent not in sentence2: break else: i = sentence2.index(ent) j = i + len_ent if potential_context or block: len_sentence2 = len(sentence2) left = \" \"+ sentence2[max(0, i - context_window) : i].replace(\",\", \" \").lower()+ \" \" right = \" \"+ sentence2[j : min(len_sentence2, j + context_window)].replace(\",\", \" \").lower() + \" \" found_context = False ent_lower = \" \"+ent.replace(\",\", \" \").lower()+ \" \" if context: for c in context: c = c.lower() if is_cjk: if c in left or c in right or c in ent_lower: found_context = True break else: if (\" \"+c+\" \" in left or \" \"+c+\" \" in right or \" \"+c+\" \" in ent_lower): found_context = True #print ('foound context', c) break else: found_context = True if block: for c in block: new_tag = None if type(c) is tuple: c, new_tag = c c = c.lower() if is_cjk: if c in left or c in right or c in ent_lower: if new_tag is not None: tag = new_tag #switching the tag to a subsumed tag. DATE=>AGE break else: found_context = False break else: if (\" \"+c+\" \" in left or \" \"+c+\" \" in right or \" \"+c+\" \" in ent_lower): if new_tag is not None: tag = new_tag #switching the tag to a subsumed tag. DATE=>AGE break else: found_context = False break if not found_context: delta += j sentence2 = sentence2[i+len(ent):] continue #check to see if the entity is really a standalone word or part of another longer word. # for example, we wont match a partial set of very long numbers as a 7 digit ID for example if is_cjk or ((i+delta == 0 or sentence2[i-1] in lstrip_chars) and (j+delta >= len_sentence-1 or sentence2[j] in rstrip_chars)): all_ner.append((ent, delta+i, delta+j, tag, extra_weight)) sentence2 = sentence2[i+len(ent):] delta += j all_ner = list(set(all_ner)) # let's remove overlapping # sort by length and position, favoring non-IDs first using the precedence list, # and additionaly giving one extra weight to language specific regex (as opposed to default rules). # NOTE: this doesn't do a perfect overlap match; just an overlap to the prior item. all_ner.sort(key=lambda a: a[1]+(1.0/(1.0+(100*((precedence.get(a[3], min(20,len(a[3])))+a[4]))+a[2]-a[1])))) #print (all_ner) if not tag_type or 'ID' in tag_type: # now do overlaps prefering longer ents, and higher prededence items over embedded IDs or dates, etc. all_ner2 = [] prev_mention = None for mention in all_ner: if prev_mention: if (prev_mention[1] == mention[1] and prev_mention[3] == mention[3] and prev_mention[4] > 0 and prev_mention[4] != mention[4]) or\\ (prev_mention[2] >= mention[1] and prev_mention[2] >= mention[2]): continue else: prev_mention = mention else: prev_mention = mention all_ner2.append(mention[:4]) all_ner = all_ner2 #TODO - refactor to check the tag_type list instead to do filtering. if no_address: all_ner = [a for a in all_ner if a[3] != 'ADDRESS'] if no_id: all_ner = [a for a in all_ner if a[3] != 'ID'] if no_date: all_ner = [a for a in all_ner if a[3] != 'DATE'] return all_ner ent_2_stdnum_type(text, src_lang=None) given a entity mention and the src_lang, determine potentially stdnum type Source code in src/pii_manager.py def ent_2_stdnum_type(text, src_lang=None): \"\"\" given a entity mention and the src_lang, determine potentially stdnum type \"\"\" stdnum_type = [] if src_lang is None: items = list(stdnum_mapper.items()) else: l1 = lang_2_stdnum.get(src_lang, []) + lang_2_stdnum.get('default', []) items = [(a1, stdnum_mapper[a1]) for a1 in l1] for ent_type, validate in items: try: found = validate(text) except: found = False if found: stdnum_type.append (ent_type) return stdnum_type is_fast_date(ent, int_arr=None, year_start=1600, year_end=2050) search for patterns like, yyyy-mm-dd, dd-mm-yyyy, yyyy-yyyy Source code in src/pii_manager.py def is_fast_date(ent, int_arr=None, year_start=1600, year_end=2050): \"\"\"search for patterns like, yyyy-mm-dd, dd-mm-yyyy, yyyy-yyyy \"\"\" if int_arr: len_int_arr = len(int_arr) if len_int_arr == 1 or len_int_arr > 3: return False if int_arr is None: ent_arr = ent.replace(\"/\", \"-\").replace(\" \",\"-\").replace(\".\",\"-\") if not (\"-\" in ent_arr and ent_arr.count(\"-\") <=2): return False int_arr = [(e, to_int(e)) for e in ent_arr.split(\"-\")] is_date = False has_year = has_month = has_day = 0 for e, val in int_arr: if val is None: break if (val <= year_end and val >= year_start): has_year +=1 elif val <= 12 and val >= 1: has_month += 1 elif val <= 31 and val >= 1: has_day += 1 else: return False if (has_year == 1 and has_month == 1) or \\ (has_year == 2 and has_month == 0 and has_day == 0) or \\ (has_year == 1 and has_month == 1 and has_day == 1): return True return False test_is_date(ent, tag, sentence, len_sentence, is_cjk, i, src_lang, sw, year_start=1600, year_end=2050) Helper function used to test if an ent is a date or not We use dateparse to find context words around the ID/date to determine if its a date or not. For example, 100 AD is a date, but 100 might not be. Input :ent: an entity mention :tag: either ID or DATE :sentence: the context :is_cjk: if this is a Zh, Ja, Ko text :i: the position of ent in the sentence Returns: (ent, tag): potentially expanded ent, and the proper tag. Could return a potentially expanded ent, and the proper tag. Returns ent as None, if originally tagged as 'DATE' and it's not a DATE and we don't know what it is. Source code in src/pii_manager.py def test_is_date(ent, tag, sentence, len_sentence, is_cjk, i, src_lang, sw, year_start=1600, year_end=2050): \"\"\" Helper function used to test if an ent is a date or not We use dateparse to find context words around the ID/date to determine if its a date or not. For example, 100 AD is a date, but 100 might not be. Input: :ent: an entity mention :tag: either ID or DATE :sentence: the context :is_cjk: if this is a Zh, Ja, Ko text :i: the position of ent in the sentence Returns: (ent, tag): potentially expanded ent, and the proper tag. Could return a potentially expanded ent, and the proper tag. Returns ent as None, if originally tagged as 'DATE' and it's not a DATE and we don't know what it is. \"\"\" # perform some fast heuristics so we don't have to do dateparser len_ent = len(ent) if len_ent > 17 or (len_ent > 8 and to_int(ent)): if tag == 'DATE': #this is a very long number and not a date return None, tag else: #no need to check the date return ent, tag if not is_cjk: if i > 0 and sentence[i-1] not in lstrip_chars: if tag == 'DATE': return None, tag else: return ent, tag if i+len_ent < len_sentence - 1 and sentence[i+len_ent+1] not in rstrip_chars: if tag == 'DATE': return None, tag else: return ent, tag int_arr = [(e, to_int(e)) for e in ent.replace(\"/\", \"-\").replace(\" \",\"-\").replace(\".\",\"-\").split(\"-\")] if is_fast_date(ent, int_arr): #this is most likely a date return ent, 'DATE' for e, val in int_arr: if val is not None and len(e) > 8: if tag == 'DATE': #this is a very long number and not a date return None, tag #test if this is a 4 digit year. we need to confirm it's a real date is_date = False is_4_digit_year = False if tag == 'DATE' and len_ent == 4: e = to_int(ent) is_4_digit_year = (e <= year_end and e >= year_start) #now do dateparser if not is_4_digit_year: try: is_date = dateparser.parse(ent, languages=[date_parser_lang_mapper.get(src_lang,src_lang)]) # use src_lang to make it faster, languages=[src_lang]) except: is_date = dateparser.parse(ent, languages=[\"en\"]) if (not is_date and tag == 'DATE') or (is_date and tag == 'ID'): j = i + len_ent #for speed we can just use these 6 windows to check for a date. #but for completeness we could check a sliding window. #Maybe in some countries a year could #be in the middle of a date: Month Year Day ent_spans = [(-3,0), (-2, 0), (-1, 0), \\ (0, 3), (0, 2), (0, 1)] before = sentence[:i] after = sentence[j:] if before and not is_cjk and before[-1] not in lstrip_chars: is_date = False elif after and not is_cjk and after[0] not in rstrip_chars: is_date = False else: if not is_cjk: before = before.split() after = after.split() len_after = len(after) len_before = len(before) for before_words, after_words in ent_spans: if after_words > len_after: continue if -before_words > len_before: continue if before_words == 0: before1 = [] else: before1 = before[max(-len_before,before_words):] after1 = after[:min(len_after,after_words)] if is_cjk: ent2 = \"\".join(before1)+ent+\"\".join(after1) else: ent2 = \" \".join(before1)+\" \"+ent+\" \"+\" \".join(after1) if ent2.strip() == ent: continue try: is_date = dateparser.parse(ent2, languages=[date_parser_lang_mapper.get(src_lang,src_lang)])# use src_lang to make it faster, languages=[src_lang]) except: is_date = dateparser.parse(ent, languages=[\"en\"]) if is_date: #sometimes dateparser says things like \"in 2020\" is a date, which it is #but we want to strip out the stopwords. if before1 and before1[-1].lower() in sw: before1 = before1[:-1] if after1 and after1[0].lower() in sw: after1 = after1[1:] if is_cjk: ent2 = \"\".join(before1)+ent+\"\".join(after1) else: ent2 = \" \".join(before1)+\" \"+ent+\" \"+\" \".join(after1) ent = ent2.strip() tag = \"DATE\" return ent, tag if tag == 'DATE' and not is_date: return None, tag return ent, tag","title":"pii_manager"},{"location":"reference/pii_manager/#pii_manager.lang_2_country","text":"Copyright, 2021-2022 Ontocord, LLC, All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"lang_2_country"},{"location":"reference/pii_manager/#pii_manager.detect_ner_with_regex_and_context","text":"Output This function returns a list of 4 tuples, representing an NER detection for [(entity, start, end, tag), ...] Input :sentence: any text, including a sentence or a document to tag :src_lang: the language of the sentence :context_window: the contxt window in characters to check for context characters for any rules that requries context :max_id_length: the maximum length of an ID :min_id_length: the minimum length of an ID :tag_type: the type of NER tags we are detecting. If None, then detect everything. :ignore_stdnum_type: the set of stdnum we will consider NOT PII and not match as an ID :prioritize_lang_match_over_ignore: if true, and an ID matches an ingore list, we still keep it as an ID if there was an ID match for this particular src_lang :all_regex: a rulebase of the form {tag: {lang: [(regex, context, block), ...], 'default': [(regex, context, block), ...]}}. context are words that must be found surronding the entity. block are words that must not be found. If all_regex is none, then we use the global regex_rulebase ALGORITHM For each regex, we check the sentence to find a match and a required context, if the context exists in a window. If the regex is an ID or a DATE, test to see if it's a stdnum we know. Stdnum are numbers formatted to specific regions, or generally. If it is a stdnum and NOT a PII type (such as ISBN numbers) skip this ID. UNLESS If the stdnum is ALSO a PII type for the local region of the language, then consider it a matched stdnum. If it's a matched stdnum that is not skipped, save it as an ID. If the ID is not a stdum, check if the ID is a DATE. If it's a DATE using context words in a context window. If it's a DATE then save it as a DATE, else save as ID. Gather all regex matches and sort the list by position, prefering longer matches, and DATEs and ADDRESSES over IDs. For all subsumed IDs and DATEs, remove those subsumed items. Return a list of potentially overlapping NER matched. NOTE: - There may be overlaps in mention spans. - Unlike presidio, we require that a context be met. We don't increase a score if a context is matched. - A regex does not need to match string boundaries or space boundaries. The matching code checks this. We require all entities that is not cjk to have space or special char boundaries or boundaries at end or begining of sentence. - As such, We don't match embedded IDs: e.g., MyIDis555-555-5555 won't match the ID. This is to preven matching extremely nosiy imput that might have patterns of numbers in long strings. Source code in src/pii_manager.py def detect_ner_with_regex_and_context(sentence, src_lang, tag_type= None, prioritize_lang_match_over_ignore=True, \\ ignore_stdnum_type={'isil', 'isbn', 'isan', 'imo', 'gs1_128', 'grid', 'figi', 'ean', 'casrn', 'cusip' }, \\ all_regex=None, context_window=20, min_id_length=6, max_id_length=50, \\ precedence={'PHONE':1, 'IP_ADDRESS':2, 'DATE':3, 'TIME':4, 'LICENSE_PLATE':5, 'USER':6, 'AGE':7, 'ID':8, 'KEY': 9, 'ADDRESS':10, 'URL':11, 'EQUATION': 12}): \"\"\" Output: - This function returns a list of 4 tuples, representing an NER detection for [(entity, start, end, tag), ...] Input: :sentence: any text, including a sentence or a document to tag :src_lang: the language of the sentence :context_window: the contxt window in characters to check for context characters for any rules that requries context :max_id_length: the maximum length of an ID :min_id_length: the minimum length of an ID :tag_type: the type of NER tags we are detecting. If None, then detect everything. :ignore_stdnum_type: the set of stdnum we will consider NOT PII and not match as an ID :prioritize_lang_match_over_ignore: if true, and an ID matches an ingore list, we still keep it as an ID if there was an ID match for this particular src_lang :all_regex: a rulebase of the form {tag: {lang: [(regex, context, block), ...], 'default': [(regex, context, block), ...]}}. context are words that must be found surronding the entity. block are words that must not be found. If all_regex is none, then we use the global regex_rulebase ALGORITHM: For each regex, we check the sentence to find a match and a required context, if the context exists in a window. If the regex is an ID or a DATE, test to see if it's a stdnum we know. Stdnum are numbers formatted to specific regions, or generally. If it is a stdnum and NOT a PII type (such as ISBN numbers) skip this ID. UNLESS If the stdnum is ALSO a PII type for the local region of the language, then consider it a matched stdnum. If it's a matched stdnum that is not skipped, save it as an ID. If the ID is not a stdum, check if the ID is a DATE. If it's a DATE using context words in a context window. If it's a DATE then save it as a DATE, else save as ID. Gather all regex matches and sort the list by position, prefering longer matches, and DATEs and ADDRESSES over IDs. For all subsumed IDs and DATEs, remove those subsumed items. Return a list of potentially overlapping NER matched. NOTE: - There may be overlaps in mention spans. - Unlike presidio, we require that a context be met. We don't increase a score if a context is matched. - A regex does not need to match string boundaries or space boundaries. The matching code checks this. We require all entities that is not cjk to have space or special char boundaries or boundaries at end or begining of sentence. - As such, We don't match embedded IDs: e.g., MyIDis555-555-5555 won't match the ID. This is to preven matching extremely nosiy imput that might have patterns of numbers in long strings. \"\"\" sw = all_stopwords.get(src_lang, {}) # if we are doing 'ID', we would still want to see if we catch an ADDRESS. # ADDRESS may have higher precedence, in which case it might overide an ID match. no_address = False if tag_type is not None and 'ID' in tag_type and 'ADDRESS' not in tag_type: no_address = True tag_type = set(list(tag_type)+['ADDRESS']) # if we are doing 'DATE' we would still want to do ID because they intersect. no_id = False if tag_type is not None and 'DATE' in tag_type and 'ID' not in tag_type: no_id = True tag_type = set(list(tag_type)+['ID']) # if we are doing 'AGE' we would still want to do DATE because they intersect. no_date = False if tag_type is not None and 'AGE' in tag_type and 'DATE' not in tag_type: no_date = True tag_type = set(list(tag_type)+['DATE']) is_cjk = src_lang in {'zh', 'zh-classical', 'zh-min-nan', 'zh-yue', 'ko', 'ja', 'th', 'jv'} if is_cjk: sentence_set = set(sentence.lower()) else: sentence_set = [] #let's do a sanity check. there should be no words beyond 100 chars. #this will really mess up our regexes. for word in sentence.split(\" \"): len_word = len(word) if len_word > 100: sentence = sentence.replace(word, \" \"*len_word) else: sentence_set.append(word.lower()) sentence_set = set([s.strip(rstrip_chars) for s in sentence_set]) all_ner = [] len_sentence = len(sentence) if all_regex is None: all_regex = regex_rulebase if tag_type is None: all_tags_to_check = list(all_regex.keys()) else: all_tags_to_check = list(tag_type) for tag in all_tags_to_check: regex_group = all_regex.get(tag) if not regex_group: continue for regex_context, extra_weight in [(a, 1) for a in regex_group.get(src_lang, [])] + [(a, 0) for a in regex_group.get(\"default\", [])]: if True: regex, context, block = regex_context #if this regex rule requires a context, find if it is satisified in general. this is a quick check. potential_context = False if context: for c1 in context: c1 = c1.lower() for c2 in c1.split(): c2 = c2.strip(rstrip_chars) if c2 in sentence_set: potential_context = True break if potential_context: break if not potential_context: continue #now apply regex for ent in list(set(list(regex.findall(sentence)))): #print (ent) if not isinstance(ent, str): continue ent = ent.strip() #ent = ent.rstrip(rstrip_chars) #ent = ent.lstrip(lstrip_chars) if not ent: continue ent_is_4_digit=False len_ent = len(ent) if len_ent == 4: try: int(ent) ent_is_4_digit=True except: ent_is_4_digit=False sentence2 = sentence delta = 0 #check to see if the ID or DATE is type of stdnum is_stdnum = False if tag in ('ID', 'DATE'): #simple length test ent_no_space = ent.replace(\" \", \"\").replace(\".\", \"\").replace(\"-\", \"\") if len(ent_no_space) > max_id_length and tag == 'ID': continue if len(ent_no_space) < min_id_length and tag == 'ID': continue #check if this is really a non PII stdnum, unless it's specifically an ID for a country using this src_lang. #TODO - complete the country to src_lang dict above. stnum_type = ent_2_stdnum_type(ent, src_lang) #if the stdnum is one of the non PII types, we will ignore it if prioritize_lang_match_over_ignore: is_stdnum = any(a for a in stnum_type if \".\" in a and src_lang in country_2_lang.get(a.split(\".\")[0], [])) if not ent_is_4_digit and not is_stdnum and any(a for a in stnum_type if a in ignore_stdnum_type): #a four digit entity might be a year, so don't skip this ent continue #this is actually an ID of known stdnum and not a DATE if any(a for a in stnum_type if a not in ignore_stdnum_type): tag = 'ID' is_stdnum = True #let's check the FIRST instance of this DATE or ID is really a date; #ideally we should do this for every instance of this ID if tag == 'DATE' or (tag == 'ID' and not is_stdnum): ent, tag = test_is_date(ent, tag, sentence, len_sentence, is_cjk, sentence.index(ent), src_lang, sw) if not ent: continue #do some confirmation for addresses if libpostal is installed. TODO, test if this works for zh. libpostal appears to test for pinyin. if tag == 'ADDRESS' and not parse_address: continue if tag == 'ADDRESS' and parse_address: address = parse_address(ent) if address and not any(ad for ad in address if ad[1] != 'house'): continue # this isn't an address if address and address[0][1] == 'house': address = address[1:] ent_lower = ent.lower() if address[0][0].lower() in ent_lower: ent = ent[ent_lower.index(address[0][0]):].strip(rstrip_chars) #print ('**', ent) if not ent or to_int (ent) is not None: continue # this isn't an address #TODO strip stopwords on either end of an ent for addresses - whether or not libpostal is installed else: pass #print ('problem with address', address) #print ('parse address', ent, '***', address) #now let's check context, block lists and turn all occurances of ent in this sentence into a span mention and also check for context and block words len_ent = len(ent) while True: if not ent or ent not in sentence2: break else: i = sentence2.index(ent) j = i + len_ent if potential_context or block: len_sentence2 = len(sentence2) left = \" \"+ sentence2[max(0, i - context_window) : i].replace(\",\", \" \").lower()+ \" \" right = \" \"+ sentence2[j : min(len_sentence2, j + context_window)].replace(\",\", \" \").lower() + \" \" found_context = False ent_lower = \" \"+ent.replace(\",\", \" \").lower()+ \" \" if context: for c in context: c = c.lower() if is_cjk: if c in left or c in right or c in ent_lower: found_context = True break else: if (\" \"+c+\" \" in left or \" \"+c+\" \" in right or \" \"+c+\" \" in ent_lower): found_context = True #print ('foound context', c) break else: found_context = True if block: for c in block: new_tag = None if type(c) is tuple: c, new_tag = c c = c.lower() if is_cjk: if c in left or c in right or c in ent_lower: if new_tag is not None: tag = new_tag #switching the tag to a subsumed tag. DATE=>AGE break else: found_context = False break else: if (\" \"+c+\" \" in left or \" \"+c+\" \" in right or \" \"+c+\" \" in ent_lower): if new_tag is not None: tag = new_tag #switching the tag to a subsumed tag. DATE=>AGE break else: found_context = False break if not found_context: delta += j sentence2 = sentence2[i+len(ent):] continue #check to see if the entity is really a standalone word or part of another longer word. # for example, we wont match a partial set of very long numbers as a 7 digit ID for example if is_cjk or ((i+delta == 0 or sentence2[i-1] in lstrip_chars) and (j+delta >= len_sentence-1 or sentence2[j] in rstrip_chars)): all_ner.append((ent, delta+i, delta+j, tag, extra_weight)) sentence2 = sentence2[i+len(ent):] delta += j all_ner = list(set(all_ner)) # let's remove overlapping # sort by length and position, favoring non-IDs first using the precedence list, # and additionaly giving one extra weight to language specific regex (as opposed to default rules). # NOTE: this doesn't do a perfect overlap match; just an overlap to the prior item. all_ner.sort(key=lambda a: a[1]+(1.0/(1.0+(100*((precedence.get(a[3], min(20,len(a[3])))+a[4]))+a[2]-a[1])))) #print (all_ner) if not tag_type or 'ID' in tag_type: # now do overlaps prefering longer ents, and higher prededence items over embedded IDs or dates, etc. all_ner2 = [] prev_mention = None for mention in all_ner: if prev_mention: if (prev_mention[1] == mention[1] and prev_mention[3] == mention[3] and prev_mention[4] > 0 and prev_mention[4] != mention[4]) or\\ (prev_mention[2] >= mention[1] and prev_mention[2] >= mention[2]): continue else: prev_mention = mention else: prev_mention = mention all_ner2.append(mention[:4]) all_ner = all_ner2 #TODO - refactor to check the tag_type list instead to do filtering. if no_address: all_ner = [a for a in all_ner if a[3] != 'ADDRESS'] if no_id: all_ner = [a for a in all_ner if a[3] != 'ID'] if no_date: all_ner = [a for a in all_ner if a[3] != 'DATE'] return all_ner","title":"detect_ner_with_regex_and_context()"},{"location":"reference/pii_manager/#pii_manager.ent_2_stdnum_type","text":"given a entity mention and the src_lang, determine potentially stdnum type Source code in src/pii_manager.py def ent_2_stdnum_type(text, src_lang=None): \"\"\" given a entity mention and the src_lang, determine potentially stdnum type \"\"\" stdnum_type = [] if src_lang is None: items = list(stdnum_mapper.items()) else: l1 = lang_2_stdnum.get(src_lang, []) + lang_2_stdnum.get('default', []) items = [(a1, stdnum_mapper[a1]) for a1 in l1] for ent_type, validate in items: try: found = validate(text) except: found = False if found: stdnum_type.append (ent_type) return stdnum_type","title":"ent_2_stdnum_type()"},{"location":"reference/pii_manager/#pii_manager.is_fast_date","text":"search for patterns like, yyyy-mm-dd, dd-mm-yyyy, yyyy-yyyy Source code in src/pii_manager.py def is_fast_date(ent, int_arr=None, year_start=1600, year_end=2050): \"\"\"search for patterns like, yyyy-mm-dd, dd-mm-yyyy, yyyy-yyyy \"\"\" if int_arr: len_int_arr = len(int_arr) if len_int_arr == 1 or len_int_arr > 3: return False if int_arr is None: ent_arr = ent.replace(\"/\", \"-\").replace(\" \",\"-\").replace(\".\",\"-\") if not (\"-\" in ent_arr and ent_arr.count(\"-\") <=2): return False int_arr = [(e, to_int(e)) for e in ent_arr.split(\"-\")] is_date = False has_year = has_month = has_day = 0 for e, val in int_arr: if val is None: break if (val <= year_end and val >= year_start): has_year +=1 elif val <= 12 and val >= 1: has_month += 1 elif val <= 31 and val >= 1: has_day += 1 else: return False if (has_year == 1 and has_month == 1) or \\ (has_year == 2 and has_month == 0 and has_day == 0) or \\ (has_year == 1 and has_month == 1 and has_day == 1): return True return False","title":"is_fast_date()"},{"location":"reference/pii_manager/#pii_manager.test_is_date","text":"Helper function used to test if an ent is a date or not We use dateparse to find context words around the ID/date to determine if its a date or not. For example, 100 AD is a date, but 100 might not be. Input :ent: an entity mention :tag: either ID or DATE :sentence: the context :is_cjk: if this is a Zh, Ja, Ko text :i: the position of ent in the sentence Returns: (ent, tag): potentially expanded ent, and the proper tag. Could return a potentially expanded ent, and the proper tag. Returns ent as None, if originally tagged as 'DATE' and it's not a DATE and we don't know what it is. Source code in src/pii_manager.py def test_is_date(ent, tag, sentence, len_sentence, is_cjk, i, src_lang, sw, year_start=1600, year_end=2050): \"\"\" Helper function used to test if an ent is a date or not We use dateparse to find context words around the ID/date to determine if its a date or not. For example, 100 AD is a date, but 100 might not be. Input: :ent: an entity mention :tag: either ID or DATE :sentence: the context :is_cjk: if this is a Zh, Ja, Ko text :i: the position of ent in the sentence Returns: (ent, tag): potentially expanded ent, and the proper tag. Could return a potentially expanded ent, and the proper tag. Returns ent as None, if originally tagged as 'DATE' and it's not a DATE and we don't know what it is. \"\"\" # perform some fast heuristics so we don't have to do dateparser len_ent = len(ent) if len_ent > 17 or (len_ent > 8 and to_int(ent)): if tag == 'DATE': #this is a very long number and not a date return None, tag else: #no need to check the date return ent, tag if not is_cjk: if i > 0 and sentence[i-1] not in lstrip_chars: if tag == 'DATE': return None, tag else: return ent, tag if i+len_ent < len_sentence - 1 and sentence[i+len_ent+1] not in rstrip_chars: if tag == 'DATE': return None, tag else: return ent, tag int_arr = [(e, to_int(e)) for e in ent.replace(\"/\", \"-\").replace(\" \",\"-\").replace(\".\",\"-\").split(\"-\")] if is_fast_date(ent, int_arr): #this is most likely a date return ent, 'DATE' for e, val in int_arr: if val is not None and len(e) > 8: if tag == 'DATE': #this is a very long number and not a date return None, tag #test if this is a 4 digit year. we need to confirm it's a real date is_date = False is_4_digit_year = False if tag == 'DATE' and len_ent == 4: e = to_int(ent) is_4_digit_year = (e <= year_end and e >= year_start) #now do dateparser if not is_4_digit_year: try: is_date = dateparser.parse(ent, languages=[date_parser_lang_mapper.get(src_lang,src_lang)]) # use src_lang to make it faster, languages=[src_lang]) except: is_date = dateparser.parse(ent, languages=[\"en\"]) if (not is_date and tag == 'DATE') or (is_date and tag == 'ID'): j = i + len_ent #for speed we can just use these 6 windows to check for a date. #but for completeness we could check a sliding window. #Maybe in some countries a year could #be in the middle of a date: Month Year Day ent_spans = [(-3,0), (-2, 0), (-1, 0), \\ (0, 3), (0, 2), (0, 1)] before = sentence[:i] after = sentence[j:] if before and not is_cjk and before[-1] not in lstrip_chars: is_date = False elif after and not is_cjk and after[0] not in rstrip_chars: is_date = False else: if not is_cjk: before = before.split() after = after.split() len_after = len(after) len_before = len(before) for before_words, after_words in ent_spans: if after_words > len_after: continue if -before_words > len_before: continue if before_words == 0: before1 = [] else: before1 = before[max(-len_before,before_words):] after1 = after[:min(len_after,after_words)] if is_cjk: ent2 = \"\".join(before1)+ent+\"\".join(after1) else: ent2 = \" \".join(before1)+\" \"+ent+\" \"+\" \".join(after1) if ent2.strip() == ent: continue try: is_date = dateparser.parse(ent2, languages=[date_parser_lang_mapper.get(src_lang,src_lang)])# use src_lang to make it faster, languages=[src_lang]) except: is_date = dateparser.parse(ent, languages=[\"en\"]) if is_date: #sometimes dateparser says things like \"in 2020\" is a date, which it is #but we want to strip out the stopwords. if before1 and before1[-1].lower() in sw: before1 = before1[:-1] if after1 and after1[0].lower() in sw: after1 = after1[1:] if is_cjk: ent2 = \"\".join(before1)+ent+\"\".join(after1) else: ent2 = \" \".join(before1)+\" \"+ent+\" \"+\" \".join(after1) ent = ent2.strip() tag = \"DATE\" return ent, tag if tag == 'DATE' and not is_date: return None, tag return ent, tag","title":"test_is_date()"},{"location":"reference/searcher_indexer/","text":"SearcherIndexer Bases: nn . Module Source code in src/searcher_indexer.py class SearcherIndexer(nn.Module): def __init__(self, filename=None, idx_dir=None, content_data_store=None, mmap_file=None, mmap_len=0, embed_dim=25, dtype=np.float16, \\ parents=None, parent_levels=None, parent_labels=None, skip_idxs=None, \\ parent2idx=None, top_parents=None, top_parent_idxs=None, clusters=None, embedder=\"minilm\", chunk_size=500, \\ embed_search_field=\"text\", bm25_field=None, downsampler=None, auto_embed_text=False, \\ auto_create_embeddings_idx=False, auto_create_bm25_idx=False, \\ span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, \\ min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, \\ universal_embed_mode = None, prototype_sentences=None, prototypes=None, universal_downsampler =None, min_num_prorotypes=50000, \\ use_tqdm=True, indexer=None ): #TODO, add a embedding_indexer. Given a batch of sentences, and an embedding, create additional embeddings corresponding to the batch. \"\"\" Cluster indexes and performs approximate nearest neighbor search on a memmap file. Also provides a wrapper for Whoosh BM25. :arg filename: Optional. The name of the file that is to be indexed and searched. Can be a txt or jsonl file or a gzip of the foregoing. :arg idx_dir: Optional. If not passed then it will be \"filename_idx\". If no filename is passed, then it will be the current directory. :arg content_data_store: Optional. The data store object, which can be a GzipFileByLineIdx, FileByLineIdx, or anything accessible by indexing content_data_store[i] and exposing len() which returns the number of items/lines. If filename is passed byt content_data_store is not passed, it will be created. :arg mmap_file: Optional, must be passed as a keyword argument. This is the file name for the embeddings representing each line in the gzip file. Used for embeddings search. :arg mmap_len Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg embed_dim Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg dtype Optional, must be passed as a keyword argument. This is the dtype of the mmap_file. :arg parents: Optional, must be passed as a keyword argument. This is a numpy or pytorch embedding of all the parents of the clusters] Where level 4 parents are the top level parents. This structure is used for approximage nearest neighbor search. :arg parent2idx: Optional, must be passed as a keyword argument. If parents are passed, this param must be also passed. It is a dict that maps the parent tuple to the index into the parents tensor :arg top_parents: Optional. The list of tuples representing the top parents. :arg top_parents_idxs: Optional. The index into the parents embedding for the top_parents. :arg clusters: Optional. A dictionary representing parent label -> [child indexes] :arg auto_create_embeddings_idx. Optional. Will create a cluster index from the contents of the mmap file. Assumes the mmap_file is populated. :arg auto_embed_text. Optional. Will populate the mmap_file from the data from filename/content_data_store. :arg auto_create_bm25_idx: Optional. Will do BM25 indexing of the contents of the file using whoosh, with stemming. :arg content_data_store Optional. The access for a file by lines. :arg embed_search_field: Optional. Defaults to \"text\". If the data is in jsonl format, this is the field that is Whoosh/bm25 indexed. :arg bm25_field: Optional. Can be different than the embed_search_field. If none, then will be set to the embed_search_field. :arg idxs: Optional. Only these idxs should be indexed and searched. :arg skip_idxs: Optional. The indexes that are empty and should not be searched or clustered. :arg content_data_store: Optional. :arg downsampler: Optional. The pythorch downsampler for mapping the output of the embedder to a lower dimension. :arg universal_embed_mode: Optional. Either None, \"assigned\", \"random\", or \"clusters\". If we should do universal embedding as described below, this will control how the prototypes are assigned. :arg prototype_sentences: Optional. A sorted list of sentences that represents the protoypes for embeddings space. If universal_embed_mode is set and prototypes are not provided,then this will be the level 0 parents sentences of the current clustering. To get universal embedding, we do cosine(target, prototypes), then normalize and then run through a universial_downsampler :arg protoypes: Optional. The embeddings in the embeddeing or (downsampled embedding) space that corresponds to the prototype_sentences. :arg min_num_prorotypes Optional. Will control the number of prototypes. :arg universal_downsampler Optional. The pythorch downsampler for mapping the output described above to a lower dimension that works across embedders and concept drift in the same embedder. maps from # of prototypes -> embed_dim. :arg indexer: Optional. If not set, then the BasicIndexer will be used. NOTE: Either pass in the parents, parent_levels, parent_labels, and parent2idx data is pased or clusters is passed. If none of these are passed and auto_create_embeddings_idx is set, then the data in the mmap file will be clustered and the data structure will be created. USAGE: for r in obj.search(\"test\"): print (r) for r in obj.search(numpy_or_pytorch_tensor): print (r) for r in obj.search(\"test\", numpy_or_pytorch_tensor): print (r) \"\"\" global device super().__init__() init_models(embedder) if indexer is None: indexer = BasicIndexer(embed_search_field=embed_search_field, bm25_field=bm25_field) self.embedder, self.indexer = embedder, indexer if idx_dir is None and filename is not None: idx_dir = f\"{filename}_idx\" elif idx_dir is None: idx_dir = \"./\" self.idx_dir = idx_dir if not os.path.exists(self.idx_dir): os.makedirs(self.idx_dir) if mmap_file is None: mmap_file = f\"{self.idx_dir}/search_index_{embed_search_field}_{embedder}_{embed_dim}.mmap\" if content_data_store is None: if filename is not None: if filename.endswith(\".gz\"): content_data_store = GzipByLineIdx.open(filename) else: content_data_store = FileByLineIdx(fobj=open(filename, \"rb\")) self.content_data_store = content_data_store if downsampler is None: model_embed_dim = get_model_embed_dim(embedder) downsampler = nn.Linear(model_embed_dim, embed_dim, bias=False).eval() if bm25_field is None: bm25_field = embed_search_field self.universal_embed_mode, self.mmap_file, self.mmap_len, self.embed_dim, self.dtype, self.clusters, self.parent2idx, self.parents, self.top_parents, self.top_parent_idxs, self.embed_search_field, self.bm25_field, self.downsampler = \\ universal_embed_mode, mmap_file, mmap_len, embed_dim, dtype, clusters, parent2idx, parents, top_parents, top_parent_idxs, embed_search_field, bm25_field, downsampler if self.mmap_len <= 0 and os.path.exists(self.mmap_file): mmap_len = self.mmap_len = get_np_mmap_length(self.mmap_file, [self.mmap_len, self.embed_dim], dtype=self.dtype, ) self.prototype_sentences, self.prototypes, self.universal_downsampler = prototype_sentences, prototypes, universal_downsampler if self.downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.downsampler = self.downsampler.half().eval().to(device) else: self.downsampler = self.downsampler.float().eval().to(device) if self.parents is not None: if self.dtype == np.float16: self.parents = self.parents.half().to(device) else: self.parents = self.parents.to(device) if skip_idxs is None: skip_idxs = [] self.skip_idxs = set(list(self.skip_idxs if hasattr(self, 'skip_idxs') and self.skip_idxs else []) + list(skip_idxs)) if universal_embed_mode not in (None, \"assigned\"): auto_embed_text = True if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) if universal_embed_mode not in (None, \"assigned\") and clusters is None: auto_create_embeddings_idx = True if os.path.exists(self.mmap_file) and (idxs is not None or auto_create_embeddings_idx): self.recreate_clusters_idx(clusters=self.clusters, span2cluster_label=span2cluster_label, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size) elif self.clusters: self.recreate_parents_data() if auto_create_bm25_idx and self.content_data_store: self.recreate_bm25_idx(auto_create_bm25_idx=auto_create_bm25_idx, idxs=idxs, use_tqdm=use_tqdm) setattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}', self.downsampler) setattr(self,f'clusters_{self.embed_search_field}_{self.embedder}_{self.embed_dim}', self.clusters) ## experimental universal embedding code self.universal_embed_mode = universal_embed_mode if universal_embed_mode: assert (prototypes is None and prototype_sentences is None and universal_downsampler is None) or universal_embed_mode == \"assigned\" if universal_embed_mode == \"random\": prototype_sentences = [get_content_from_line(self.content_data_store[i], embed_search_field) for i in random.sample(list(range(len(self.content_data_store)), min_num_prorotypes))] elif universal_embed_mode == \"cluster\": level_0_parents = [span[1] for span in self.parent2idx.keys() if span[0] == 0] prototype_sentences = [get_content_from_line(self.content_data_store[span[1]], embed_search_field) for span in level_0_parents] assert prototype_sentences if len(prototype_senences) > min_num_prorotypes: assert universal_embed_mode != \"assigned\" prototype_senences = random.sample(prototype_senences,min_num_prorotypes) elif len(prototype_senences) < min_num_prorotypes: assert universal_embed_mode != \"assigned\" prototype_sentences.extend([get_content_from_line(self.content_data_store[i], embed_search_field) for i in random.sample(list(range(len(self.content_data_store)), min_num_prorotypes-len(prorotype_senences)))]) prototypes = self.get_embeddings(prototype_sentences, universal_downsampler_mode=None) # we don't want to apply the universal downsampler, because it is use in that routine universal_downsampler = nn.Linear(len(prototype_sentences), embed_dim, bias=False) self.prototype_sentences, self.prototypes, self.universal_downsampler = prototype_sentences, prototypes, universal_downsampler if self.universal_downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.universal_downsampler = self.universal_downsampler.half().eval().to(device) else: self.universal_downsampler= self.universal_downsampler.eval().to(device) if self.prototypes is not None: if self.dtype == np.float16: self.prototypes = self.prototypes.half().to(device) else: self.prototypes = self.prototypes.to(device) #now re-create the embeddings, and remove the old embedder based embeddings since we won't use those anymore. os.system(f\"rm -rf {self.mmap_file}\") self.mmap_file = f\"{self.idx_dir}/search_index_{embed_search_field}_universal_{embed_dim}.mmap\" if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) self.recreate_parents_data() parents = self.parents del self.parents self.register_buffer('parents', parents) prototypes = self.prototypes del self.prototypes self.register_buffer('prototypes', prototypes) # get the downsampled sentence embeddings. can be used to train the downsampler(s). def forward(self, *args, **kwargs): with torch.no_grad(): if self.embedder == \"clip\": dat = clip_model.get_text_features(*args, **kwargs) elif self.embedder == \"minilm\": dat = minilm_model(*args, **kwargs) dat = mean_pooling(dat, kwargs['attention_mask']) elif self.embedder == \"doc2query\": dat = doc2query_encoder(*args, **kwargs) dat = mean_pooling(dat, kwargs['attention_mask']) elif self.embedder == \"codebert\": dat = codebert_model(*args, **kwargs) dat = mean_pooling(dat, kwargs['attention_mask']) elif self.embedder == \"labse\": dat = labse_model(*args, **kwargs).pooler_output dat = torch.nn.functional.normalize(dat, dim=1) dat = self.downsampler(dat) if self.universal_embed_mode: dat = cosine_similarity(dat, self.prototypes) dat = torch.nn.functional.normalize(dat, dim=1) dat = self.universal_downsampler(dat) return dat #switch to a different embedder for the same data. def switch_search_context(self, downsampler = None, mmap_file=None, embedder=\"minilm\", clusters=None, \\ span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, chunk_size=500, \\ parent2idx=None, parents=None, top_parents=None, top_parent_idxs=None, skip_idxs=None, \\ auto_embed_text=False,auto_create_embeddings_idx=False, auto_create_bm25_idx=False, \\ reuse_clusters=False, min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, use_tqdm=True ): global device init_models(embedder) if hasattr(self,f'downsampler_{self.embed_search_field}_{self.embedder}_{self.embed_dim}'): getattr(self,f'downsampler_{self.embed_search_field}_{self.embedder}_{self.embed_dim}').cpu() if hasattr(self, 'downsampler') and self.downsampler is not None: self.downsampler.cpu() content_data_store = self.content_data_store if self.universal_embed_mode == \"clustered\": clusters = self.clusters elif reuse_clusters: assert clusters is None clusters = self.clusters auto_create_embeddings_idx=False if mmap_file is None: if self.universal_embed_mode: mmap_file = f\"{self.idx_dir}/search_index_{self.embed_search_field}_universal_{self.embed_dim}.mmap\" auto_embed_text=not os.path.exists(self.mmap_file) # the universal embeddings are created once so don't recluster else: mmap_file = f\"{self.idx_dir}/search_index_{self.embed_search_field}_{embedder}_{self.embed_dim}.mmap\" if downsampler is None: if hasattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}'): downsampler = getattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}') else: model_embed_dim = get_model_embed_dim(embedder) downsampler = nn.Linear(model_embed_dim, self.embed_dim, bias=False).eval() if clusters is None: if hasattr(self,f'clusters_{self.embed_search_field}_{embedder}_{self.embed_dim}'): clusters = getattr(self,f'clusters_{self.embed_search_field}_{embedder}_{self.embed_dim}') self.embedder, self.mmap_file, self.clusters, self.parent2idx, self.parents, self.top_parents, self.top_parent_idxs, self.downsampler = \\ embedder, mmap_file, clusters, parent2idx, parents, top_parents, top_parent_idxs, downsampler if self.mmap_len <= 0 and os.path.exists(self.mmap_file): mmap_len = self.mmap_len = get_np_mmap_length(self.mmap_file, [self.mmap_len, self.embed_dim], dtype=self.dtype, ) if skip_idxs is None: skip_idxs = [] self.skip_idxs = set(list(self.skip_idxs if hasattr(self, 'skip_idxs') and self.skip_idxs else []) + list(skip_idxs)) if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) if os.path.exists(self.mmap_file) and (idxs is not None or auto_create_embeddings_idx): self.recreate_clusters_idx(clusters=self.clusters, span2cluster_label=span2cluster_label, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size) elif self.clusters: self.recreate_parents_data() if auto_create_bm25_idx and self.content_data_store: self.recreate_bm25_idx(auto_create_bm25_idx=auto_create_bm25_idx, idxs=idxs, use_tqdm=use_tqdm) if self.downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.downsampler = self.downsampler.half().eval().to(device) else: self.downsampler = self.downsampler.float().eval().to(device) if self.parents is not None: if device == 'cuda' and self.dtype == np.float16: self.parents = self.parents.half().to(device) else: self.parents = self.parents.to(device) #experimental universal embedding code if self.universal_embed_mode is not None and self.prototype_sentences: self.prototypes = self.get_embeddings(self.prototype_sentences) if self.prototypes is not None: if device == 'cuda' and self.dtype == np.float16: self.prototypes = self.prototypes.half().to(device) else: self.prototypes = self.prototypes.to(device) setattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}', self.downsampler) setattr(self,f'clusters_{self.embed_search_field}_{self.embedder}_{self.embed_dim}', self.clusters) # register the tensor variables in the pytorch method parents = self.parents del self.parents self.register_buffer('parents', parents) prototypes = self.prototypes del self.prototypes self.register_buffer('prototypes', prototypes) return self #get the sentence embedding for the sent or batch of sentences #temperature should be used with getting the target embeddings for search. def get_embeddings(self, sent_or_batch, temperature=None, universal_embed_mode=\"\"): return get_embeddings(sent_or_batch, downsampler=self.downsampler, dtype=self.dtype, embedder=self.embedder, \\ universal_embed_mode=self.universal_embed_mode if universal_embed_mode == \"\" else universal_embed_mode, prototypes=self.prototypes, \\ universal_downsampler=self.universal_downsampler,temperature=temperature) #embed all of self.content_data_store or (idx, content) for idx in idxs for the row/content from content_data_store #NOTE: We do not use the temperature here because we will compute the embeddings with temperature on the fly during searching def embed_text(self, start_idx=None, chunk_size=500, idxs=None, use_tqdm=True, auto_create_bm25_idx=False, **kwargs): assert self.content_data_store is not None if start_idx is None: start_idx = 0 embed_search_field = self.embed_search_field ### def content_data_store_reader(): content_data_store = self.content_data_store if hasattr(content_data_store, 'tell'): pos = content_data_store.tell() content_data_store.seek(0, 0) for l in content_data_store: yield get_content_from_line(l, embed_search_field) if hasattr(content_data_store, 'tell'): content_data_store.seek(pos,0) ### if idxs is not None: #TODO: #data_iterator = [(idx, self.indexer.process_one_line_for_embed_search(self.content_data_store[idx])) for idx in idxs] data_iterator = [(idx, get_content_from_line(self.content_data_store[idx], embed_search_field)) for idx in idxs] else: # TODO: # self.indexer.reset_embed_search_idx(0) # data_iterator = self.indexer.process_embed_search_field(data_iterator, **kwargs) data_iterator = content_data_store_reader() self.mmap_len, skip_idxs = embed_text(data_iterator, self.mmap_file, start_idx=start_idx, downsampler=self.downsampler, \\ mmap_len=self.mmap_len, embed_dim=self.embed_dim, embedder=self.embedder, chunk_size=chunk_size, use_tqdm=use_tqdm, \\ universal_embed_mode=self.universal_embed_mode, prototypes=self.prototypes, universal_downsampler=self.universal_downsampler) setattr(self,f'downsampler_{self.embed_search_field}_{self.embedder}_{self.embed_dim}', self.downsampler) self.skip_idxs = set(list(self.skip_idxs)+skip_idxs) #the below is probably in-efficient def recreate_parents_data(self): global device assert self.clusters all_parents = list(self.clusters.keys()) all_parents.sort(key=lambda a: a[0], reverse=True) max_level = all_parents[0][0] self.top_parents = [a for a in all_parents if a[0] == max_level] self.top_parent_idxs = [idx for idx, a in enumerate(all_parents) if a[0] == max_level] self.parent2idx = dict([(a,idx) for idx, a in enumerate(all_parents)]) self.parents = torch.from_numpy(np_memmap(self.mmap_file, shape=[self.mmap_len, self.embed_dim], dtype=self.dtype)[[a[1] for a in all_parents]]).to(device) #recreate the cluster index using the parameters def recreate_clusters_idx(self, clusters=None, span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, \\ min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000,): global device if clusters is None or idxs is not None: if clusters is None and idxs is not None: clusters = self.clusters clusters, _ = self.cluster(clusters=clusters, span2cluster_label=span2cluster_label, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size) #print (clusters) self.clusters = clusters self.recreate_parents_data() def get_cluster_and_span2cluster_label(self): span2cluster_label = {} for label, a_cluster in self.clusters: for span in a_cluster: span2cluster_label[span] = label return self.clusters, span2cluster_label def get_all_parents(self): return self.parent2idx.keys() def cluster(self, clusters=None, span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, \\ min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, use_tqdm=True): return create_hiearchical_clusters(clusters=clusters, span2cluster_label=span2cluster_label, mmap_file=self.mmap_file, \\ mmap_len=self.mmap_len, embed_dim=self.embed_dim, dtype=self.dtype, \\ skip_idxs=self.skip_idxs, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, \\ prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size, use_tqdm=use_tqdm) def recreate_bm25_idx(self, auto_create_bm25_idx=False, idxs=None, use_tqdm=True): assert self.content_data_store is not None content_data_store = self.content_data_store bm25_field = self.bm25_field schema = Schema(id=ID(stored=True), content=TEXT(analyzer=StemmingAnalyzer())) #TODO determine how to clear out the whoosh index besides rm -rf _M* MAIN* idx_dir = self.idx_dir os.system(f\"mkdir -p {idx_dir}/bm25_{bm25_field}\") need_reindex = auto_create_bm25_idx or not os.path.exists(f\"{idx_dir}/bm25_{bm25_field}/_MAIN_1.toc\") #CHECK IF THIS IS RIGHT if not need_reindex: self.whoosh_ix = whoosh_index.open_dir(f\"{idx_dir}/bm25_{bm25_field}\") else: self.whoosh_ix = create_in(f\"{idx_dir}/bm25_{bm25_field}\", schema) writer = self.whoosh_ix.writer(multisegment=True, limitmb=1024, procs=multiprocessing.cpu_count()) #writer = self.whoosh_ix.writer(multisegment=True, procs=multiprocessing.cpu_count()) if hasattr(content_data_store, 'tell'): pos = content_data_store.tell() content_data_store.seek(0, 0) if idxs is not None: idx_text_pairs = [(idx, self.content_data_store[idx]) for idx in idxs] if use_tqdm: data_iterator = tqdm.tqdm(idx_text_pairs) else: data_iterator = idx_text_pairs else: if use_tqdm: data_iterator = tqdm.tqdm(enumerate(content_data_store)) else: data_iterator = enumerate(content_data_store) # TODO: #self.indexer.reset_bm25_idx(0) #data_iterator = self.indexer.process_bm25_field(content_data_store, **kwargs) for idx, l in data_iterator: content= get_content_from_line(l, bm25_field) if not content: continue writer.add_document(id=str(idx), content=content) writer.commit() if hasattr(content_data_store, 'tell'): content_data_store.seek(pos,0) #search using embedding based and/or bm25 search. returns generator of a dict obj containing the results in 'id', 'text',and 'score. #if the underlying data is jsonl, then the result of the data will also be returned in the dict. #WARNING: we overwrite the 'id', 'score', and 'text' field, so we might want to use a different field name like f'{field_prefix}_score' #key_terms. TODO: See https://whoosh.readthedocs.io/en/latest/keywords.html #the temperature field will determine how broad or narrow to search for the target vector. def search(self, query=None, target=None, do_bm25_only=False, k=5, chunk_size=100, limit=None, search_temperature=None,): def _get_data(idx): l = self.content_data_store[idx] if type(l) is not str: dat = l.decode().replace(\"\\\\n\", \"\\n\").replace(\"\\\\t\", \"\\t\").strip() if dat[0] == \"{\" and dat[-1] == \"}\": try: dat = json.loads(l) except: pass return dat embedder = self.embedder if type(query) in (np.array, torch.Tensor): target = query query = None assert target is None or self.parents is not None if target is None and query is not None and hasattr(self, 'downsampler') and self.downsampler is not None: target = self.get_embeddings(query, temperature=search_temperature) if not hasattr(self, 'whoosh_ix') or self.whoosh_ix is None: query = None embedding_search_results = embeddings_search(target, mmap_file= self.mmap_file, mmap_len=self.mmap_len, embed_dim=self.embed_dim, dtype=self.dtype, \\ parents=self.parents, clusters=self.clusters, top_parent_idxs=self.top_parent_idxs, \\ top_parents=self.top_parents, parent2idx=self.parent2idx, k=k) if limit is None: cnt = 10^6 else: cnt = limit if query is not None: assert hasattr(self, 'whoosh_ix'), \"must be created with bm25 indexing\" with self.whoosh_ix.searcher() as searcher: if type(query) is str: query = QueryParser(\"content\", self.whoosh_ix.schema).parse(query) results = searcher.search(query, limit=limit) if target is None or do_bm25_only: for r in results: data = _get_data(int(r['id'])) if type(data) is dict: data['id'] = int(r['id']) yield data else: yield {'id': int(r['id']), 'text': data} cnt -= 1 if cnt <= 0: return else: idxs = [] key_terms = [] n_chunks = 0 for r in results: idxs.append(int(r['id'])) key_terms.append([]) # r.key_terms()) n_chunks += 1 if n_chunks > chunk_size: embedding_results = {} for _, r in zip(range(chunk_size), embedding_search_results): embedding_results[r[0]] = ([], r[1]) idxs = [idx for idx in idxs if idx not in embedding_results] embeddings = torch.from_numpy(np_memmap(self.mmap_file, shape=[self.mmap_len, self.embed_dim], dtype=self.dtype)[idxs]).to(device) results = cosine_similarity(target, embeddings) for idx, score, key_term in zip(idxs, results, key_terms): embedding_results[idx] = (key_term, score.item()) embedding_results = list(embedding_results.items()) embedding_results.sort(key=lambda a: a[1][1], reverse=True) for idx, score_keyterm in embedding_results: data = _get_data(idx) if type(data) is dict: data['id'] = idx data['score'] = score_keyterm[1] yield data else: yield {'id': idx, 'text': data, 'score': score_keyterm[1]} cnt -= 1 if cnt <= 0: return idxs = [] key_terms = [] n_chunk = 0 if idxs: embedding_results = {} for _, r in zip(range(chunk_size), embedding_search_results): embedding_results[r[0]] = ([], r[1]) idxs = [idx for idx in idxs if idx not in embedding_results] embeddings = torch.from_numpy(np_memmap(self.mmap_file, shape=[self.mmap_len, self.embed_dim], dtype=self.dtype)[idxs]).to(device) results = cosine_similarity(target, embeddings) for idx, score, key_term in zip(idxs, results, key_terms): embedding_results[idx] = (key_term, score.item()) embedding_results = list(embedding_results.items()) embedding_results.sort(key=lambda a: a[1][1], reverse=True) for idx, score_keyterm in embedding_results: data = _get_data(idx) if type(data) is dict: data['id'] = idx data['score'] = score_keyterm[1] yield data else: yield {'id': idx, 'text': data, 'score': score_keyterm[1]} cnt -= 1 if cnt <= 0: return #return any stragglers for r in embedding_search_results: data = _get_data(r[0]) if type(data) is dict: data['id'] = r[0] data['score'] = r[1] yield data else: yield {'id': r[0], 'text': data, 'score': r[1]} cnt -= 1 if cnt <= 0: return def save_pretrained(self, idx_dir=None): if idx_dir is not None: if self.idx_dir != idx_dir: os.system(f\"cp -rf {self.idx_dir} {idx_dir}\") else: idx_dir = self.idx_dir content_data_store = self.content_data_store mmap_file = self.mmap_file old_idx_dir = self.idx_dir self.idx_dir = None if self.mmap_file.startswith(old_idx_dir): self.mmap_file = self.mmap_file.split(\"/\")[-1] if hasattr(self, 'content_data_store') and self.content_data_store is not None: if type(self.content_data_store) is GzipByLineIdx: self.content_data_store = None elif type(self.content_data_store) is FileByLineIdx: fobj = self.content_data_store.fobj self.content_data_store.fobj = None device2 = \"cpu\" if self.downsampler is not None: device2 = next(self.downsampler.parameters()).device self.downsampler.cpu() for field in dir(self): if field.startswith(\"downsampler_\"): downsampler = getattr(self, field) if downsampler is not None: setattr(self, field, downsampler.cpu()) parents = self.parents self.parents = None torch.save(self, open(f\"{idx_dir}/search_index.pickle\", \"wb\")) self.mmap_file = mmap_file self.idx_dir = old_idx_dir self.content_data_store = content_data_store if self.downsampler is not None: self.downsampler.to(device2) self.parents = parents if type(self.content_data_store) is FileByLineIdx: self.content_data_store.fobj = fobj @staticmethod def from_pretrained(filename=None, idx_dir=None): global device assert idx_dir is not None or filename is not None if idx_dir is None: idx_dir = f\"{filename}_idx\" self = torch.load(open(f\"{idx_dir}/search_index.pickle\", \"rb\")) self.idx_dir = idx_dir if os.path.exists(f\"{idx_dir}/{self.mmap_file}\"): self.mmap_file = f\"{idx_dir}/{self.mmap_file}\" if filename: if filename.endswith(\".gz\"): self.content_data_store = GzipByLineIdx.open(filename) elif type(self.content_data_store) is FileByLineIdx: self.content_data_store.fobj=open(filename, \"rb\") self.downsampler.eval().to(device) if self.clusters: self.recreate_parents_data() if self.prototype_sentences and self.prototypes is None: self.prototypes = self.get_embeddings(self.prototype_sentences) parents = self.parents del self.parents self.register_buffer('parents', parents) prototypes = self.prototypes del self.prototypes self.register_buffer('prototypes', prototypes) return self __init__(filename=None, idx_dir=None, content_data_store=None, mmap_file=None, mmap_len=0, embed_dim=25, dtype=np.float16, parents=None, parent_levels=None, parent_labels=None, skip_idxs=None, parent2idx=None, top_parents=None, top_parent_idxs=None, clusters=None, embedder='minilm', chunk_size=500, embed_search_field='text', bm25_field=None, downsampler=None, auto_embed_text=False, auto_create_embeddings_idx=False, auto_create_bm25_idx=False, span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, universal_embed_mode=None, prototype_sentences=None, prototypes=None, universal_downsampler=None, min_num_prorotypes=50000, use_tqdm=True, indexer=None) Cluster indexes and performs approximate nearest neighbor search on a memmap file. Also provides a wrapper for Whoosh BM25. :arg filename: Optional. The name of the file that is to be indexed and searched. Can be a txt or jsonl file or a gzip of the foregoing. :arg idx_dir: Optional. If not passed then it will be \"filename_idx\". If no filename is passed, then it will be the current directory. :arg content_data_store: Optional. The data store object, which can be a GzipFileByLineIdx, FileByLineIdx, or anything accessible by indexing content_data_store[i] and exposing len() which returns the number of items/lines. If filename is passed byt content_data_store is not passed, it will be created. :arg mmap_file: Optional, must be passed as a keyword argument. This is the file name for the embeddings representing each line in the gzip file. Used for embeddings search. :arg mmap_len Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg embed_dim Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg dtype Optional, must be passed as a keyword argument. This is the dtype of the mmap_file. :arg parents: Optional, must be passed as a keyword argument. This is a numpy or pytorch embedding of all the parents of the clusters] Where level 4 parents are the top level parents. This structure is used for approximage nearest neighbor search. :arg parent2idx: Optional, must be passed as a keyword argument. If parents are passed, this param must be also passed. It is a dict that maps the parent tuple to the index into the parents tensor :arg top_parents: Optional. The list of tuples representing the top parents. :arg top_parents_idxs: Optional. The index into the parents embedding for the top_parents. :arg clusters: Optional. A dictionary representing parent label -> [child indexes] :arg auto_create_embeddings_idx. Optional. Will create a cluster index from the contents of the mmap file. Assumes the mmap_file is populated. :arg auto_embed_text. Optional. Will populate the mmap_file from the data from filename/content_data_store. :arg auto_create_bm25_idx: Optional. Will do BM25 indexing of the contents of the file using whoosh, with stemming. :arg content_data_store Optional. The access for a file by lines. :arg embed_search_field: Optional. Defaults to \"text\". If the data is in jsonl format, this is the field that is Whoosh/bm25 indexed. :arg bm25_field: Optional. Can be different than the embed_search_field. If none, then will be set to the embed_search_field. :arg idxs: Optional. Only these idxs should be indexed and searched. :arg skip_idxs: Optional. The indexes that are empty and should not be searched or clustered. :arg content_data_store: Optional. :arg downsampler: Optional. The pythorch downsampler for mapping the output of the embedder to a lower dimension. :arg universal_embed_mode: Optional. Either None, \"assigned\", \"random\", or \"clusters\". If we should do universal embedding as described below, this will control how the prototypes are assigned. :arg prototype_sentences: Optional. A sorted list of sentences that represents the protoypes for embeddings space. If universal_embed_mode is set and prototypes are not provided,then this will be the level 0 parents sentences of the current clustering. To get universal embedding, we do cosine(target, prototypes), then normalize and then run through a universial_downsampler :arg protoypes: Optional. The embeddings in the embeddeing or (downsampled embedding) space that corresponds to the prototype_sentences. :arg min_num_prorotypes Optional. Will control the number of prototypes. :arg universal_downsampler Optional. The pythorch downsampler for mapping the output described above to a lower dimension that works across embedders and concept drift in the same embedder. maps from # of prototypes -> embed_dim. :arg indexer: Optional. If not set, then the BasicIndexer will be used. Either pass in the parents, parent_levels, parent_labels, and parent2idx data is pased or clusters is passed. If none of these are passed and auto_create_embeddings_idx is set, then the data in the mmap file will be clustered and the data structure will be created. USAGE for r in obj.search(\"test\"): print (r) for r in obj.search(numpy_or_pytorch_tensor): print (r) for r in obj.search(\"test\", numpy_or_pytorch_tensor): print (r) Source code in src/searcher_indexer.py def __init__(self, filename=None, idx_dir=None, content_data_store=None, mmap_file=None, mmap_len=0, embed_dim=25, dtype=np.float16, \\ parents=None, parent_levels=None, parent_labels=None, skip_idxs=None, \\ parent2idx=None, top_parents=None, top_parent_idxs=None, clusters=None, embedder=\"minilm\", chunk_size=500, \\ embed_search_field=\"text\", bm25_field=None, downsampler=None, auto_embed_text=False, \\ auto_create_embeddings_idx=False, auto_create_bm25_idx=False, \\ span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, \\ min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, \\ universal_embed_mode = None, prototype_sentences=None, prototypes=None, universal_downsampler =None, min_num_prorotypes=50000, \\ use_tqdm=True, indexer=None ): #TODO, add a embedding_indexer. Given a batch of sentences, and an embedding, create additional embeddings corresponding to the batch. \"\"\" Cluster indexes and performs approximate nearest neighbor search on a memmap file. Also provides a wrapper for Whoosh BM25. :arg filename: Optional. The name of the file that is to be indexed and searched. Can be a txt or jsonl file or a gzip of the foregoing. :arg idx_dir: Optional. If not passed then it will be \"filename_idx\". If no filename is passed, then it will be the current directory. :arg content_data_store: Optional. The data store object, which can be a GzipFileByLineIdx, FileByLineIdx, or anything accessible by indexing content_data_store[i] and exposing len() which returns the number of items/lines. If filename is passed byt content_data_store is not passed, it will be created. :arg mmap_file: Optional, must be passed as a keyword argument. This is the file name for the embeddings representing each line in the gzip file. Used for embeddings search. :arg mmap_len Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg embed_dim Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg dtype Optional, must be passed as a keyword argument. This is the dtype of the mmap_file. :arg parents: Optional, must be passed as a keyword argument. This is a numpy or pytorch embedding of all the parents of the clusters] Where level 4 parents are the top level parents. This structure is used for approximage nearest neighbor search. :arg parent2idx: Optional, must be passed as a keyword argument. If parents are passed, this param must be also passed. It is a dict that maps the parent tuple to the index into the parents tensor :arg top_parents: Optional. The list of tuples representing the top parents. :arg top_parents_idxs: Optional. The index into the parents embedding for the top_parents. :arg clusters: Optional. A dictionary representing parent label -> [child indexes] :arg auto_create_embeddings_idx. Optional. Will create a cluster index from the contents of the mmap file. Assumes the mmap_file is populated. :arg auto_embed_text. Optional. Will populate the mmap_file from the data from filename/content_data_store. :arg auto_create_bm25_idx: Optional. Will do BM25 indexing of the contents of the file using whoosh, with stemming. :arg content_data_store Optional. The access for a file by lines. :arg embed_search_field: Optional. Defaults to \"text\". If the data is in jsonl format, this is the field that is Whoosh/bm25 indexed. :arg bm25_field: Optional. Can be different than the embed_search_field. If none, then will be set to the embed_search_field. :arg idxs: Optional. Only these idxs should be indexed and searched. :arg skip_idxs: Optional. The indexes that are empty and should not be searched or clustered. :arg content_data_store: Optional. :arg downsampler: Optional. The pythorch downsampler for mapping the output of the embedder to a lower dimension. :arg universal_embed_mode: Optional. Either None, \"assigned\", \"random\", or \"clusters\". If we should do universal embedding as described below, this will control how the prototypes are assigned. :arg prototype_sentences: Optional. A sorted list of sentences that represents the protoypes for embeddings space. If universal_embed_mode is set and prototypes are not provided,then this will be the level 0 parents sentences of the current clustering. To get universal embedding, we do cosine(target, prototypes), then normalize and then run through a universial_downsampler :arg protoypes: Optional. The embeddings in the embeddeing or (downsampled embedding) space that corresponds to the prototype_sentences. :arg min_num_prorotypes Optional. Will control the number of prototypes. :arg universal_downsampler Optional. The pythorch downsampler for mapping the output described above to a lower dimension that works across embedders and concept drift in the same embedder. maps from # of prototypes -> embed_dim. :arg indexer: Optional. If not set, then the BasicIndexer will be used. NOTE: Either pass in the parents, parent_levels, parent_labels, and parent2idx data is pased or clusters is passed. If none of these are passed and auto_create_embeddings_idx is set, then the data in the mmap file will be clustered and the data structure will be created. USAGE: for r in obj.search(\"test\"): print (r) for r in obj.search(numpy_or_pytorch_tensor): print (r) for r in obj.search(\"test\", numpy_or_pytorch_tensor): print (r) \"\"\" global device super().__init__() init_models(embedder) if indexer is None: indexer = BasicIndexer(embed_search_field=embed_search_field, bm25_field=bm25_field) self.embedder, self.indexer = embedder, indexer if idx_dir is None and filename is not None: idx_dir = f\"{filename}_idx\" elif idx_dir is None: idx_dir = \"./\" self.idx_dir = idx_dir if not os.path.exists(self.idx_dir): os.makedirs(self.idx_dir) if mmap_file is None: mmap_file = f\"{self.idx_dir}/search_index_{embed_search_field}_{embedder}_{embed_dim}.mmap\" if content_data_store is None: if filename is not None: if filename.endswith(\".gz\"): content_data_store = GzipByLineIdx.open(filename) else: content_data_store = FileByLineIdx(fobj=open(filename, \"rb\")) self.content_data_store = content_data_store if downsampler is None: model_embed_dim = get_model_embed_dim(embedder) downsampler = nn.Linear(model_embed_dim, embed_dim, bias=False).eval() if bm25_field is None: bm25_field = embed_search_field self.universal_embed_mode, self.mmap_file, self.mmap_len, self.embed_dim, self.dtype, self.clusters, self.parent2idx, self.parents, self.top_parents, self.top_parent_idxs, self.embed_search_field, self.bm25_field, self.downsampler = \\ universal_embed_mode, mmap_file, mmap_len, embed_dim, dtype, clusters, parent2idx, parents, top_parents, top_parent_idxs, embed_search_field, bm25_field, downsampler if self.mmap_len <= 0 and os.path.exists(self.mmap_file): mmap_len = self.mmap_len = get_np_mmap_length(self.mmap_file, [self.mmap_len, self.embed_dim], dtype=self.dtype, ) self.prototype_sentences, self.prototypes, self.universal_downsampler = prototype_sentences, prototypes, universal_downsampler if self.downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.downsampler = self.downsampler.half().eval().to(device) else: self.downsampler = self.downsampler.float().eval().to(device) if self.parents is not None: if self.dtype == np.float16: self.parents = self.parents.half().to(device) else: self.parents = self.parents.to(device) if skip_idxs is None: skip_idxs = [] self.skip_idxs = set(list(self.skip_idxs if hasattr(self, 'skip_idxs') and self.skip_idxs else []) + list(skip_idxs)) if universal_embed_mode not in (None, \"assigned\"): auto_embed_text = True if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) if universal_embed_mode not in (None, \"assigned\") and clusters is None: auto_create_embeddings_idx = True if os.path.exists(self.mmap_file) and (idxs is not None or auto_create_embeddings_idx): self.recreate_clusters_idx(clusters=self.clusters, span2cluster_label=span2cluster_label, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size) elif self.clusters: self.recreate_parents_data() if auto_create_bm25_idx and self.content_data_store: self.recreate_bm25_idx(auto_create_bm25_idx=auto_create_bm25_idx, idxs=idxs, use_tqdm=use_tqdm) setattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}', self.downsampler) setattr(self,f'clusters_{self.embed_search_field}_{self.embedder}_{self.embed_dim}', self.clusters) ## experimental universal embedding code self.universal_embed_mode = universal_embed_mode if universal_embed_mode: assert (prototypes is None and prototype_sentences is None and universal_downsampler is None) or universal_embed_mode == \"assigned\" if universal_embed_mode == \"random\": prototype_sentences = [get_content_from_line(self.content_data_store[i], embed_search_field) for i in random.sample(list(range(len(self.content_data_store)), min_num_prorotypes))] elif universal_embed_mode == \"cluster\": level_0_parents = [span[1] for span in self.parent2idx.keys() if span[0] == 0] prototype_sentences = [get_content_from_line(self.content_data_store[span[1]], embed_search_field) for span in level_0_parents] assert prototype_sentences if len(prototype_senences) > min_num_prorotypes: assert universal_embed_mode != \"assigned\" prototype_senences = random.sample(prototype_senences,min_num_prorotypes) elif len(prototype_senences) < min_num_prorotypes: assert universal_embed_mode != \"assigned\" prototype_sentences.extend([get_content_from_line(self.content_data_store[i], embed_search_field) for i in random.sample(list(range(len(self.content_data_store)), min_num_prorotypes-len(prorotype_senences)))]) prototypes = self.get_embeddings(prototype_sentences, universal_downsampler_mode=None) # we don't want to apply the universal downsampler, because it is use in that routine universal_downsampler = nn.Linear(len(prototype_sentences), embed_dim, bias=False) self.prototype_sentences, self.prototypes, self.universal_downsampler = prototype_sentences, prototypes, universal_downsampler if self.universal_downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.universal_downsampler = self.universal_downsampler.half().eval().to(device) else: self.universal_downsampler= self.universal_downsampler.eval().to(device) if self.prototypes is not None: if self.dtype == np.float16: self.prototypes = self.prototypes.half().to(device) else: self.prototypes = self.prototypes.to(device) #now re-create the embeddings, and remove the old embedder based embeddings since we won't use those anymore. os.system(f\"rm -rf {self.mmap_file}\") self.mmap_file = f\"{self.idx_dir}/search_index_{embed_search_field}_universal_{embed_dim}.mmap\" if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) self.recreate_parents_data() parents = self.parents del self.parents self.register_buffer('parents', parents) prototypes = self.prototypes del self.prototypes self.register_buffer('prototypes', prototypes)","title":"searcher_indexer"},{"location":"reference/searcher_indexer/#searcher_indexer.SearcherIndexer","text":"Bases: nn . Module Source code in src/searcher_indexer.py class SearcherIndexer(nn.Module): def __init__(self, filename=None, idx_dir=None, content_data_store=None, mmap_file=None, mmap_len=0, embed_dim=25, dtype=np.float16, \\ parents=None, parent_levels=None, parent_labels=None, skip_idxs=None, \\ parent2idx=None, top_parents=None, top_parent_idxs=None, clusters=None, embedder=\"minilm\", chunk_size=500, \\ embed_search_field=\"text\", bm25_field=None, downsampler=None, auto_embed_text=False, \\ auto_create_embeddings_idx=False, auto_create_bm25_idx=False, \\ span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, \\ min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, \\ universal_embed_mode = None, prototype_sentences=None, prototypes=None, universal_downsampler =None, min_num_prorotypes=50000, \\ use_tqdm=True, indexer=None ): #TODO, add a embedding_indexer. Given a batch of sentences, and an embedding, create additional embeddings corresponding to the batch. \"\"\" Cluster indexes and performs approximate nearest neighbor search on a memmap file. Also provides a wrapper for Whoosh BM25. :arg filename: Optional. The name of the file that is to be indexed and searched. Can be a txt or jsonl file or a gzip of the foregoing. :arg idx_dir: Optional. If not passed then it will be \"filename_idx\". If no filename is passed, then it will be the current directory. :arg content_data_store: Optional. The data store object, which can be a GzipFileByLineIdx, FileByLineIdx, or anything accessible by indexing content_data_store[i] and exposing len() which returns the number of items/lines. If filename is passed byt content_data_store is not passed, it will be created. :arg mmap_file: Optional, must be passed as a keyword argument. This is the file name for the embeddings representing each line in the gzip file. Used for embeddings search. :arg mmap_len Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg embed_dim Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg dtype Optional, must be passed as a keyword argument. This is the dtype of the mmap_file. :arg parents: Optional, must be passed as a keyword argument. This is a numpy or pytorch embedding of all the parents of the clusters] Where level 4 parents are the top level parents. This structure is used for approximage nearest neighbor search. :arg parent2idx: Optional, must be passed as a keyword argument. If parents are passed, this param must be also passed. It is a dict that maps the parent tuple to the index into the parents tensor :arg top_parents: Optional. The list of tuples representing the top parents. :arg top_parents_idxs: Optional. The index into the parents embedding for the top_parents. :arg clusters: Optional. A dictionary representing parent label -> [child indexes] :arg auto_create_embeddings_idx. Optional. Will create a cluster index from the contents of the mmap file. Assumes the mmap_file is populated. :arg auto_embed_text. Optional. Will populate the mmap_file from the data from filename/content_data_store. :arg auto_create_bm25_idx: Optional. Will do BM25 indexing of the contents of the file using whoosh, with stemming. :arg content_data_store Optional. The access for a file by lines. :arg embed_search_field: Optional. Defaults to \"text\". If the data is in jsonl format, this is the field that is Whoosh/bm25 indexed. :arg bm25_field: Optional. Can be different than the embed_search_field. If none, then will be set to the embed_search_field. :arg idxs: Optional. Only these idxs should be indexed and searched. :arg skip_idxs: Optional. The indexes that are empty and should not be searched or clustered. :arg content_data_store: Optional. :arg downsampler: Optional. The pythorch downsampler for mapping the output of the embedder to a lower dimension. :arg universal_embed_mode: Optional. Either None, \"assigned\", \"random\", or \"clusters\". If we should do universal embedding as described below, this will control how the prototypes are assigned. :arg prototype_sentences: Optional. A sorted list of sentences that represents the protoypes for embeddings space. If universal_embed_mode is set and prototypes are not provided,then this will be the level 0 parents sentences of the current clustering. To get universal embedding, we do cosine(target, prototypes), then normalize and then run through a universial_downsampler :arg protoypes: Optional. The embeddings in the embeddeing or (downsampled embedding) space that corresponds to the prototype_sentences. :arg min_num_prorotypes Optional. Will control the number of prototypes. :arg universal_downsampler Optional. The pythorch downsampler for mapping the output described above to a lower dimension that works across embedders and concept drift in the same embedder. maps from # of prototypes -> embed_dim. :arg indexer: Optional. If not set, then the BasicIndexer will be used. NOTE: Either pass in the parents, parent_levels, parent_labels, and parent2idx data is pased or clusters is passed. If none of these are passed and auto_create_embeddings_idx is set, then the data in the mmap file will be clustered and the data structure will be created. USAGE: for r in obj.search(\"test\"): print (r) for r in obj.search(numpy_or_pytorch_tensor): print (r) for r in obj.search(\"test\", numpy_or_pytorch_tensor): print (r) \"\"\" global device super().__init__() init_models(embedder) if indexer is None: indexer = BasicIndexer(embed_search_field=embed_search_field, bm25_field=bm25_field) self.embedder, self.indexer = embedder, indexer if idx_dir is None and filename is not None: idx_dir = f\"{filename}_idx\" elif idx_dir is None: idx_dir = \"./\" self.idx_dir = idx_dir if not os.path.exists(self.idx_dir): os.makedirs(self.idx_dir) if mmap_file is None: mmap_file = f\"{self.idx_dir}/search_index_{embed_search_field}_{embedder}_{embed_dim}.mmap\" if content_data_store is None: if filename is not None: if filename.endswith(\".gz\"): content_data_store = GzipByLineIdx.open(filename) else: content_data_store = FileByLineIdx(fobj=open(filename, \"rb\")) self.content_data_store = content_data_store if downsampler is None: model_embed_dim = get_model_embed_dim(embedder) downsampler = nn.Linear(model_embed_dim, embed_dim, bias=False).eval() if bm25_field is None: bm25_field = embed_search_field self.universal_embed_mode, self.mmap_file, self.mmap_len, self.embed_dim, self.dtype, self.clusters, self.parent2idx, self.parents, self.top_parents, self.top_parent_idxs, self.embed_search_field, self.bm25_field, self.downsampler = \\ universal_embed_mode, mmap_file, mmap_len, embed_dim, dtype, clusters, parent2idx, parents, top_parents, top_parent_idxs, embed_search_field, bm25_field, downsampler if self.mmap_len <= 0 and os.path.exists(self.mmap_file): mmap_len = self.mmap_len = get_np_mmap_length(self.mmap_file, [self.mmap_len, self.embed_dim], dtype=self.dtype, ) self.prototype_sentences, self.prototypes, self.universal_downsampler = prototype_sentences, prototypes, universal_downsampler if self.downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.downsampler = self.downsampler.half().eval().to(device) else: self.downsampler = self.downsampler.float().eval().to(device) if self.parents is not None: if self.dtype == np.float16: self.parents = self.parents.half().to(device) else: self.parents = self.parents.to(device) if skip_idxs is None: skip_idxs = [] self.skip_idxs = set(list(self.skip_idxs if hasattr(self, 'skip_idxs') and self.skip_idxs else []) + list(skip_idxs)) if universal_embed_mode not in (None, \"assigned\"): auto_embed_text = True if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) if universal_embed_mode not in (None, \"assigned\") and clusters is None: auto_create_embeddings_idx = True if os.path.exists(self.mmap_file) and (idxs is not None or auto_create_embeddings_idx): self.recreate_clusters_idx(clusters=self.clusters, span2cluster_label=span2cluster_label, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size) elif self.clusters: self.recreate_parents_data() if auto_create_bm25_idx and self.content_data_store: self.recreate_bm25_idx(auto_create_bm25_idx=auto_create_bm25_idx, idxs=idxs, use_tqdm=use_tqdm) setattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}', self.downsampler) setattr(self,f'clusters_{self.embed_search_field}_{self.embedder}_{self.embed_dim}', self.clusters) ## experimental universal embedding code self.universal_embed_mode = universal_embed_mode if universal_embed_mode: assert (prototypes is None and prototype_sentences is None and universal_downsampler is None) or universal_embed_mode == \"assigned\" if universal_embed_mode == \"random\": prototype_sentences = [get_content_from_line(self.content_data_store[i], embed_search_field) for i in random.sample(list(range(len(self.content_data_store)), min_num_prorotypes))] elif universal_embed_mode == \"cluster\": level_0_parents = [span[1] for span in self.parent2idx.keys() if span[0] == 0] prototype_sentences = [get_content_from_line(self.content_data_store[span[1]], embed_search_field) for span in level_0_parents] assert prototype_sentences if len(prototype_senences) > min_num_prorotypes: assert universal_embed_mode != \"assigned\" prototype_senences = random.sample(prototype_senences,min_num_prorotypes) elif len(prototype_senences) < min_num_prorotypes: assert universal_embed_mode != \"assigned\" prototype_sentences.extend([get_content_from_line(self.content_data_store[i], embed_search_field) for i in random.sample(list(range(len(self.content_data_store)), min_num_prorotypes-len(prorotype_senences)))]) prototypes = self.get_embeddings(prototype_sentences, universal_downsampler_mode=None) # we don't want to apply the universal downsampler, because it is use in that routine universal_downsampler = nn.Linear(len(prototype_sentences), embed_dim, bias=False) self.prototype_sentences, self.prototypes, self.universal_downsampler = prototype_sentences, prototypes, universal_downsampler if self.universal_downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.universal_downsampler = self.universal_downsampler.half().eval().to(device) else: self.universal_downsampler= self.universal_downsampler.eval().to(device) if self.prototypes is not None: if self.dtype == np.float16: self.prototypes = self.prototypes.half().to(device) else: self.prototypes = self.prototypes.to(device) #now re-create the embeddings, and remove the old embedder based embeddings since we won't use those anymore. os.system(f\"rm -rf {self.mmap_file}\") self.mmap_file = f\"{self.idx_dir}/search_index_{embed_search_field}_universal_{embed_dim}.mmap\" if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) self.recreate_parents_data() parents = self.parents del self.parents self.register_buffer('parents', parents) prototypes = self.prototypes del self.prototypes self.register_buffer('prototypes', prototypes) # get the downsampled sentence embeddings. can be used to train the downsampler(s). def forward(self, *args, **kwargs): with torch.no_grad(): if self.embedder == \"clip\": dat = clip_model.get_text_features(*args, **kwargs) elif self.embedder == \"minilm\": dat = minilm_model(*args, **kwargs) dat = mean_pooling(dat, kwargs['attention_mask']) elif self.embedder == \"doc2query\": dat = doc2query_encoder(*args, **kwargs) dat = mean_pooling(dat, kwargs['attention_mask']) elif self.embedder == \"codebert\": dat = codebert_model(*args, **kwargs) dat = mean_pooling(dat, kwargs['attention_mask']) elif self.embedder == \"labse\": dat = labse_model(*args, **kwargs).pooler_output dat = torch.nn.functional.normalize(dat, dim=1) dat = self.downsampler(dat) if self.universal_embed_mode: dat = cosine_similarity(dat, self.prototypes) dat = torch.nn.functional.normalize(dat, dim=1) dat = self.universal_downsampler(dat) return dat #switch to a different embedder for the same data. def switch_search_context(self, downsampler = None, mmap_file=None, embedder=\"minilm\", clusters=None, \\ span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, chunk_size=500, \\ parent2idx=None, parents=None, top_parents=None, top_parent_idxs=None, skip_idxs=None, \\ auto_embed_text=False,auto_create_embeddings_idx=False, auto_create_bm25_idx=False, \\ reuse_clusters=False, min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, use_tqdm=True ): global device init_models(embedder) if hasattr(self,f'downsampler_{self.embed_search_field}_{self.embedder}_{self.embed_dim}'): getattr(self,f'downsampler_{self.embed_search_field}_{self.embedder}_{self.embed_dim}').cpu() if hasattr(self, 'downsampler') and self.downsampler is not None: self.downsampler.cpu() content_data_store = self.content_data_store if self.universal_embed_mode == \"clustered\": clusters = self.clusters elif reuse_clusters: assert clusters is None clusters = self.clusters auto_create_embeddings_idx=False if mmap_file is None: if self.universal_embed_mode: mmap_file = f\"{self.idx_dir}/search_index_{self.embed_search_field}_universal_{self.embed_dim}.mmap\" auto_embed_text=not os.path.exists(self.mmap_file) # the universal embeddings are created once so don't recluster else: mmap_file = f\"{self.idx_dir}/search_index_{self.embed_search_field}_{embedder}_{self.embed_dim}.mmap\" if downsampler is None: if hasattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}'): downsampler = getattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}') else: model_embed_dim = get_model_embed_dim(embedder) downsampler = nn.Linear(model_embed_dim, self.embed_dim, bias=False).eval() if clusters is None: if hasattr(self,f'clusters_{self.embed_search_field}_{embedder}_{self.embed_dim}'): clusters = getattr(self,f'clusters_{self.embed_search_field}_{embedder}_{self.embed_dim}') self.embedder, self.mmap_file, self.clusters, self.parent2idx, self.parents, self.top_parents, self.top_parent_idxs, self.downsampler = \\ embedder, mmap_file, clusters, parent2idx, parents, top_parents, top_parent_idxs, downsampler if self.mmap_len <= 0 and os.path.exists(self.mmap_file): mmap_len = self.mmap_len = get_np_mmap_length(self.mmap_file, [self.mmap_len, self.embed_dim], dtype=self.dtype, ) if skip_idxs is None: skip_idxs = [] self.skip_idxs = set(list(self.skip_idxs if hasattr(self, 'skip_idxs') and self.skip_idxs else []) + list(skip_idxs)) if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) if os.path.exists(self.mmap_file) and (idxs is not None or auto_create_embeddings_idx): self.recreate_clusters_idx(clusters=self.clusters, span2cluster_label=span2cluster_label, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size) elif self.clusters: self.recreate_parents_data() if auto_create_bm25_idx and self.content_data_store: self.recreate_bm25_idx(auto_create_bm25_idx=auto_create_bm25_idx, idxs=idxs, use_tqdm=use_tqdm) if self.downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.downsampler = self.downsampler.half().eval().to(device) else: self.downsampler = self.downsampler.float().eval().to(device) if self.parents is not None: if device == 'cuda' and self.dtype == np.float16: self.parents = self.parents.half().to(device) else: self.parents = self.parents.to(device) #experimental universal embedding code if self.universal_embed_mode is not None and self.prototype_sentences: self.prototypes = self.get_embeddings(self.prototype_sentences) if self.prototypes is not None: if device == 'cuda' and self.dtype == np.float16: self.prototypes = self.prototypes.half().to(device) else: self.prototypes = self.prototypes.to(device) setattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}', self.downsampler) setattr(self,f'clusters_{self.embed_search_field}_{self.embedder}_{self.embed_dim}', self.clusters) # register the tensor variables in the pytorch method parents = self.parents del self.parents self.register_buffer('parents', parents) prototypes = self.prototypes del self.prototypes self.register_buffer('prototypes', prototypes) return self #get the sentence embedding for the sent or batch of sentences #temperature should be used with getting the target embeddings for search. def get_embeddings(self, sent_or_batch, temperature=None, universal_embed_mode=\"\"): return get_embeddings(sent_or_batch, downsampler=self.downsampler, dtype=self.dtype, embedder=self.embedder, \\ universal_embed_mode=self.universal_embed_mode if universal_embed_mode == \"\" else universal_embed_mode, prototypes=self.prototypes, \\ universal_downsampler=self.universal_downsampler,temperature=temperature) #embed all of self.content_data_store or (idx, content) for idx in idxs for the row/content from content_data_store #NOTE: We do not use the temperature here because we will compute the embeddings with temperature on the fly during searching def embed_text(self, start_idx=None, chunk_size=500, idxs=None, use_tqdm=True, auto_create_bm25_idx=False, **kwargs): assert self.content_data_store is not None if start_idx is None: start_idx = 0 embed_search_field = self.embed_search_field ### def content_data_store_reader(): content_data_store = self.content_data_store if hasattr(content_data_store, 'tell'): pos = content_data_store.tell() content_data_store.seek(0, 0) for l in content_data_store: yield get_content_from_line(l, embed_search_field) if hasattr(content_data_store, 'tell'): content_data_store.seek(pos,0) ### if idxs is not None: #TODO: #data_iterator = [(idx, self.indexer.process_one_line_for_embed_search(self.content_data_store[idx])) for idx in idxs] data_iterator = [(idx, get_content_from_line(self.content_data_store[idx], embed_search_field)) for idx in idxs] else: # TODO: # self.indexer.reset_embed_search_idx(0) # data_iterator = self.indexer.process_embed_search_field(data_iterator, **kwargs) data_iterator = content_data_store_reader() self.mmap_len, skip_idxs = embed_text(data_iterator, self.mmap_file, start_idx=start_idx, downsampler=self.downsampler, \\ mmap_len=self.mmap_len, embed_dim=self.embed_dim, embedder=self.embedder, chunk_size=chunk_size, use_tqdm=use_tqdm, \\ universal_embed_mode=self.universal_embed_mode, prototypes=self.prototypes, universal_downsampler=self.universal_downsampler) setattr(self,f'downsampler_{self.embed_search_field}_{self.embedder}_{self.embed_dim}', self.downsampler) self.skip_idxs = set(list(self.skip_idxs)+skip_idxs) #the below is probably in-efficient def recreate_parents_data(self): global device assert self.clusters all_parents = list(self.clusters.keys()) all_parents.sort(key=lambda a: a[0], reverse=True) max_level = all_parents[0][0] self.top_parents = [a for a in all_parents if a[0] == max_level] self.top_parent_idxs = [idx for idx, a in enumerate(all_parents) if a[0] == max_level] self.parent2idx = dict([(a,idx) for idx, a in enumerate(all_parents)]) self.parents = torch.from_numpy(np_memmap(self.mmap_file, shape=[self.mmap_len, self.embed_dim], dtype=self.dtype)[[a[1] for a in all_parents]]).to(device) #recreate the cluster index using the parameters def recreate_clusters_idx(self, clusters=None, span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, \\ min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000,): global device if clusters is None or idxs is not None: if clusters is None and idxs is not None: clusters = self.clusters clusters, _ = self.cluster(clusters=clusters, span2cluster_label=span2cluster_label, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size) #print (clusters) self.clusters = clusters self.recreate_parents_data() def get_cluster_and_span2cluster_label(self): span2cluster_label = {} for label, a_cluster in self.clusters: for span in a_cluster: span2cluster_label[span] = label return self.clusters, span2cluster_label def get_all_parents(self): return self.parent2idx.keys() def cluster(self, clusters=None, span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, \\ min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, use_tqdm=True): return create_hiearchical_clusters(clusters=clusters, span2cluster_label=span2cluster_label, mmap_file=self.mmap_file, \\ mmap_len=self.mmap_len, embed_dim=self.embed_dim, dtype=self.dtype, \\ skip_idxs=self.skip_idxs, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, \\ prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size, use_tqdm=use_tqdm) def recreate_bm25_idx(self, auto_create_bm25_idx=False, idxs=None, use_tqdm=True): assert self.content_data_store is not None content_data_store = self.content_data_store bm25_field = self.bm25_field schema = Schema(id=ID(stored=True), content=TEXT(analyzer=StemmingAnalyzer())) #TODO determine how to clear out the whoosh index besides rm -rf _M* MAIN* idx_dir = self.idx_dir os.system(f\"mkdir -p {idx_dir}/bm25_{bm25_field}\") need_reindex = auto_create_bm25_idx or not os.path.exists(f\"{idx_dir}/bm25_{bm25_field}/_MAIN_1.toc\") #CHECK IF THIS IS RIGHT if not need_reindex: self.whoosh_ix = whoosh_index.open_dir(f\"{idx_dir}/bm25_{bm25_field}\") else: self.whoosh_ix = create_in(f\"{idx_dir}/bm25_{bm25_field}\", schema) writer = self.whoosh_ix.writer(multisegment=True, limitmb=1024, procs=multiprocessing.cpu_count()) #writer = self.whoosh_ix.writer(multisegment=True, procs=multiprocessing.cpu_count()) if hasattr(content_data_store, 'tell'): pos = content_data_store.tell() content_data_store.seek(0, 0) if idxs is not None: idx_text_pairs = [(idx, self.content_data_store[idx]) for idx in idxs] if use_tqdm: data_iterator = tqdm.tqdm(idx_text_pairs) else: data_iterator = idx_text_pairs else: if use_tqdm: data_iterator = tqdm.tqdm(enumerate(content_data_store)) else: data_iterator = enumerate(content_data_store) # TODO: #self.indexer.reset_bm25_idx(0) #data_iterator = self.indexer.process_bm25_field(content_data_store, **kwargs) for idx, l in data_iterator: content= get_content_from_line(l, bm25_field) if not content: continue writer.add_document(id=str(idx), content=content) writer.commit() if hasattr(content_data_store, 'tell'): content_data_store.seek(pos,0) #search using embedding based and/or bm25 search. returns generator of a dict obj containing the results in 'id', 'text',and 'score. #if the underlying data is jsonl, then the result of the data will also be returned in the dict. #WARNING: we overwrite the 'id', 'score', and 'text' field, so we might want to use a different field name like f'{field_prefix}_score' #key_terms. TODO: See https://whoosh.readthedocs.io/en/latest/keywords.html #the temperature field will determine how broad or narrow to search for the target vector. def search(self, query=None, target=None, do_bm25_only=False, k=5, chunk_size=100, limit=None, search_temperature=None,): def _get_data(idx): l = self.content_data_store[idx] if type(l) is not str: dat = l.decode().replace(\"\\\\n\", \"\\n\").replace(\"\\\\t\", \"\\t\").strip() if dat[0] == \"{\" and dat[-1] == \"}\": try: dat = json.loads(l) except: pass return dat embedder = self.embedder if type(query) in (np.array, torch.Tensor): target = query query = None assert target is None or self.parents is not None if target is None and query is not None and hasattr(self, 'downsampler') and self.downsampler is not None: target = self.get_embeddings(query, temperature=search_temperature) if not hasattr(self, 'whoosh_ix') or self.whoosh_ix is None: query = None embedding_search_results = embeddings_search(target, mmap_file= self.mmap_file, mmap_len=self.mmap_len, embed_dim=self.embed_dim, dtype=self.dtype, \\ parents=self.parents, clusters=self.clusters, top_parent_idxs=self.top_parent_idxs, \\ top_parents=self.top_parents, parent2idx=self.parent2idx, k=k) if limit is None: cnt = 10^6 else: cnt = limit if query is not None: assert hasattr(self, 'whoosh_ix'), \"must be created with bm25 indexing\" with self.whoosh_ix.searcher() as searcher: if type(query) is str: query = QueryParser(\"content\", self.whoosh_ix.schema).parse(query) results = searcher.search(query, limit=limit) if target is None or do_bm25_only: for r in results: data = _get_data(int(r['id'])) if type(data) is dict: data['id'] = int(r['id']) yield data else: yield {'id': int(r['id']), 'text': data} cnt -= 1 if cnt <= 0: return else: idxs = [] key_terms = [] n_chunks = 0 for r in results: idxs.append(int(r['id'])) key_terms.append([]) # r.key_terms()) n_chunks += 1 if n_chunks > chunk_size: embedding_results = {} for _, r in zip(range(chunk_size), embedding_search_results): embedding_results[r[0]] = ([], r[1]) idxs = [idx for idx in idxs if idx not in embedding_results] embeddings = torch.from_numpy(np_memmap(self.mmap_file, shape=[self.mmap_len, self.embed_dim], dtype=self.dtype)[idxs]).to(device) results = cosine_similarity(target, embeddings) for idx, score, key_term in zip(idxs, results, key_terms): embedding_results[idx] = (key_term, score.item()) embedding_results = list(embedding_results.items()) embedding_results.sort(key=lambda a: a[1][1], reverse=True) for idx, score_keyterm in embedding_results: data = _get_data(idx) if type(data) is dict: data['id'] = idx data['score'] = score_keyterm[1] yield data else: yield {'id': idx, 'text': data, 'score': score_keyterm[1]} cnt -= 1 if cnt <= 0: return idxs = [] key_terms = [] n_chunk = 0 if idxs: embedding_results = {} for _, r in zip(range(chunk_size), embedding_search_results): embedding_results[r[0]] = ([], r[1]) idxs = [idx for idx in idxs if idx not in embedding_results] embeddings = torch.from_numpy(np_memmap(self.mmap_file, shape=[self.mmap_len, self.embed_dim], dtype=self.dtype)[idxs]).to(device) results = cosine_similarity(target, embeddings) for idx, score, key_term in zip(idxs, results, key_terms): embedding_results[idx] = (key_term, score.item()) embedding_results = list(embedding_results.items()) embedding_results.sort(key=lambda a: a[1][1], reverse=True) for idx, score_keyterm in embedding_results: data = _get_data(idx) if type(data) is dict: data['id'] = idx data['score'] = score_keyterm[1] yield data else: yield {'id': idx, 'text': data, 'score': score_keyterm[1]} cnt -= 1 if cnt <= 0: return #return any stragglers for r in embedding_search_results: data = _get_data(r[0]) if type(data) is dict: data['id'] = r[0] data['score'] = r[1] yield data else: yield {'id': r[0], 'text': data, 'score': r[1]} cnt -= 1 if cnt <= 0: return def save_pretrained(self, idx_dir=None): if idx_dir is not None: if self.idx_dir != idx_dir: os.system(f\"cp -rf {self.idx_dir} {idx_dir}\") else: idx_dir = self.idx_dir content_data_store = self.content_data_store mmap_file = self.mmap_file old_idx_dir = self.idx_dir self.idx_dir = None if self.mmap_file.startswith(old_idx_dir): self.mmap_file = self.mmap_file.split(\"/\")[-1] if hasattr(self, 'content_data_store') and self.content_data_store is not None: if type(self.content_data_store) is GzipByLineIdx: self.content_data_store = None elif type(self.content_data_store) is FileByLineIdx: fobj = self.content_data_store.fobj self.content_data_store.fobj = None device2 = \"cpu\" if self.downsampler is not None: device2 = next(self.downsampler.parameters()).device self.downsampler.cpu() for field in dir(self): if field.startswith(\"downsampler_\"): downsampler = getattr(self, field) if downsampler is not None: setattr(self, field, downsampler.cpu()) parents = self.parents self.parents = None torch.save(self, open(f\"{idx_dir}/search_index.pickle\", \"wb\")) self.mmap_file = mmap_file self.idx_dir = old_idx_dir self.content_data_store = content_data_store if self.downsampler is not None: self.downsampler.to(device2) self.parents = parents if type(self.content_data_store) is FileByLineIdx: self.content_data_store.fobj = fobj @staticmethod def from_pretrained(filename=None, idx_dir=None): global device assert idx_dir is not None or filename is not None if idx_dir is None: idx_dir = f\"{filename}_idx\" self = torch.load(open(f\"{idx_dir}/search_index.pickle\", \"rb\")) self.idx_dir = idx_dir if os.path.exists(f\"{idx_dir}/{self.mmap_file}\"): self.mmap_file = f\"{idx_dir}/{self.mmap_file}\" if filename: if filename.endswith(\".gz\"): self.content_data_store = GzipByLineIdx.open(filename) elif type(self.content_data_store) is FileByLineIdx: self.content_data_store.fobj=open(filename, \"rb\") self.downsampler.eval().to(device) if self.clusters: self.recreate_parents_data() if self.prototype_sentences and self.prototypes is None: self.prototypes = self.get_embeddings(self.prototype_sentences) parents = self.parents del self.parents self.register_buffer('parents', parents) prototypes = self.prototypes del self.prototypes self.register_buffer('prototypes', prototypes) return self","title":"SearcherIndexer"},{"location":"reference/searcher_indexer/#searcher_indexer.SearcherIndexer.__init__","text":"Cluster indexes and performs approximate nearest neighbor search on a memmap file. Also provides a wrapper for Whoosh BM25. :arg filename: Optional. The name of the file that is to be indexed and searched. Can be a txt or jsonl file or a gzip of the foregoing. :arg idx_dir: Optional. If not passed then it will be \"filename_idx\". If no filename is passed, then it will be the current directory. :arg content_data_store: Optional. The data store object, which can be a GzipFileByLineIdx, FileByLineIdx, or anything accessible by indexing content_data_store[i] and exposing len() which returns the number of items/lines. If filename is passed byt content_data_store is not passed, it will be created. :arg mmap_file: Optional, must be passed as a keyword argument. This is the file name for the embeddings representing each line in the gzip file. Used for embeddings search. :arg mmap_len Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg embed_dim Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg dtype Optional, must be passed as a keyword argument. This is the dtype of the mmap_file. :arg parents: Optional, must be passed as a keyword argument. This is a numpy or pytorch embedding of all the parents of the clusters] Where level 4 parents are the top level parents. This structure is used for approximage nearest neighbor search. :arg parent2idx: Optional, must be passed as a keyword argument. If parents are passed, this param must be also passed. It is a dict that maps the parent tuple to the index into the parents tensor :arg top_parents: Optional. The list of tuples representing the top parents. :arg top_parents_idxs: Optional. The index into the parents embedding for the top_parents. :arg clusters: Optional. A dictionary representing parent label -> [child indexes] :arg auto_create_embeddings_idx. Optional. Will create a cluster index from the contents of the mmap file. Assumes the mmap_file is populated. :arg auto_embed_text. Optional. Will populate the mmap_file from the data from filename/content_data_store. :arg auto_create_bm25_idx: Optional. Will do BM25 indexing of the contents of the file using whoosh, with stemming. :arg content_data_store Optional. The access for a file by lines. :arg embed_search_field: Optional. Defaults to \"text\". If the data is in jsonl format, this is the field that is Whoosh/bm25 indexed. :arg bm25_field: Optional. Can be different than the embed_search_field. If none, then will be set to the embed_search_field. :arg idxs: Optional. Only these idxs should be indexed and searched. :arg skip_idxs: Optional. The indexes that are empty and should not be searched or clustered. :arg content_data_store: Optional. :arg downsampler: Optional. The pythorch downsampler for mapping the output of the embedder to a lower dimension. :arg universal_embed_mode: Optional. Either None, \"assigned\", \"random\", or \"clusters\". If we should do universal embedding as described below, this will control how the prototypes are assigned. :arg prototype_sentences: Optional. A sorted list of sentences that represents the protoypes for embeddings space. If universal_embed_mode is set and prototypes are not provided,then this will be the level 0 parents sentences of the current clustering. To get universal embedding, we do cosine(target, prototypes), then normalize and then run through a universial_downsampler :arg protoypes: Optional. The embeddings in the embeddeing or (downsampled embedding) space that corresponds to the prototype_sentences. :arg min_num_prorotypes Optional. Will control the number of prototypes. :arg universal_downsampler Optional. The pythorch downsampler for mapping the output described above to a lower dimension that works across embedders and concept drift in the same embedder. maps from # of prototypes -> embed_dim. :arg indexer: Optional. If not set, then the BasicIndexer will be used. Either pass in the parents, parent_levels, parent_labels, and parent2idx data is pased or clusters is passed. If none of these are passed and auto_create_embeddings_idx is set, then the data in the mmap file will be clustered and the data structure will be created. USAGE for r in obj.search(\"test\"): print (r) for r in obj.search(numpy_or_pytorch_tensor): print (r) for r in obj.search(\"test\", numpy_or_pytorch_tensor): print (r) Source code in src/searcher_indexer.py def __init__(self, filename=None, idx_dir=None, content_data_store=None, mmap_file=None, mmap_len=0, embed_dim=25, dtype=np.float16, \\ parents=None, parent_levels=None, parent_labels=None, skip_idxs=None, \\ parent2idx=None, top_parents=None, top_parent_idxs=None, clusters=None, embedder=\"minilm\", chunk_size=500, \\ embed_search_field=\"text\", bm25_field=None, downsampler=None, auto_embed_text=False, \\ auto_create_embeddings_idx=False, auto_create_bm25_idx=False, \\ span2cluster_label=None, idxs=None, max_level=4, max_cluster_size=200, \\ min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, \\ universal_embed_mode = None, prototype_sentences=None, prototypes=None, universal_downsampler =None, min_num_prorotypes=50000, \\ use_tqdm=True, indexer=None ): #TODO, add a embedding_indexer. Given a batch of sentences, and an embedding, create additional embeddings corresponding to the batch. \"\"\" Cluster indexes and performs approximate nearest neighbor search on a memmap file. Also provides a wrapper for Whoosh BM25. :arg filename: Optional. The name of the file that is to be indexed and searched. Can be a txt or jsonl file or a gzip of the foregoing. :arg idx_dir: Optional. If not passed then it will be \"filename_idx\". If no filename is passed, then it will be the current directory. :arg content_data_store: Optional. The data store object, which can be a GzipFileByLineIdx, FileByLineIdx, or anything accessible by indexing content_data_store[i] and exposing len() which returns the number of items/lines. If filename is passed byt content_data_store is not passed, it will be created. :arg mmap_file: Optional, must be passed as a keyword argument. This is the file name for the embeddings representing each line in the gzip file. Used for embeddings search. :arg mmap_len Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg embed_dim Optional, must be passed as a keyword argument if mmap_file is passed. This is the shape of the mmap_file. :arg dtype Optional, must be passed as a keyword argument. This is the dtype of the mmap_file. :arg parents: Optional, must be passed as a keyword argument. This is a numpy or pytorch embedding of all the parents of the clusters] Where level 4 parents are the top level parents. This structure is used for approximage nearest neighbor search. :arg parent2idx: Optional, must be passed as a keyword argument. If parents are passed, this param must be also passed. It is a dict that maps the parent tuple to the index into the parents tensor :arg top_parents: Optional. The list of tuples representing the top parents. :arg top_parents_idxs: Optional. The index into the parents embedding for the top_parents. :arg clusters: Optional. A dictionary representing parent label -> [child indexes] :arg auto_create_embeddings_idx. Optional. Will create a cluster index from the contents of the mmap file. Assumes the mmap_file is populated. :arg auto_embed_text. Optional. Will populate the mmap_file from the data from filename/content_data_store. :arg auto_create_bm25_idx: Optional. Will do BM25 indexing of the contents of the file using whoosh, with stemming. :arg content_data_store Optional. The access for a file by lines. :arg embed_search_field: Optional. Defaults to \"text\". If the data is in jsonl format, this is the field that is Whoosh/bm25 indexed. :arg bm25_field: Optional. Can be different than the embed_search_field. If none, then will be set to the embed_search_field. :arg idxs: Optional. Only these idxs should be indexed and searched. :arg skip_idxs: Optional. The indexes that are empty and should not be searched or clustered. :arg content_data_store: Optional. :arg downsampler: Optional. The pythorch downsampler for mapping the output of the embedder to a lower dimension. :arg universal_embed_mode: Optional. Either None, \"assigned\", \"random\", or \"clusters\". If we should do universal embedding as described below, this will control how the prototypes are assigned. :arg prototype_sentences: Optional. A sorted list of sentences that represents the protoypes for embeddings space. If universal_embed_mode is set and prototypes are not provided,then this will be the level 0 parents sentences of the current clustering. To get universal embedding, we do cosine(target, prototypes), then normalize and then run through a universial_downsampler :arg protoypes: Optional. The embeddings in the embeddeing or (downsampled embedding) space that corresponds to the prototype_sentences. :arg min_num_prorotypes Optional. Will control the number of prototypes. :arg universal_downsampler Optional. The pythorch downsampler for mapping the output described above to a lower dimension that works across embedders and concept drift in the same embedder. maps from # of prototypes -> embed_dim. :arg indexer: Optional. If not set, then the BasicIndexer will be used. NOTE: Either pass in the parents, parent_levels, parent_labels, and parent2idx data is pased or clusters is passed. If none of these are passed and auto_create_embeddings_idx is set, then the data in the mmap file will be clustered and the data structure will be created. USAGE: for r in obj.search(\"test\"): print (r) for r in obj.search(numpy_or_pytorch_tensor): print (r) for r in obj.search(\"test\", numpy_or_pytorch_tensor): print (r) \"\"\" global device super().__init__() init_models(embedder) if indexer is None: indexer = BasicIndexer(embed_search_field=embed_search_field, bm25_field=bm25_field) self.embedder, self.indexer = embedder, indexer if idx_dir is None and filename is not None: idx_dir = f\"{filename}_idx\" elif idx_dir is None: idx_dir = \"./\" self.idx_dir = idx_dir if not os.path.exists(self.idx_dir): os.makedirs(self.idx_dir) if mmap_file is None: mmap_file = f\"{self.idx_dir}/search_index_{embed_search_field}_{embedder}_{embed_dim}.mmap\" if content_data_store is None: if filename is not None: if filename.endswith(\".gz\"): content_data_store = GzipByLineIdx.open(filename) else: content_data_store = FileByLineIdx(fobj=open(filename, \"rb\")) self.content_data_store = content_data_store if downsampler is None: model_embed_dim = get_model_embed_dim(embedder) downsampler = nn.Linear(model_embed_dim, embed_dim, bias=False).eval() if bm25_field is None: bm25_field = embed_search_field self.universal_embed_mode, self.mmap_file, self.mmap_len, self.embed_dim, self.dtype, self.clusters, self.parent2idx, self.parents, self.top_parents, self.top_parent_idxs, self.embed_search_field, self.bm25_field, self.downsampler = \\ universal_embed_mode, mmap_file, mmap_len, embed_dim, dtype, clusters, parent2idx, parents, top_parents, top_parent_idxs, embed_search_field, bm25_field, downsampler if self.mmap_len <= 0 and os.path.exists(self.mmap_file): mmap_len = self.mmap_len = get_np_mmap_length(self.mmap_file, [self.mmap_len, self.embed_dim], dtype=self.dtype, ) self.prototype_sentences, self.prototypes, self.universal_downsampler = prototype_sentences, prototypes, universal_downsampler if self.downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.downsampler = self.downsampler.half().eval().to(device) else: self.downsampler = self.downsampler.float().eval().to(device) if self.parents is not None: if self.dtype == np.float16: self.parents = self.parents.half().to(device) else: self.parents = self.parents.to(device) if skip_idxs is None: skip_idxs = [] self.skip_idxs = set(list(self.skip_idxs if hasattr(self, 'skip_idxs') and self.skip_idxs else []) + list(skip_idxs)) if universal_embed_mode not in (None, \"assigned\"): auto_embed_text = True if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) if universal_embed_mode not in (None, \"assigned\") and clusters is None: auto_create_embeddings_idx = True if os.path.exists(self.mmap_file) and (idxs is not None or auto_create_embeddings_idx): self.recreate_clusters_idx(clusters=self.clusters, span2cluster_label=span2cluster_label, idxs=idxs, max_level=max_level, max_cluster_size=max_cluster_size, \\ min_overlap_merge_cluster=min_overlap_merge_cluster, prefered_leaf_node_size=prefered_leaf_node_size, kmeans_batch_size=kmeans_batch_size) elif self.clusters: self.recreate_parents_data() if auto_create_bm25_idx and self.content_data_store: self.recreate_bm25_idx(auto_create_bm25_idx=auto_create_bm25_idx, idxs=idxs, use_tqdm=use_tqdm) setattr(self,f'downsampler_{self.embed_search_field}_{embedder}_{self.embed_dim}', self.downsampler) setattr(self,f'clusters_{self.embed_search_field}_{self.embedder}_{self.embed_dim}', self.clusters) ## experimental universal embedding code self.universal_embed_mode = universal_embed_mode if universal_embed_mode: assert (prototypes is None and prototype_sentences is None and universal_downsampler is None) or universal_embed_mode == \"assigned\" if universal_embed_mode == \"random\": prototype_sentences = [get_content_from_line(self.content_data_store[i], embed_search_field) for i in random.sample(list(range(len(self.content_data_store)), min_num_prorotypes))] elif universal_embed_mode == \"cluster\": level_0_parents = [span[1] for span in self.parent2idx.keys() if span[0] == 0] prototype_sentences = [get_content_from_line(self.content_data_store[span[1]], embed_search_field) for span in level_0_parents] assert prototype_sentences if len(prototype_senences) > min_num_prorotypes: assert universal_embed_mode != \"assigned\" prototype_senences = random.sample(prototype_senences,min_num_prorotypes) elif len(prototype_senences) < min_num_prorotypes: assert universal_embed_mode != \"assigned\" prototype_sentences.extend([get_content_from_line(self.content_data_store[i], embed_search_field) for i in random.sample(list(range(len(self.content_data_store)), min_num_prorotypes-len(prorotype_senences)))]) prototypes = self.get_embeddings(prototype_sentences, universal_downsampler_mode=None) # we don't want to apply the universal downsampler, because it is use in that routine universal_downsampler = nn.Linear(len(prototype_sentences), embed_dim, bias=False) self.prototype_sentences, self.prototypes, self.universal_downsampler = prototype_sentences, prototypes, universal_downsampler if self.universal_downsampler is not None: if device == 'cuda' and self.dtype == np.float16: self.universal_downsampler = self.universal_downsampler.half().eval().to(device) else: self.universal_downsampler= self.universal_downsampler.eval().to(device) if self.prototypes is not None: if self.dtype == np.float16: self.prototypes = self.prototypes.half().to(device) else: self.prototypes = self.prototypes.to(device) #now re-create the embeddings, and remove the old embedder based embeddings since we won't use those anymore. os.system(f\"rm -rf {self.mmap_file}\") self.mmap_file = f\"{self.idx_dir}/search_index_{embed_search_field}_universal_{embed_dim}.mmap\" if auto_embed_text and self.content_data_store is not None: self.embed_text(chunk_size=chunk_size, use_tqdm=use_tqdm) self.recreate_parents_data() parents = self.parents del self.parents self.register_buffer('parents', parents) prototypes = self.prototypes del self.prototypes self.register_buffer('prototypes', prototypes)","title":"__init__()"},{"location":"reference/simhash/","text":"hashing(document, tokenization='character', window_size=20, ignore_punctuation=True, lowercase=True) Hashing a document with SimHash. spanmeters str The text to use for hashing, by default \"text\" str, optional Method to use for tokenization, by default \"character\" int, optional The size of the token window, by default 6 bool, optional To ignore punctuation or not, by default True bool, optional To lowercase the text or not, by default True Returns int: The hash code Raises Exception Unrecognized tokenization spanmeter Source code in src/simhash.py def hashing( document: str, tokenization: str = \"character\", window_size: int = 20, ignore_punctuation: bool = True, lowercase: bool = True ) -> Dict[str, int]: \"\"\"Hashing a document with SimHash. spanmeters ---------- document : str The text to use for hashing, by default \"text\" tokenization : str, optional Method to use for tokenization, by default \"character\" window_size : int, optional The size of the token window, by default 6 ignore_punctuation : bool, optional To ignore punctuation or not, by default True lowercase : bool, optional To lowercase the text or not, by default True Returns ------- int: The hash code Raises ------ Exception Unrecognized tokenization spanmeter \"\"\" if lowercase: document = document.lower() if ignore_punctuation: document = PUNCTUATION_REGEX.sub(\"\", document) if tokenization == \"character\": document = \" \".join(document.split()) tokens = [ str.encode(document[i : i + window_size]) for i in range(len(document) - window_size) ] if not tokens: tokens = [str.encode(document)] elif tokenization == \"punctuation\": tokens0 = PUNCTUATION_REGEX.split(document) tokens = [ str.encode(\" \".join(tokens0[i : i + window_size])) for i in range(len(tokens0) - window_size) ] if not tokens: tokens = [str.encode(t) for t in tokens0] elif tokenization == \"space\": tokens0 = document.split(\" \") #consider whether we want to just use .split() to match \\n and \\t tokens = [ str.encode(\" \".join(tokens0[i : i + window_size])) for i in range(len(tokens0) - window_size) ] if not tokens: tokens = [str.encode(t) for t in tokens0] # we could try other types of tokenizations such as stemming and removal of stopwords else: raise Exception(f\"Unrecognized tokenization spanmeter {tokenization}\") assert tokens #TODO: the hash code is actually a 64bit int. Check sys.maxsize. #Was having a problem with serialzing np.int64 in json so i casted to int. #might not be an issue in parquet in which case we should revert back to np.int64. return int(simhash.compute(map(simhash.unsigned_hash, tokens))) incremental_span_and_document_neardedup(dup_span, dup_doc, unformatted_text, formatted_text=None, shingle_size=5, cleanup_dup_span_limit=1000000, cleanup_dup_doc_limit=1000000, normalize_text=True, keep_first_dup_in_unformatted_text=False, keep_first_dup_in_formatted_text=True, replace_char='*') Given a document text and a dict representing any near duplicate spans and duplicate docs, remove duplicate spans of shingle size from the text. The text can be in the form of clean unformatted text, e.g., removed formatting and any extraneous tags, and the corresponding formatted text, Assumes that double spaces denote sentence break in the text, and formatted_text. normalize_text will add double spaces between common punctuations and quotes. Return doc_is_dup, deduped unformatted_text, deduped formatted_text where doc_is_dup is 0 if there is no duplicates, 1 if there are partial span dups, and 2 if the whole document is a near dup. text is the original text with any duplicate spans replaced with the replace_char, collapsing multiple replace chars into one char. NOTE: the formatted_text are not guaranted to be deduped b/c there may be formatting in between spans that affects deduplication. Source code in src/simhash.py def incremental_span_and_document_neardedup( dup_span, dup_doc, unformatted_text, formatted_text=None, shingle_size = 5, cleanup_dup_span_limit=1000000, cleanup_dup_doc_limit=1000000, normalize_text=True, keep_first_dup_in_unformatted_text=False, keep_first_dup_in_formatted_text=True, replace_char='*'): \"\"\" Given a document text and a dict representing any near duplicate spans and duplicate docs, remove duplicate spans of shingle size from the text. The text can be in the form of clean unformatted text, e.g., removed formatting and any extraneous tags, and the corresponding formatted text, Assumes that double spaces denote sentence break in the text, and formatted_text. normalize_text will add double spaces between common punctuations and quotes. Return: doc_is_dup, deduped unformatted_text, deduped formatted_text where doc_is_dup is 0 if there is no duplicates, 1 if there are partial span dups, and 2 if the whole document is a near dup. text is the original text with any duplicate spans replaced with the replace_char, collapsing multiple replace chars into one char. NOTE: the formatted_text are not guaranted to be deduped b/c there may be formatting in between spans that affects deduplication. \"\"\" is_dup_chunk={} if normalize_text: #simple normalize and add double spaces after sentences. TODO, add other lang punc. unformatted_text = unformatted_text.replace(\"! \", \"! \").replace(\"? \", \"? \").replace(\". \", \". \").replace(\"\uff0e\", \"\uff0e \").replace(\"\u3002\", \"\u3002 \").replace(\"\uff1f\", \"\uff1f \")\\ .replace(\"!\\\" \", \"!\\\" \").replace(\"?\\\" \", \"?\\\" \").replace(\".\\\" \", \".\\\" \").replace(\"\uff0e\\\"\", \"\uff0e\\\" \").replace(\"\u3002\\\"\", \"\u3002\\\" \").replace(\"\uff1f\\\"\", \"\uff1f\\\" \")\\ .replace(\"!\u201d \", \"!\u201d \").replace(\"?\u201d \", \"?\u201d \").replace(\".\u201d \", \".\u201d \").replace(\"\uff0e\u201d\", \"\uff0e\u201d \").replace(\"\u3002\u201d\", \"\u3002\u201d \").replace(\"\uff1f\u201d\", \"\uff1f\u201d \")\\ .replace(\"!\u300b \", \"!\u300b \").replace(\"?\u300b \", \"?\u300b \").replace(\".\u300b \", \".\u300b \").replace(\"\uff0e\u300b\", \"\uff0e\u300b \").replace(\"\u3002\u300b\", \"\u3002\u300b \").replace(\"\uff1f\u300b\", \"\uff1f\u300b \")\\ .replace(\"\u3001\", \"\u3001 \").replace(\"\u2019s\", \" 's\").replace(\"`s\", \" 's\").replace(\"'s\", \" 's\") if formatted_text is None: formatted_text = unformatted_text text_arr = [a.strip() for a in unformatted_text.split(\" \") if a.strip()] #chunkify into sentences chunks = [] for sent in text_arr: if not sent: continue if \" \" not in sent and len(sent) > 20: while sent: chunks.append(sent[:20]) sent = sent[20:] else: chunks.append(sent) replace_text = \" \"+replace_char+\" \" shingles = [\" \".join(chunks[i : i + shingle_size]) for i in range(len(chunks) - shingle_size)] is_dup_within_doc = {} unformatted_text = \" \".join(unformatted_text.split()) #dedup spans other than the first matching span using shingle_size of sentences (e.g., a span) for ch_idx in range(len(chunks) - shingle_size): orig_shingle= \" \".join(chunks[ch_idx : ch_idx + shingle_size]) shingle = DIGIT_REGEX.sub('1', orig_shingle).strip() if not shingle: continue hashcode = hashing(shingle) if hashcode in is_dup_within_doc: prev_ch_idx = is_dup_within_doc[hashcode][0] prev_chunk = chunks[prev_ch_idx] clean_position = unformatted_text.find(prev_chunk) formatted_text_position = formatted_text.find(prev_chunk) if clean_position >= 0 and formatted_text_position >= 0: clean_position += len(shingle) formatted_text_position += len(shingle) unformatted_text2 = unformatted_text[clean_position+1:] formatted_text2 = formatted_text[formatted_text_position+1:] if shingle in unformatted_text2: unformatted_text2 = unformatted_text2.replace(shingle, replace_text) else: for chunk in chunks[ch_idx : ch_idx + shingle_size]: if len(chunk) > 3: unformatted_text2 = unformatted_text2.replace(chunk, replace_text) if shingle in formatted_text2: formatted_text2 = formatted_text2.replace(shingle, replace_text) else: for chunk in chunks[ch_idx : ch_idx + shingle_size]: if len(chunk) > 3: formatted_text2 = formatted_text2.replace(chunk, replace_text) unformatted_text = unformatted_text[:clean_position+1] + unformatted_text2 formatted_text = formatted_text[:formatted_text_position+1] + formatted_text2 is_dup_within_doc[hashcode] = is_dup_within_doc.get(hashcode, []) + [ch_idx] if hashcode in dup_span: dup_span[hashcode] += 1 else: dup_span[hashcode] = 1 if not keep_first_dup_in_formatted_text: for hashcode, ch_idx in is_dup_within_doc.items(): if hashcode in dup_span and dup_span.get(hashcode, len(ch_idx)) > len(ch_idx): #this item is a duplicate across documents ch_idx = ch_idx[0] shingle= \" \".join(chunks[ch_idx : ch_idx + shingle_size]) if shingle in formatted_text: formatted_text = formatted_text.replace(shingle, replace_text) else: for chunk in chunks[ch_idx : ch_idx + shingle_size]: formatted_text = formatted_text.replace(chunk, replace_text) if not keep_first_dup_in_unformatted_text: for hashcode, ch_idx in is_dup_within_doc.items(): if hashcode in dup_span and dup_span.get(hashcode,0) > len(ch_idx): #this item is a duplicate across documents ch_idx = ch_idx[0] shingle= \" \".join(chunks[ch_idx : ch_idx + shingle_size]) if shingle in unformatted_text: unformatted_text = unformatted_text.replace(shingle, replace_text) else: for chunk in chunks[ch_idx : ch_idx + shingle_size]: unformatted_text = unformatted_text.replace(chunk, replace_text) unformatted_text = unformatted_text.replace(replace_char+\" .\", replace_text).\\ replace(replace_char+\" !\", replace_text).\\ replace(replace_char+\" ?\", replace_text).\\ replace(replace_char+\" .\", replace_text).\\ replace(replace_char+\" \uff0e\", replace_text).\\ replace(replace_char+\" \u3002\", replace_text).\\ replace(replace_char+\" \uff1f\", replace_text).\\ replace(\" \", \" \").\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char).\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char).\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char) unformatted_text = \" \".join(unformatted_text.split()) formatted_text = formatted_text.replace(replace_char+\" .\", replace_text).\\ replace(replace_char+\" !\", replace_text).\\ replace(replace_char+\" ?\", replace_text).\\ replace(replace_char+\" .\", replace_text).\\ replace(replace_char+\" \uff0e\", replace_text).\\ replace(replace_char+\" \u3002\", replace_text).\\ replace(replace_char+\" \uff1f\", replace_text).\\ replace(\" \", \" \").\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char).\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char) formatted_text = \" \".join(formatted_text.split()) #TODO: improve this so we cleaup by value until we reach the limit if len(dup_span) > cleanup_dup_span_limit: for key, val in list(dup_span.items()): if val <= 1: del dup_span[key] if len(dup_doc) > cleanup_dup_doc_limit: for key, val in list(dup_doc.items()): if val <= 1: del dup_doc[key] doc_is_dup = 0 if any([a for h, a in is_dup_within_doc.items() if len(a) > 1 or len(a) < dup_span.get(h,len(a))]): hashcode = \" \".join(unformatted_text.replace(\"*\", \"\").split()) hashcode = hashcode.strip(' '+replace_char).lower() hashcode = DIGIT_REGEX.sub('1', hashcode) hashcode = hashing(hashcode) if hashcode in dup_doc: dup_doc[hashcode] += 1 doc_is_dup=2 else: dup_doc[hashcode] = 1 doc_is_dup=1 return doc_is_dup, unformatted_text, formatted_text index_clusters_batch_python(visited, hash2cluster, cluster2hash, hashes, num_blocks, hamming_distance) Create clusters within hamming distance. Collapses a->b, b->c to all be in the same cluster. NOTE: this isn't always true that a and c are within hamming_distance. NOTE: The cluster_id is the hashcode of the first item in the cluster and thus can be used to do further clustering and hamming distance matching. Source code in src/simhash.py def index_clusters_batch_python(visited, hash2cluster, cluster2hash, hashes, num_blocks, hamming_distance): \"\"\" Create clusters within hamming distance. Collapses a->b, b->c to all be in the same cluster. NOTE: this isn't always true that a and c are within hamming_distance. NOTE: The cluster_id is the hashcode of the first item in the cluster and thus can be used to do further clustering and hamming distance matching. \"\"\" matches = simhash.find_all(hashes, num_blocks, hamming_distance) graph = defaultdict(dict) for x, y in matches: graph[x][y] = True graph[y][x] = True hashes = set(hashes) while hashes: hash = hashes.pop() if hash in visited: continue # BFS to find the cluster if hash not in graph: hash2cluster[hash] = -1 continue q = deque([hash]) visited.add(hash) cluster_id = hash hash2cluster[hash] = cluster_id cluster2hash[cluster_id] = cluster2hash.get(cluster_id, []) + [hash] while q: node = q.popleft() for neighbor in graph[node]: if neighbor in visited: continue visited.add(neighbor) q.append(neighbor) hash2cluster[neighbor] = cluster_id cluster2hash[cluster_id] = cluster2hash.get(cluster_id, []) + [neighbor] return visited, hash2cluster, cluster2hash, index_clusters_python(hashes, num_blocks, hamming_distance, do_sort=True, batch_size=900000, verbose=False) Incrementally find clusters of int64 bit hashes of around the same hamming distance from each other. Returns hash2cluster and cluster2hash dicts, where the ids are all int64 bit hashes. Source code in src/simhash.py def index_clusters_python(hashes, num_blocks, hamming_distance, do_sort=True, batch_size=900000, verbose=False): \"\"\" Incrementally find clusters of int64 bit hashes of *around* the same hamming distance from each other. Returns hash2cluster and cluster2hash dicts, where the ids are all int64 bit hashes. \"\"\" if do_sort: hashes.sort() # we are assuming no exact duplicates. if we want to deal with exact duplicates, we can easily just collapse them in sequence # since this is a sorted list cluster2hash = {} hash2cluster = {} visited: Set[int] = set() if len(hashes) <= batch_size: visited, hash2cluster, cluster2hash = find_clusters_batch(visited, hash2cluster, cluster2hash, hashes, num_blocks, hamming_distance) return hash2cluster, cluster2hash batch_size2 = int(batch_size/2) if verbose: a_iter = tqdm.tqdm(range(0, len(hashes), batch_size2)) else: a_iter = range(0, len(hashes), batch_size2) for rng in a_iter: max_rng = min(len(hashes), rng+batch_size2) hashes2 = hashes[rng:max_rng] hashes3 = [] if cluster2hash: iterms_per_clusters = int(max(1, batch_size2/len(cluster2hash))) hashes3 = list(itertools.chain(*[val[:iterms_per_clusters] for val in cluster2hash.values()])) if len(hashes3) > int(batch_size2/2): hashes3 = random.sample(hashes3, batch_size2) if rng > 0 and len(hashes3) < batch_size2: hashes3 = list(set(hashes3+random.sample(hashes[:rng], batch_size2-len(hashes3)))) #print (len(hashes3)) hashes2.extend(hashes3) #print (len(hashes2)) visited, hash2cluster, cluster2hash = index_clusters_batch_python(visited, hash2cluster, cluster2hash, hashes2, num_blocks, hamming_distance) return hash2cluster, cluster2hash index_faiss(hashes, d=16) hashes: the array of ints representing the simhash d: Dimension of the ints. Source code in src/simhash.py def index_faiss(hashes, d=16): \"\"\" hashes: the array of ints representing the simhash d: Dimension of the ints. \"\"\" sqrt_size = int(math.sqrt(len(hashes))) # Vectors to train the quantizer. training = [hashes[i] for i in random.sample(range(len(hashes)), 2*sqrt_size)] # Initializing the quantizer. quantizer = faiss.IndexBinaryFlat(d) sqrt_size = int(math.sqrt(len(hashes))) # Number of clusters. nlist = sqrt_size # Initializing index. index = faiss.IndexBinaryIVF(quantizer, d, nlist) index.nprobe = 4 # Number of nearest clusters to be searched per query. # Training the quantizer. index.train(training) # Adding the database vectors. index.add(hashes) return index search_faiss(queries, qindices, hamming_distance, k=500, index=None) k: Number of nearest neighbors to retrieve per query vector. Source code in src/simhash.py def search_faiss(queries, qindices, hamming_distance, k=500, index=None): \"\"\" k: Number of nearest neighbors to retrieve per query vector. \"\"\" if index is None: index = index_faiss(queries) ret = [] # Querying the index. D, I = index.search(queries, k) for i, matches, indices in zip(qindices, D, I): for score, j in zip(matches, indices): if score <= hamming_distance: ret.append((i, j)) return ret search_python_only(queries, num_blocks, hamming_distance) Create clusters within hamming distance. Collapses a->b, b->c to all be in the same cluster. NOTE: this isn't always true that a and c are within hamming_distance. NOTE: The cluster_id is the hashcode of the first item in the cluster and thus can be used to do further clustering and hamming distance matching. Source code in src/simhash.py def search_python_only(queries, num_blocks, hamming_distance): \"\"\" Create clusters within hamming distance. Collapses a->b, b->c to all be in the same cluster. NOTE: this isn't always true that a and c are within hamming_distance. NOTE: The cluster_id is the hashcode of the first item in the cluster and thus can be used to do further clustering and hamming distance matching. \"\"\" return simhash.find_all(queries, num_blocks, hamming_distance)","title":"simhash"},{"location":"reference/simhash/#simhash.hashing","text":"Hashing a document with SimHash. spanmeters str The text to use for hashing, by default \"text\" str, optional Method to use for tokenization, by default \"character\" int, optional The size of the token window, by default 6 bool, optional To ignore punctuation or not, by default True bool, optional To lowercase the text or not, by default True","title":"hashing()"},{"location":"reference/simhash/#simhash.hashing--returns","text":"int: The hash code","title":"Returns"},{"location":"reference/simhash/#simhash.hashing--raises","text":"Exception Unrecognized tokenization spanmeter Source code in src/simhash.py def hashing( document: str, tokenization: str = \"character\", window_size: int = 20, ignore_punctuation: bool = True, lowercase: bool = True ) -> Dict[str, int]: \"\"\"Hashing a document with SimHash. spanmeters ---------- document : str The text to use for hashing, by default \"text\" tokenization : str, optional Method to use for tokenization, by default \"character\" window_size : int, optional The size of the token window, by default 6 ignore_punctuation : bool, optional To ignore punctuation or not, by default True lowercase : bool, optional To lowercase the text or not, by default True Returns ------- int: The hash code Raises ------ Exception Unrecognized tokenization spanmeter \"\"\" if lowercase: document = document.lower() if ignore_punctuation: document = PUNCTUATION_REGEX.sub(\"\", document) if tokenization == \"character\": document = \" \".join(document.split()) tokens = [ str.encode(document[i : i + window_size]) for i in range(len(document) - window_size) ] if not tokens: tokens = [str.encode(document)] elif tokenization == \"punctuation\": tokens0 = PUNCTUATION_REGEX.split(document) tokens = [ str.encode(\" \".join(tokens0[i : i + window_size])) for i in range(len(tokens0) - window_size) ] if not tokens: tokens = [str.encode(t) for t in tokens0] elif tokenization == \"space\": tokens0 = document.split(\" \") #consider whether we want to just use .split() to match \\n and \\t tokens = [ str.encode(\" \".join(tokens0[i : i + window_size])) for i in range(len(tokens0) - window_size) ] if not tokens: tokens = [str.encode(t) for t in tokens0] # we could try other types of tokenizations such as stemming and removal of stopwords else: raise Exception(f\"Unrecognized tokenization spanmeter {tokenization}\") assert tokens #TODO: the hash code is actually a 64bit int. Check sys.maxsize. #Was having a problem with serialzing np.int64 in json so i casted to int. #might not be an issue in parquet in which case we should revert back to np.int64. return int(simhash.compute(map(simhash.unsigned_hash, tokens)))","title":"Raises"},{"location":"reference/simhash/#simhash.incremental_span_and_document_neardedup","text":"Given a document text and a dict representing any near duplicate spans and duplicate docs, remove duplicate spans of shingle size from the text. The text can be in the form of clean unformatted text, e.g., removed formatting and any extraneous tags, and the corresponding formatted text, Assumes that double spaces denote sentence break in the text, and formatted_text. normalize_text will add double spaces between common punctuations and quotes. Return doc_is_dup, deduped unformatted_text, deduped formatted_text where doc_is_dup is 0 if there is no duplicates, 1 if there are partial span dups, and 2 if the whole document is a near dup. text is the original text with any duplicate spans replaced with the replace_char, collapsing multiple replace chars into one char. NOTE: the formatted_text are not guaranted to be deduped b/c there may be formatting in between spans that affects deduplication. Source code in src/simhash.py def incremental_span_and_document_neardedup( dup_span, dup_doc, unformatted_text, formatted_text=None, shingle_size = 5, cleanup_dup_span_limit=1000000, cleanup_dup_doc_limit=1000000, normalize_text=True, keep_first_dup_in_unformatted_text=False, keep_first_dup_in_formatted_text=True, replace_char='*'): \"\"\" Given a document text and a dict representing any near duplicate spans and duplicate docs, remove duplicate spans of shingle size from the text. The text can be in the form of clean unformatted text, e.g., removed formatting and any extraneous tags, and the corresponding formatted text, Assumes that double spaces denote sentence break in the text, and formatted_text. normalize_text will add double spaces between common punctuations and quotes. Return: doc_is_dup, deduped unformatted_text, deduped formatted_text where doc_is_dup is 0 if there is no duplicates, 1 if there are partial span dups, and 2 if the whole document is a near dup. text is the original text with any duplicate spans replaced with the replace_char, collapsing multiple replace chars into one char. NOTE: the formatted_text are not guaranted to be deduped b/c there may be formatting in between spans that affects deduplication. \"\"\" is_dup_chunk={} if normalize_text: #simple normalize and add double spaces after sentences. TODO, add other lang punc. unformatted_text = unformatted_text.replace(\"! \", \"! \").replace(\"? \", \"? \").replace(\". \", \". \").replace(\"\uff0e\", \"\uff0e \").replace(\"\u3002\", \"\u3002 \").replace(\"\uff1f\", \"\uff1f \")\\ .replace(\"!\\\" \", \"!\\\" \").replace(\"?\\\" \", \"?\\\" \").replace(\".\\\" \", \".\\\" \").replace(\"\uff0e\\\"\", \"\uff0e\\\" \").replace(\"\u3002\\\"\", \"\u3002\\\" \").replace(\"\uff1f\\\"\", \"\uff1f\\\" \")\\ .replace(\"!\u201d \", \"!\u201d \").replace(\"?\u201d \", \"?\u201d \").replace(\".\u201d \", \".\u201d \").replace(\"\uff0e\u201d\", \"\uff0e\u201d \").replace(\"\u3002\u201d\", \"\u3002\u201d \").replace(\"\uff1f\u201d\", \"\uff1f\u201d \")\\ .replace(\"!\u300b \", \"!\u300b \").replace(\"?\u300b \", \"?\u300b \").replace(\".\u300b \", \".\u300b \").replace(\"\uff0e\u300b\", \"\uff0e\u300b \").replace(\"\u3002\u300b\", \"\u3002\u300b \").replace(\"\uff1f\u300b\", \"\uff1f\u300b \")\\ .replace(\"\u3001\", \"\u3001 \").replace(\"\u2019s\", \" 's\").replace(\"`s\", \" 's\").replace(\"'s\", \" 's\") if formatted_text is None: formatted_text = unformatted_text text_arr = [a.strip() for a in unformatted_text.split(\" \") if a.strip()] #chunkify into sentences chunks = [] for sent in text_arr: if not sent: continue if \" \" not in sent and len(sent) > 20: while sent: chunks.append(sent[:20]) sent = sent[20:] else: chunks.append(sent) replace_text = \" \"+replace_char+\" \" shingles = [\" \".join(chunks[i : i + shingle_size]) for i in range(len(chunks) - shingle_size)] is_dup_within_doc = {} unformatted_text = \" \".join(unformatted_text.split()) #dedup spans other than the first matching span using shingle_size of sentences (e.g., a span) for ch_idx in range(len(chunks) - shingle_size): orig_shingle= \" \".join(chunks[ch_idx : ch_idx + shingle_size]) shingle = DIGIT_REGEX.sub('1', orig_shingle).strip() if not shingle: continue hashcode = hashing(shingle) if hashcode in is_dup_within_doc: prev_ch_idx = is_dup_within_doc[hashcode][0] prev_chunk = chunks[prev_ch_idx] clean_position = unformatted_text.find(prev_chunk) formatted_text_position = formatted_text.find(prev_chunk) if clean_position >= 0 and formatted_text_position >= 0: clean_position += len(shingle) formatted_text_position += len(shingle) unformatted_text2 = unformatted_text[clean_position+1:] formatted_text2 = formatted_text[formatted_text_position+1:] if shingle in unformatted_text2: unformatted_text2 = unformatted_text2.replace(shingle, replace_text) else: for chunk in chunks[ch_idx : ch_idx + shingle_size]: if len(chunk) > 3: unformatted_text2 = unformatted_text2.replace(chunk, replace_text) if shingle in formatted_text2: formatted_text2 = formatted_text2.replace(shingle, replace_text) else: for chunk in chunks[ch_idx : ch_idx + shingle_size]: if len(chunk) > 3: formatted_text2 = formatted_text2.replace(chunk, replace_text) unformatted_text = unformatted_text[:clean_position+1] + unformatted_text2 formatted_text = formatted_text[:formatted_text_position+1] + formatted_text2 is_dup_within_doc[hashcode] = is_dup_within_doc.get(hashcode, []) + [ch_idx] if hashcode in dup_span: dup_span[hashcode] += 1 else: dup_span[hashcode] = 1 if not keep_first_dup_in_formatted_text: for hashcode, ch_idx in is_dup_within_doc.items(): if hashcode in dup_span and dup_span.get(hashcode, len(ch_idx)) > len(ch_idx): #this item is a duplicate across documents ch_idx = ch_idx[0] shingle= \" \".join(chunks[ch_idx : ch_idx + shingle_size]) if shingle in formatted_text: formatted_text = formatted_text.replace(shingle, replace_text) else: for chunk in chunks[ch_idx : ch_idx + shingle_size]: formatted_text = formatted_text.replace(chunk, replace_text) if not keep_first_dup_in_unformatted_text: for hashcode, ch_idx in is_dup_within_doc.items(): if hashcode in dup_span and dup_span.get(hashcode,0) > len(ch_idx): #this item is a duplicate across documents ch_idx = ch_idx[0] shingle= \" \".join(chunks[ch_idx : ch_idx + shingle_size]) if shingle in unformatted_text: unformatted_text = unformatted_text.replace(shingle, replace_text) else: for chunk in chunks[ch_idx : ch_idx + shingle_size]: unformatted_text = unformatted_text.replace(chunk, replace_text) unformatted_text = unformatted_text.replace(replace_char+\" .\", replace_text).\\ replace(replace_char+\" !\", replace_text).\\ replace(replace_char+\" ?\", replace_text).\\ replace(replace_char+\" .\", replace_text).\\ replace(replace_char+\" \uff0e\", replace_text).\\ replace(replace_char+\" \u3002\", replace_text).\\ replace(replace_char+\" \uff1f\", replace_text).\\ replace(\" \", \" \").\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char).\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char).\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char) unformatted_text = \" \".join(unformatted_text.split()) formatted_text = formatted_text.replace(replace_char+\" .\", replace_text).\\ replace(replace_char+\" !\", replace_text).\\ replace(replace_char+\" ?\", replace_text).\\ replace(replace_char+\" .\", replace_text).\\ replace(replace_char+\" \uff0e\", replace_text).\\ replace(replace_char+\" \u3002\", replace_text).\\ replace(replace_char+\" \uff1f\", replace_text).\\ replace(\" \", \" \").\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char).\\ replace(' '+replace_char+' '+replace_char, \" \"+replace_char) formatted_text = \" \".join(formatted_text.split()) #TODO: improve this so we cleaup by value until we reach the limit if len(dup_span) > cleanup_dup_span_limit: for key, val in list(dup_span.items()): if val <= 1: del dup_span[key] if len(dup_doc) > cleanup_dup_doc_limit: for key, val in list(dup_doc.items()): if val <= 1: del dup_doc[key] doc_is_dup = 0 if any([a for h, a in is_dup_within_doc.items() if len(a) > 1 or len(a) < dup_span.get(h,len(a))]): hashcode = \" \".join(unformatted_text.replace(\"*\", \"\").split()) hashcode = hashcode.strip(' '+replace_char).lower() hashcode = DIGIT_REGEX.sub('1', hashcode) hashcode = hashing(hashcode) if hashcode in dup_doc: dup_doc[hashcode] += 1 doc_is_dup=2 else: dup_doc[hashcode] = 1 doc_is_dup=1 return doc_is_dup, unformatted_text, formatted_text","title":"incremental_span_and_document_neardedup()"},{"location":"reference/simhash/#simhash.index_clusters_batch_python","text":"Create clusters within hamming distance. Collapses a->b, b->c to all be in the same cluster. NOTE: this isn't always true that a and c are within hamming_distance. NOTE: The cluster_id is the hashcode of the first item in the cluster and thus can be used to do further clustering and hamming distance matching. Source code in src/simhash.py def index_clusters_batch_python(visited, hash2cluster, cluster2hash, hashes, num_blocks, hamming_distance): \"\"\" Create clusters within hamming distance. Collapses a->b, b->c to all be in the same cluster. NOTE: this isn't always true that a and c are within hamming_distance. NOTE: The cluster_id is the hashcode of the first item in the cluster and thus can be used to do further clustering and hamming distance matching. \"\"\" matches = simhash.find_all(hashes, num_blocks, hamming_distance) graph = defaultdict(dict) for x, y in matches: graph[x][y] = True graph[y][x] = True hashes = set(hashes) while hashes: hash = hashes.pop() if hash in visited: continue # BFS to find the cluster if hash not in graph: hash2cluster[hash] = -1 continue q = deque([hash]) visited.add(hash) cluster_id = hash hash2cluster[hash] = cluster_id cluster2hash[cluster_id] = cluster2hash.get(cluster_id, []) + [hash] while q: node = q.popleft() for neighbor in graph[node]: if neighbor in visited: continue visited.add(neighbor) q.append(neighbor) hash2cluster[neighbor] = cluster_id cluster2hash[cluster_id] = cluster2hash.get(cluster_id, []) + [neighbor] return visited, hash2cluster, cluster2hash,","title":"index_clusters_batch_python()"},{"location":"reference/simhash/#simhash.index_clusters_python","text":"Incrementally find clusters of int64 bit hashes of around the same hamming distance from each other. Returns hash2cluster and cluster2hash dicts, where the ids are all int64 bit hashes. Source code in src/simhash.py def index_clusters_python(hashes, num_blocks, hamming_distance, do_sort=True, batch_size=900000, verbose=False): \"\"\" Incrementally find clusters of int64 bit hashes of *around* the same hamming distance from each other. Returns hash2cluster and cluster2hash dicts, where the ids are all int64 bit hashes. \"\"\" if do_sort: hashes.sort() # we are assuming no exact duplicates. if we want to deal with exact duplicates, we can easily just collapse them in sequence # since this is a sorted list cluster2hash = {} hash2cluster = {} visited: Set[int] = set() if len(hashes) <= batch_size: visited, hash2cluster, cluster2hash = find_clusters_batch(visited, hash2cluster, cluster2hash, hashes, num_blocks, hamming_distance) return hash2cluster, cluster2hash batch_size2 = int(batch_size/2) if verbose: a_iter = tqdm.tqdm(range(0, len(hashes), batch_size2)) else: a_iter = range(0, len(hashes), batch_size2) for rng in a_iter: max_rng = min(len(hashes), rng+batch_size2) hashes2 = hashes[rng:max_rng] hashes3 = [] if cluster2hash: iterms_per_clusters = int(max(1, batch_size2/len(cluster2hash))) hashes3 = list(itertools.chain(*[val[:iterms_per_clusters] for val in cluster2hash.values()])) if len(hashes3) > int(batch_size2/2): hashes3 = random.sample(hashes3, batch_size2) if rng > 0 and len(hashes3) < batch_size2: hashes3 = list(set(hashes3+random.sample(hashes[:rng], batch_size2-len(hashes3)))) #print (len(hashes3)) hashes2.extend(hashes3) #print (len(hashes2)) visited, hash2cluster, cluster2hash = index_clusters_batch_python(visited, hash2cluster, cluster2hash, hashes2, num_blocks, hamming_distance) return hash2cluster, cluster2hash","title":"index_clusters_python()"},{"location":"reference/simhash/#simhash.index_faiss","text":"hashes: the array of ints representing the simhash d: Dimension of the ints. Source code in src/simhash.py def index_faiss(hashes, d=16): \"\"\" hashes: the array of ints representing the simhash d: Dimension of the ints. \"\"\" sqrt_size = int(math.sqrt(len(hashes))) # Vectors to train the quantizer. training = [hashes[i] for i in random.sample(range(len(hashes)), 2*sqrt_size)] # Initializing the quantizer. quantizer = faiss.IndexBinaryFlat(d) sqrt_size = int(math.sqrt(len(hashes))) # Number of clusters. nlist = sqrt_size # Initializing index. index = faiss.IndexBinaryIVF(quantizer, d, nlist) index.nprobe = 4 # Number of nearest clusters to be searched per query. # Training the quantizer. index.train(training) # Adding the database vectors. index.add(hashes) return index","title":"index_faiss()"},{"location":"reference/simhash/#simhash.search_faiss","text":"k: Number of nearest neighbors to retrieve per query vector. Source code in src/simhash.py def search_faiss(queries, qindices, hamming_distance, k=500, index=None): \"\"\" k: Number of nearest neighbors to retrieve per query vector. \"\"\" if index is None: index = index_faiss(queries) ret = [] # Querying the index. D, I = index.search(queries, k) for i, matches, indices in zip(qindices, D, I): for score, j in zip(matches, indices): if score <= hamming_distance: ret.append((i, j)) return ret","title":"search_faiss()"},{"location":"reference/simhash/#simhash.search_python_only","text":"Create clusters within hamming distance. Collapses a->b, b->c to all be in the same cluster. NOTE: this isn't always true that a and c are within hamming_distance. NOTE: The cluster_id is the hashcode of the first item in the cluster and thus can be used to do further clustering and hamming distance matching. Source code in src/simhash.py def search_python_only(queries, num_blocks, hamming_distance): \"\"\" Create clusters within hamming distance. Collapses a->b, b->c to all be in the same cluster. NOTE: this isn't always true that a and c are within hamming_distance. NOTE: The cluster_id is the hashcode of the first item in the cluster and thus can be used to do further clustering and hamming distance matching. \"\"\" return simhash.find_all(queries, num_blocks, hamming_distance)","title":"search_python_only()"},{"location":"reference/stopwords/","text":"","title":"stopwords"},{"location":"reference/translation/","text":"","title":"translation"},{"location":"reference/utils/","text":"FileByLineIdx A class for accessing a file by line numbers. Requires fobj that provides a seek, and tell method. Optionally, the dat representing the line seek points can also be passed as dat. Source code in src/utils.py class FileByLineIdx: \"\"\" A class for accessing a file by line numbers. Requires fobj that provides a seek, and tell method. Optionally, the dat representing the line seek points can also be passed as dat. \"\"\" def __init__(self, fobj, dat=None): self.dat = dat self.fobj = fobj pos = fobj.tell() fobj.seek(0, os.SEEK_END) self.file_size = file_size = fobj.tell() if self.dat is not None: fobj.seek(pos,0) else: def reader(fobj, rng, max_rng, ret): fobj.seek(rng,0) pos = fobj.tell() while rng < max_rng: fobj.readline() pos = fobj.tell() if pos < max_rng: ret.append(pos) else: break rng = pos workers=[] line_nums = [] for rng in range(0, file_size, 10000000): max_rng = min(rng + 10000000, file_size) line_nums.append([]) worker = threading.Thread(target=reader, args=(copy.copy(fobj), rng, max_rng, line_nums[-1])) workers.append(worker) worker.start() for worker in workers: worker.join() self.dat = [0]+list(itertools.chain(*line_nums)) fobj.seek(pos,0) def __iter__(self): fobj = self.fobj len_self = len(self) for start in range(0, len_self, 1000): end = min(len_self, start+1000) start = self.dat[start] if end == len_self: end = self.file_size else: end= self.dat[end]-1 ret = [] pos = self.tell() fobj.seek(start, 0) ret= fobj.read(end-start).split(b'\\n') fobj.seek(pos, 0) for line in ret: yield line def __len__(self): return len(self.dat) def __getitem__(self, keys): fobj = self.fobj start, end = None, None if isinstance(keys, int): contiguous = False else: contiguous, start, end = _is_contiguous(keys) if isinstance(keys, slice): contiguous = True start = 0 if keys.start is None else keys.start end = len(self) if keys.stop is None else keys.stop if contiguous: start = self.dat[start] if end >= len(self.dat): end = self.file_size else: end= self.dat[end+1]-1 pos = fobj.tell() fobj.seek(start, 0) ret= fobj.read(end-start).split(b'\\n') fobj.seek(pos, 0) return ret elif isinstance(keys, int): start = self.dat[keys] pos = fobj.tell() fobj.seek(start, 0) ret= fobj.readline() fobj.seek(pos, 0) return ret else: return [self[idx] for idx in keys] GzipByLineIdx Bases: igzip . IndexedGzipFile This class inheriets from ingdex_gzip.IndexedGzipFile . This class allows in addition to the functionality of IndexedGzipFile, access to a specific line based on the seek point of the line, using the getitem method. Additionally, a (conginguous) list or slice can be used, which will be more efficient then doing line by line access. The base IndexedGzipFile class allows for fast random access of a gzip file by using the zran library to build and maintain an index of seek points into the file. IndexedGzipFile is an io.BufferedReader which wraps an :class: _IndexedGzipFile instance. By accessing the _IndexedGzipFile instance through an io.BufferedReader , read performance is improved through buffering, and access to the I/O methods is made thread-safe. A :meth: pread method is also implemented, as it is not implemented by the io.BufferedReader . Source code in src/utils.py class GzipByLineIdx(igzip.IndexedGzipFile): #TODO: refactor to use FileByLineIdx as a member obj. \"\"\"This class inheriets from `` ingdex_gzip.IndexedGzipFile``. This class allows in addition to the functionality of IndexedGzipFile, access to a specific line based on the seek point of the line, using the __getitem__ method. Additionally, a (conginguous) list or slice can be used, which will be more efficient then doing line by line access. The base IndexedGzipFile class allows for fast random access of a gzip file by using the ``zran`` library to build and maintain an index of seek points into the file. ``IndexedGzipFile`` is an ``io.BufferedReader`` which wraps an :class:`_IndexedGzipFile` instance. By accessing the ``_IndexedGzipFile`` instance through an ``io.BufferedReader``, read performance is improved through buffering, and access to the I/O methods is made thread-safe. A :meth:`pread` method is also implemented, as it is not implemented by the ``io.BufferedReader``. \"\"\" def __init__(self, *args, **kwargs): \"\"\"Create an ``LineIndexGzipFileExt``. The file may be specified either with an open file handle (``fileobj``), or with a ``filename``. If the former, the file must have been opened in ``'rb'`` mode. .. note:: The ``auto_build`` behaviour only takes place on calls to :meth:`seek`. :arg filename: File name or open file handle. :arg fileobj: Open file handle. :arg mode: Opening mode. Must be either ``'r'`` or ``'rb``. :arg auto_build: If ``True`` (the default), the index is automatically built on calls to :meth:`seek`. :arg skip_crc_check: Defaults to ``False``. If ``True``, CRC/size validation of the uncompressed data is not performed. :arg spacing: Number of bytes between index seek points. :arg window_size: Number of bytes of uncompressed data stored with each seek point. :arg readbuf_size: Size of buffer in bytes for storing compressed data read in from the file. :arg readall_buf_size: Size of buffer in bytes used by :meth:`read` when reading until EOF. :arg drop_handles: Has no effect if an open ``fid`` is specified, rather than a ``filename``. If ``True`` (the default), a handle to the file is opened and closed on every access. Otherwise the file is opened at ``__cinit__``, and kept open until this ``_IndexedGzipFile`` is destroyed. :arg index_file: Pre-generated index for this ``gz`` file - if provided, passed through to :meth:`import_index`. :arg buffer_size: Optional, must be passed as a keyword argument. Passed through to ``io.BufferedReader.__init__``. If not provided, a default value of 1048576 is used. :arg line2seekpoint: Optional, must be passed as a keyword argument. If not passed, this will automatically be created. \"\"\" filename = kwargs.get(\"filename\") if args and not filename: filename = args[0] need_export_index = False if filename: if not os.path.exists(filename+\"_idx\"): need_export_index = True os.makedirs(filename+\"_idx\") if not os.path.exists(filename+\"_idx/igzip.pickle\"): need_export_index = True else: kwargs['index_file'] = kwargs.pop('index_file', filename+\"_idx/igzip.pickle\") if 'file_size' in kwargs: file_size = self.file_size = kwargs.pop('file_size', None) need_export_index = False self.line2seekpoint = kwargs.pop('line2seekpoint', None) if need_export_index and 'auto_build' not in kwargs: kwargs['auto_build'] = True super(GzipByLineIdx, self).__init__(*args, **kwargs) if not hasattr(self, 'file_size'): self.build_full_index() pos = self.tell() self.seek(0, os.SEEK_END) self.file_size = file_size = self.tell() if self.line2seekpoint is None: def reader(fobj, rng, max_rng, ret): fobj.seek(rng,0) pos = fobj.tell() while rng < max_rng: fobj.readline() pos = fobj.tell() if pos < max_rng: ret.append(pos) else: break rng = pos workers=[] line_nums = [] for rng in range(0, file_size, 10000000): max_rng = min(rng + 10000000, file_size) line_nums.append([]) worker = threading.Thread(target=reader, args=(copy.copy(self), rng, max_rng, line_nums[-1])) workers.append(worker) worker.start() for worker in workers: worker.join() self.line2seekpoint = [0]+list(itertools.chain(*line_nums)) if filename and need_export_index: self.export_index(filename+\"_idx/igzip.pickle\") def __reduce__(self): \"\"\"Used to pickle an ``GzipByLineIdx``. Returns a tuple containing: - a reference to the ``unpickle`` function - a tuple containing a \"state\" object, which can be passed to ``unpickle``. \"\"\" fobj = self._IndexedGzipFile__igz_fobj if (not fobj.drop_handles) or (not fobj.own_file): raise pickle.PicklingError( 'Cannot pickle GzipByLineIdx that has been created ' 'with an open file object, or that has been created ' 'with drop_handles=False') # export and serialise the index if # any index points have been created. # The index data is serialised as a # bytes object. if fobj.npoints == 0: index = None else: index = io.BytesIO() self.export_index(fileobj=index) index = index.getvalue() state = { 'filename' : fobj.filename, 'auto_build' : fobj.auto_build, 'spacing' : fobj.spacing, 'window_size' : fobj.window_size, 'readbuf_size' : fobj.readbuf_size, 'readall_buf_size' : fobj.readall_buf_size, 'buffer_size' : self._IndexedGzipFile__buffer_size, 'line2seekpoint' : self.line2seekpoint, 'file_size' : self.file_size, 'tell' : self.tell(), 'index' : index} return (_unpickle_gzip_by_line, (state, )) #TODO: refactor to do def __iter__(self): len_self = len(self) for start in range(0, len_self, 1000): end = min(len_self, start+1000) orig_start = start orig_end = end while start < len_self and self.line2seekpoint[start] == -1: start+=1 while end >= 0 and self.line2seekpoint[end] == -1: end-=1 start = self.line2seekpoint[start] if end == len_self: seek_points = self.line2seekpoint[orig_start:] end = self.file_size else: seek_points = self.line2seekpoint[orig_start:orig_end] end= self.line2seekpoint[end]-1 ret = [] with self._IndexedGzipFile__file_lock: pos = self.tell() self.seek(start, 0) ret= self.read(end-start).split(b'\\n') self.seek(pos, 0) #if a seekpoint is -1, this means the data has been deleted - either not in the file at all or #blanked out for line in ret: while seek_points and seek_points[0] == -1 and line.strip(): yield \"\" seek_points.pop() if not seek_points: break if seek_points[0] == -1 and line.strip(): yield \"\" else: yield line seek_points.pop() while seek_points and seek_points[0] == -1: yield \"\" seek_points.pop() def __len__(self): return len(self.line2seekpoint) def __getitem__(self, keys): start, end = None, None if isinstance(keys, int): contiguous = False else: contiguous, start, end = _is_contiguous(keys) if isinstance(keys, slice): contiguous = True start = 0 if keys.start is None else keys.start end = len(self) if keys.stop is None else keys.stop if contiguous: start = self.line2seekpoint[start] if end >= len(self.line2seekpoint): end = self.file_size else: end= self.line2seekpoint[end+1]-1 with self._IndexedGzipFile__file_lock: pos = self.tell() self.seek(start, 0) ret= self.read(end-start).split(b'\\n') self.seek(pos, 0) return ret elif isinstance(keys, int): start = self.line2seekpoint[keys] if start < 0: return b\"\" with self._IndexedGzipFile__file_lock: pos = self.tell() self.seek(start, 0) ret= self.readline() self.seek(pos, 0) return ret else: return [self[idx] for idx in keys] @staticmethod def open(filename): if os.path.exists(filename+\"_idx/index.pickle\"): return GzipByLineIdx(filename, index_file=filename+\"_idx/index.pickle\") else: return GzipByLineIdx(filename) __init__(*args, **kwargs) Create an LineIndexGzipFileExt . The file may be specified either with an open file handle ( fileobj ), or with a filename . If the former, the file must have been opened in 'rb' mode. .. note:: The auto_build behaviour only takes place on calls to :meth: seek . :arg filename: File name or open file handle. :arg fileobj: Open file handle. :arg mode: Opening mode. Must be either 'r' or 'rb . :arg auto_build: If True (the default), the index is automatically built on calls to :meth: seek . :arg skip_crc_check: Defaults to False . If True , CRC/size validation of the uncompressed data is not performed. :arg spacing: Number of bytes between index seek points. :arg window_size: Number of bytes of uncompressed data stored with each seek point. :arg readbuf_size: Size of buffer in bytes for storing compressed data read in from the file. :arg readall_buf_size: Size of buffer in bytes used by :meth: read when reading until EOF. :arg drop_handles: Has no effect if an open fid is specified, rather than a filename . If True (the default), a handle to the file is opened and closed on every access. Otherwise the file is opened at __cinit__ , and kept open until this _IndexedGzipFile is destroyed. :arg index_file: Pre-generated index for this gz file - if provided, passed through to :meth: import_index . :arg buffer_size: Optional, must be passed as a keyword argument. Passed through to io.BufferedReader.__init__ . If not provided, a default value of 1048576 is used. :arg line2seekpoint: Optional, must be passed as a keyword argument. If not passed, this will automatically be created. Source code in src/utils.py def __init__(self, *args, **kwargs): \"\"\"Create an ``LineIndexGzipFileExt``. The file may be specified either with an open file handle (``fileobj``), or with a ``filename``. If the former, the file must have been opened in ``'rb'`` mode. .. note:: The ``auto_build`` behaviour only takes place on calls to :meth:`seek`. :arg filename: File name or open file handle. :arg fileobj: Open file handle. :arg mode: Opening mode. Must be either ``'r'`` or ``'rb``. :arg auto_build: If ``True`` (the default), the index is automatically built on calls to :meth:`seek`. :arg skip_crc_check: Defaults to ``False``. If ``True``, CRC/size validation of the uncompressed data is not performed. :arg spacing: Number of bytes between index seek points. :arg window_size: Number of bytes of uncompressed data stored with each seek point. :arg readbuf_size: Size of buffer in bytes for storing compressed data read in from the file. :arg readall_buf_size: Size of buffer in bytes used by :meth:`read` when reading until EOF. :arg drop_handles: Has no effect if an open ``fid`` is specified, rather than a ``filename``. If ``True`` (the default), a handle to the file is opened and closed on every access. Otherwise the file is opened at ``__cinit__``, and kept open until this ``_IndexedGzipFile`` is destroyed. :arg index_file: Pre-generated index for this ``gz`` file - if provided, passed through to :meth:`import_index`. :arg buffer_size: Optional, must be passed as a keyword argument. Passed through to ``io.BufferedReader.__init__``. If not provided, a default value of 1048576 is used. :arg line2seekpoint: Optional, must be passed as a keyword argument. If not passed, this will automatically be created. \"\"\" filename = kwargs.get(\"filename\") if args and not filename: filename = args[0] need_export_index = False if filename: if not os.path.exists(filename+\"_idx\"): need_export_index = True os.makedirs(filename+\"_idx\") if not os.path.exists(filename+\"_idx/igzip.pickle\"): need_export_index = True else: kwargs['index_file'] = kwargs.pop('index_file', filename+\"_idx/igzip.pickle\") if 'file_size' in kwargs: file_size = self.file_size = kwargs.pop('file_size', None) need_export_index = False self.line2seekpoint = kwargs.pop('line2seekpoint', None) if need_export_index and 'auto_build' not in kwargs: kwargs['auto_build'] = True super(GzipByLineIdx, self).__init__(*args, **kwargs) if not hasattr(self, 'file_size'): self.build_full_index() pos = self.tell() self.seek(0, os.SEEK_END) self.file_size = file_size = self.tell() if self.line2seekpoint is None: def reader(fobj, rng, max_rng, ret): fobj.seek(rng,0) pos = fobj.tell() while rng < max_rng: fobj.readline() pos = fobj.tell() if pos < max_rng: ret.append(pos) else: break rng = pos workers=[] line_nums = [] for rng in range(0, file_size, 10000000): max_rng = min(rng + 10000000, file_size) line_nums.append([]) worker = threading.Thread(target=reader, args=(copy.copy(self), rng, max_rng, line_nums[-1])) workers.append(worker) worker.start() for worker in workers: worker.join() self.line2seekpoint = [0]+list(itertools.chain(*line_nums)) if filename and need_export_index: self.export_index(filename+\"_idx/igzip.pickle\") __reduce__() Used to pickle an GzipByLineIdx . Returns a tuple containing a reference to the unpickle function a tuple containing a \"state\" object, which can be passed to unpickle . Source code in src/utils.py def __reduce__(self): \"\"\"Used to pickle an ``GzipByLineIdx``. Returns a tuple containing: - a reference to the ``unpickle`` function - a tuple containing a \"state\" object, which can be passed to ``unpickle``. \"\"\" fobj = self._IndexedGzipFile__igz_fobj if (not fobj.drop_handles) or (not fobj.own_file): raise pickle.PicklingError( 'Cannot pickle GzipByLineIdx that has been created ' 'with an open file object, or that has been created ' 'with drop_handles=False') # export and serialise the index if # any index points have been created. # The index data is serialised as a # bytes object. if fobj.npoints == 0: index = None else: index = io.BytesIO() self.export_index(fileobj=index) index = index.getvalue() state = { 'filename' : fobj.filename, 'auto_build' : fobj.auto_build, 'spacing' : fobj.spacing, 'window_size' : fobj.window_size, 'readbuf_size' : fobj.readbuf_size, 'readall_buf_size' : fobj.readall_buf_size, 'buffer_size' : self._IndexedGzipFile__buffer_size, 'line2seekpoint' : self.line2seekpoint, 'file_size' : self.file_size, 'tell' : self.tell(), 'index' : index} return (_unpickle_gzip_by_line, (state, )) create_hiearchical_clusters(clusters, span2cluster_label, mmap_file, mmap_len=0, embed_dim=25, dtype=np.float16, skip_idxs=None, idxs=None, max_level=4, max_cluster_size=200, min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, recluster_start_iter=0.85, max_decluster_iter=0.95, use_tqdm=True, grouping_fn=None, grouping_fn_callback_data=None) Incremental hiearchical clustering from embeddings stored in a mmap file. Can also used to create an index for searching. with a max of 4 levels, and each node containing 200 items, we can have up to 1.6B items approximately span2cluster_label maps a child span to a parent span. spans can be of the form int|(int,int). leaf nodes are ints. non-leaf nodes are (int,int) tuples clusters maps cluster_label => list of spans when we use the term 'idx', we normally refer to the index in an embedding file. :arg clusters: the dict mapping parent span to list of child span :arg span2cluster_label: the inverse of the above. :arg mmap_file: the name of the mmap file. :arg mmap_len: the current length of the mmap file. :arg embed_dim: the dimension of an embedding. :arg dtype: the numpy dtype. :arg skip_idxs: Optioal. the idx into the embeddings that will not be clustered or searched for. :arg idxs: Optioal. if provided, the particular embedding idx that will be clustered in this call. :arg max_level: the maximum level of the cluster hiearchy. :arg max_cluster_size:the maximum size of any particular cluster. :arg min_overlap_merge_cluster. When incremental clustering, the minimum overlap between one cluster and another before merging them. :arg kmeans_batch_size: the size of each batch of embeddings that are kmean batched. :arg use_tqdm: whether to report the progress of the clustering. :arg grouping_fn: Optional. a function that takes in a grouping_fn_callback_data, embeddings, and a list of spans, will return a hash of form {'group_X': [...], 'group_Y': [...], etc.} :arg grouping_fn_callback_data: Optional. arbitrary data to pass to the grouping_fn Source code in src/utils.py def create_hiearchical_clusters(clusters, span2cluster_label, mmap_file, mmap_len=0, embed_dim=25, dtype=np.float16, skip_idxs=None, idxs=None, max_level=4, \\ max_cluster_size=200, min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, \\ recluster_start_iter=0.85, max_decluster_iter=0.95, use_tqdm=True, grouping_fn=None, grouping_fn_callback_data=None): \"\"\" Incremental hiearchical clustering from embeddings stored in a mmap file. Can also used to create an index for searching. with a max of 4 levels, and each node containing 200 items, we can have up to 1.6B items approximately span2cluster_label maps a child span to a parent span. spans can be of the form int|(int,int). leaf nodes are ints. non-leaf nodes are (int,int) tuples clusters maps cluster_label => list of spans when we use the term 'idx', we normally refer to the index in an embedding file. :arg clusters: the dict mapping parent span to list of child span :arg span2cluster_label: the inverse of the above. :arg mmap_file: the name of the mmap file. :arg mmap_len: the current length of the mmap file. :arg embed_dim: the dimension of an embedding. :arg dtype: the numpy dtype. :arg skip_idxs: Optioal. the idx into the embeddings that will not be clustered or searched for. :arg idxs: Optioal. if provided, the particular embedding idx that will be clustered in this call. :arg max_level: the maximum level of the cluster hiearchy. :arg max_cluster_size:the maximum size of any particular cluster. :arg min_overlap_merge_cluster. When incremental clustering, the minimum overlap between one cluster and another before merging them. :arg kmeans_batch_size: the size of each batch of embeddings that are kmean batched. :arg use_tqdm: whether to report the progress of the clustering. :arg grouping_fn: Optional. a function that takes in a grouping_fn_callback_data, embeddings, and a list of spans, will return a hash of form {'group_X': [...], 'group_Y': [...], etc.} :arg grouping_fn_callback_data: Optional. arbitrary data to pass to the grouping_fn \"\"\" global device if skip_idxs is None: skip_idxs = set() else: skip_idxs = set(skip_idxs) if clusters is None: clusters = {} if span2cluster_label is None: span2cluster_label = {} for label, a_cluster in clusters: for span in a_cluster: span2cluster_label[span] = label else: #make sure clusters have the same data as span2cluster_label for span, label in span2cluster_label.items(): if span not in clusters.get(label,[]): clusters[label] = clusters.get(label,[]) + [span] #we are not going to cluster idxs that should be skipped if idxs: idxs = [idx for idx in idxs if idx not in skip_idxs] #remove some idx from the clusters so we can re-compute the clusters remove_idxs = list(skip_idxs) + ([] if idxs is None else idxs) if remove_idxs is not None: need_recompute_clusters=False for idx in remove_idxs: label = span2cluster_label.get(idx) if label is not None: clusters[label].remove(idx) a_cluster = clusters[label] del span2cluster_label[idx] need_recompute_clusters=True # now re-create the label if the idx is the proto index. if idx == label[1]: new_idx = a_cluster[0] for level in range(0, max_level): new_label2 = (level, new_idx) old_label2 = (level, idx) if old_label2 in span2cluster_label: #rename the label for the children clusters[new_label2] = clusters[old_label2] del clusters[old_label2] for span in clusters[new_label2]: span2cluster_label[span] = new_label2 #make the parent refer to the new label parent_label = span2cluster_label[old_label2] clusters[parent_label].remove(old_label2) clusters[parent_label].append(new_label2) span2cluster_label[new_label2] = parent_label #belt and suspenders, let's just recreate the clusters if need_recompute_clusters: clusters.clear() for span, label in span2cluster_label.items(): clusters[label] = clusters.get(label, []) + [span] #print (mmap_len, clusters, span2cluster_label) if prefered_leaf_node_size is None: prefered_leaf_node_size = max_cluster_size cluster_embeddings = np_memmap(mmap_file, shape=[mmap_len, embed_dim], dtype=dtype) # at level 0, the spans are the indexes themselves, so no need to map using all_spans all_spans = None for level in range(max_level): assert level == 0 or (all_spans is not None and idxs is not None) #print (\"got here\") if idxs is None: len_spans = mmap_len else: len_spans = len(idxs) # we are going to do a minimum of 6 times in case there are not already clustered items # from previous iterations. num_times = max(6,math.ceil(len_spans/int(.7*kmeans_batch_size))) recluster_at = max(0,num_times*recluster_start_iter) rng = 0 if use_tqdm: num_times2 = tqdm.tqdm(range(num_times)) else: num_times2 = range(num_times) for times in num_times2: max_rng = min(len_spans, rng+int(.7*kmeans_batch_size)) #create the next batch to cluster if idxs is None: spans = list(range(rng, max_rng)) not_already_clustered = [idx for idx in range(rng) if (all_spans is not None and all_spans[idx] not in span2cluster_label) or \\ (all_spans is None and idx not in span2cluster_label)] else: spans = idxs[rng: max_rng] not_already_clustered = [idx for idx in range(rng) if idxs[:rng] if (all_spans is not None and all_spans[idx] not in span2cluster_label) or \\ (all_spans is None and idx not in span2cluster_label)] num_itmes_left = kmeans_batch_size - len(spans) if len(not_already_clustered) > int(.5*num_itmes_left): spans.extend(random.sample(not_already_clustered, int(.5*num_itmes_left))) else: spans.extend(not_already_clustered) if len(spans) == 0: continue if level == 0: already_clustered = [idx for idx in range(mmap_len) if idx in span2cluster_label] else: already_clustered = [idx for idx, span in enumerate(all_spans) if span in span2cluster_label] if len(already_clustered) > int(.5*num_itmes_left): spans.extend(random.sample(already_clustered, int(.5*num_itmes_left))) else: spans.extend(already_clustered) # get the embedding indexs for the cluster if level == 0: spans = [span for span in spans if span not in skip_idxs] embedding_idxs = spans else: spans = [all_spans[idx] for idx in spans] spans = [span for span in spans if span[1] not in skip_idxs] embedding_idxs = [span[1] for span in spans] #print (spans) #do kmeans clustering in batches with the embedding indexes if level == 0: true_k = int(len(embedding_idxs)/prefered_leaf_node_size) else: true_k = int(len(embedding_idxs)/max_cluster_size) _cluster_one_batch(true_k, spans, embedding_idxs, clusters, span2cluster_label, level, cluster_embeddings, min_overlap_merge_cluster, grouping_fn, grouping_fn_callback_data) # re-cluster any small clusters or break up large clusters if times >= recluster_at: need_recompute_clusters = False for parent, spans in list(clusters.items()): if times < max(0, num_times*max_decluster_iter) and \\ ((level == 0 and len(spans) < prefered_leaf_node_size*.5) or (level != 0 and len(spans) < max_cluster_size*.5)): need_recompute_clusters = True for span in spans: del span2cluster_label[span] elif len(spans) > max_cluster_size: need_recompute_clusters = True for token in spans: del span2cluster_label[token] embedding_idxs = [span if type(span) is int else span[1] for span in spans] if level == 0: true_k = int(len(embedding_idxs)/prefered_leaf_node_size) else: true_k = int(len(embedding_idxs)/max_cluster_size) _cluster_one_batch(true_k, spans, embedding_idxs, clusters, span2cluster_label, level, cluster_embeddings, min_overlap_merge_cluster, grouping_fn, grouping_fn_callback_data) if need_recompute_clusters: clusters.clear() for span, label in span2cluster_label.items(): clusters[label] = clusters.get(label, []) + [span] rng = max_rng # prepare data for next level clustering all_spans = [label for label in clusters.keys() if label[0] == level] if len(all_spans) < max_cluster_size: break idxs = [idx for idx, label in enumerate(all_spans) if label not in span2cluster_label] return clusters, span2cluster_label","title":"utils"},{"location":"reference/utils/#utils.FileByLineIdx","text":"A class for accessing a file by line numbers. Requires fobj that provides a seek, and tell method. Optionally, the dat representing the line seek points can also be passed as dat. Source code in src/utils.py class FileByLineIdx: \"\"\" A class for accessing a file by line numbers. Requires fobj that provides a seek, and tell method. Optionally, the dat representing the line seek points can also be passed as dat. \"\"\" def __init__(self, fobj, dat=None): self.dat = dat self.fobj = fobj pos = fobj.tell() fobj.seek(0, os.SEEK_END) self.file_size = file_size = fobj.tell() if self.dat is not None: fobj.seek(pos,0) else: def reader(fobj, rng, max_rng, ret): fobj.seek(rng,0) pos = fobj.tell() while rng < max_rng: fobj.readline() pos = fobj.tell() if pos < max_rng: ret.append(pos) else: break rng = pos workers=[] line_nums = [] for rng in range(0, file_size, 10000000): max_rng = min(rng + 10000000, file_size) line_nums.append([]) worker = threading.Thread(target=reader, args=(copy.copy(fobj), rng, max_rng, line_nums[-1])) workers.append(worker) worker.start() for worker in workers: worker.join() self.dat = [0]+list(itertools.chain(*line_nums)) fobj.seek(pos,0) def __iter__(self): fobj = self.fobj len_self = len(self) for start in range(0, len_self, 1000): end = min(len_self, start+1000) start = self.dat[start] if end == len_self: end = self.file_size else: end= self.dat[end]-1 ret = [] pos = self.tell() fobj.seek(start, 0) ret= fobj.read(end-start).split(b'\\n') fobj.seek(pos, 0) for line in ret: yield line def __len__(self): return len(self.dat) def __getitem__(self, keys): fobj = self.fobj start, end = None, None if isinstance(keys, int): contiguous = False else: contiguous, start, end = _is_contiguous(keys) if isinstance(keys, slice): contiguous = True start = 0 if keys.start is None else keys.start end = len(self) if keys.stop is None else keys.stop if contiguous: start = self.dat[start] if end >= len(self.dat): end = self.file_size else: end= self.dat[end+1]-1 pos = fobj.tell() fobj.seek(start, 0) ret= fobj.read(end-start).split(b'\\n') fobj.seek(pos, 0) return ret elif isinstance(keys, int): start = self.dat[keys] pos = fobj.tell() fobj.seek(start, 0) ret= fobj.readline() fobj.seek(pos, 0) return ret else: return [self[idx] for idx in keys]","title":"FileByLineIdx"},{"location":"reference/utils/#utils.GzipByLineIdx","text":"Bases: igzip . IndexedGzipFile This class inheriets from ingdex_gzip.IndexedGzipFile . This class allows in addition to the functionality of IndexedGzipFile, access to a specific line based on the seek point of the line, using the getitem method. Additionally, a (conginguous) list or slice can be used, which will be more efficient then doing line by line access. The base IndexedGzipFile class allows for fast random access of a gzip file by using the zran library to build and maintain an index of seek points into the file. IndexedGzipFile is an io.BufferedReader which wraps an :class: _IndexedGzipFile instance. By accessing the _IndexedGzipFile instance through an io.BufferedReader , read performance is improved through buffering, and access to the I/O methods is made thread-safe. A :meth: pread method is also implemented, as it is not implemented by the io.BufferedReader . Source code in src/utils.py class GzipByLineIdx(igzip.IndexedGzipFile): #TODO: refactor to use FileByLineIdx as a member obj. \"\"\"This class inheriets from `` ingdex_gzip.IndexedGzipFile``. This class allows in addition to the functionality of IndexedGzipFile, access to a specific line based on the seek point of the line, using the __getitem__ method. Additionally, a (conginguous) list or slice can be used, which will be more efficient then doing line by line access. The base IndexedGzipFile class allows for fast random access of a gzip file by using the ``zran`` library to build and maintain an index of seek points into the file. ``IndexedGzipFile`` is an ``io.BufferedReader`` which wraps an :class:`_IndexedGzipFile` instance. By accessing the ``_IndexedGzipFile`` instance through an ``io.BufferedReader``, read performance is improved through buffering, and access to the I/O methods is made thread-safe. A :meth:`pread` method is also implemented, as it is not implemented by the ``io.BufferedReader``. \"\"\" def __init__(self, *args, **kwargs): \"\"\"Create an ``LineIndexGzipFileExt``. The file may be specified either with an open file handle (``fileobj``), or with a ``filename``. If the former, the file must have been opened in ``'rb'`` mode. .. note:: The ``auto_build`` behaviour only takes place on calls to :meth:`seek`. :arg filename: File name or open file handle. :arg fileobj: Open file handle. :arg mode: Opening mode. Must be either ``'r'`` or ``'rb``. :arg auto_build: If ``True`` (the default), the index is automatically built on calls to :meth:`seek`. :arg skip_crc_check: Defaults to ``False``. If ``True``, CRC/size validation of the uncompressed data is not performed. :arg spacing: Number of bytes between index seek points. :arg window_size: Number of bytes of uncompressed data stored with each seek point. :arg readbuf_size: Size of buffer in bytes for storing compressed data read in from the file. :arg readall_buf_size: Size of buffer in bytes used by :meth:`read` when reading until EOF. :arg drop_handles: Has no effect if an open ``fid`` is specified, rather than a ``filename``. If ``True`` (the default), a handle to the file is opened and closed on every access. Otherwise the file is opened at ``__cinit__``, and kept open until this ``_IndexedGzipFile`` is destroyed. :arg index_file: Pre-generated index for this ``gz`` file - if provided, passed through to :meth:`import_index`. :arg buffer_size: Optional, must be passed as a keyword argument. Passed through to ``io.BufferedReader.__init__``. If not provided, a default value of 1048576 is used. :arg line2seekpoint: Optional, must be passed as a keyword argument. If not passed, this will automatically be created. \"\"\" filename = kwargs.get(\"filename\") if args and not filename: filename = args[0] need_export_index = False if filename: if not os.path.exists(filename+\"_idx\"): need_export_index = True os.makedirs(filename+\"_idx\") if not os.path.exists(filename+\"_idx/igzip.pickle\"): need_export_index = True else: kwargs['index_file'] = kwargs.pop('index_file', filename+\"_idx/igzip.pickle\") if 'file_size' in kwargs: file_size = self.file_size = kwargs.pop('file_size', None) need_export_index = False self.line2seekpoint = kwargs.pop('line2seekpoint', None) if need_export_index and 'auto_build' not in kwargs: kwargs['auto_build'] = True super(GzipByLineIdx, self).__init__(*args, **kwargs) if not hasattr(self, 'file_size'): self.build_full_index() pos = self.tell() self.seek(0, os.SEEK_END) self.file_size = file_size = self.tell() if self.line2seekpoint is None: def reader(fobj, rng, max_rng, ret): fobj.seek(rng,0) pos = fobj.tell() while rng < max_rng: fobj.readline() pos = fobj.tell() if pos < max_rng: ret.append(pos) else: break rng = pos workers=[] line_nums = [] for rng in range(0, file_size, 10000000): max_rng = min(rng + 10000000, file_size) line_nums.append([]) worker = threading.Thread(target=reader, args=(copy.copy(self), rng, max_rng, line_nums[-1])) workers.append(worker) worker.start() for worker in workers: worker.join() self.line2seekpoint = [0]+list(itertools.chain(*line_nums)) if filename and need_export_index: self.export_index(filename+\"_idx/igzip.pickle\") def __reduce__(self): \"\"\"Used to pickle an ``GzipByLineIdx``. Returns a tuple containing: - a reference to the ``unpickle`` function - a tuple containing a \"state\" object, which can be passed to ``unpickle``. \"\"\" fobj = self._IndexedGzipFile__igz_fobj if (not fobj.drop_handles) or (not fobj.own_file): raise pickle.PicklingError( 'Cannot pickle GzipByLineIdx that has been created ' 'with an open file object, or that has been created ' 'with drop_handles=False') # export and serialise the index if # any index points have been created. # The index data is serialised as a # bytes object. if fobj.npoints == 0: index = None else: index = io.BytesIO() self.export_index(fileobj=index) index = index.getvalue() state = { 'filename' : fobj.filename, 'auto_build' : fobj.auto_build, 'spacing' : fobj.spacing, 'window_size' : fobj.window_size, 'readbuf_size' : fobj.readbuf_size, 'readall_buf_size' : fobj.readall_buf_size, 'buffer_size' : self._IndexedGzipFile__buffer_size, 'line2seekpoint' : self.line2seekpoint, 'file_size' : self.file_size, 'tell' : self.tell(), 'index' : index} return (_unpickle_gzip_by_line, (state, )) #TODO: refactor to do def __iter__(self): len_self = len(self) for start in range(0, len_self, 1000): end = min(len_self, start+1000) orig_start = start orig_end = end while start < len_self and self.line2seekpoint[start] == -1: start+=1 while end >= 0 and self.line2seekpoint[end] == -1: end-=1 start = self.line2seekpoint[start] if end == len_self: seek_points = self.line2seekpoint[orig_start:] end = self.file_size else: seek_points = self.line2seekpoint[orig_start:orig_end] end= self.line2seekpoint[end]-1 ret = [] with self._IndexedGzipFile__file_lock: pos = self.tell() self.seek(start, 0) ret= self.read(end-start).split(b'\\n') self.seek(pos, 0) #if a seekpoint is -1, this means the data has been deleted - either not in the file at all or #blanked out for line in ret: while seek_points and seek_points[0] == -1 and line.strip(): yield \"\" seek_points.pop() if not seek_points: break if seek_points[0] == -1 and line.strip(): yield \"\" else: yield line seek_points.pop() while seek_points and seek_points[0] == -1: yield \"\" seek_points.pop() def __len__(self): return len(self.line2seekpoint) def __getitem__(self, keys): start, end = None, None if isinstance(keys, int): contiguous = False else: contiguous, start, end = _is_contiguous(keys) if isinstance(keys, slice): contiguous = True start = 0 if keys.start is None else keys.start end = len(self) if keys.stop is None else keys.stop if contiguous: start = self.line2seekpoint[start] if end >= len(self.line2seekpoint): end = self.file_size else: end= self.line2seekpoint[end+1]-1 with self._IndexedGzipFile__file_lock: pos = self.tell() self.seek(start, 0) ret= self.read(end-start).split(b'\\n') self.seek(pos, 0) return ret elif isinstance(keys, int): start = self.line2seekpoint[keys] if start < 0: return b\"\" with self._IndexedGzipFile__file_lock: pos = self.tell() self.seek(start, 0) ret= self.readline() self.seek(pos, 0) return ret else: return [self[idx] for idx in keys] @staticmethod def open(filename): if os.path.exists(filename+\"_idx/index.pickle\"): return GzipByLineIdx(filename, index_file=filename+\"_idx/index.pickle\") else: return GzipByLineIdx(filename)","title":"GzipByLineIdx"},{"location":"reference/utils/#utils.GzipByLineIdx.__init__","text":"Create an LineIndexGzipFileExt . The file may be specified either with an open file handle ( fileobj ), or with a filename . If the former, the file must have been opened in 'rb' mode. .. note:: The auto_build behaviour only takes place on calls to :meth: seek . :arg filename: File name or open file handle. :arg fileobj: Open file handle. :arg mode: Opening mode. Must be either 'r' or 'rb . :arg auto_build: If True (the default), the index is automatically built on calls to :meth: seek . :arg skip_crc_check: Defaults to False . If True , CRC/size validation of the uncompressed data is not performed. :arg spacing: Number of bytes between index seek points. :arg window_size: Number of bytes of uncompressed data stored with each seek point. :arg readbuf_size: Size of buffer in bytes for storing compressed data read in from the file. :arg readall_buf_size: Size of buffer in bytes used by :meth: read when reading until EOF. :arg drop_handles: Has no effect if an open fid is specified, rather than a filename . If True (the default), a handle to the file is opened and closed on every access. Otherwise the file is opened at __cinit__ , and kept open until this _IndexedGzipFile is destroyed. :arg index_file: Pre-generated index for this gz file - if provided, passed through to :meth: import_index . :arg buffer_size: Optional, must be passed as a keyword argument. Passed through to io.BufferedReader.__init__ . If not provided, a default value of 1048576 is used. :arg line2seekpoint: Optional, must be passed as a keyword argument. If not passed, this will automatically be created. Source code in src/utils.py def __init__(self, *args, **kwargs): \"\"\"Create an ``LineIndexGzipFileExt``. The file may be specified either with an open file handle (``fileobj``), or with a ``filename``. If the former, the file must have been opened in ``'rb'`` mode. .. note:: The ``auto_build`` behaviour only takes place on calls to :meth:`seek`. :arg filename: File name or open file handle. :arg fileobj: Open file handle. :arg mode: Opening mode. Must be either ``'r'`` or ``'rb``. :arg auto_build: If ``True`` (the default), the index is automatically built on calls to :meth:`seek`. :arg skip_crc_check: Defaults to ``False``. If ``True``, CRC/size validation of the uncompressed data is not performed. :arg spacing: Number of bytes between index seek points. :arg window_size: Number of bytes of uncompressed data stored with each seek point. :arg readbuf_size: Size of buffer in bytes for storing compressed data read in from the file. :arg readall_buf_size: Size of buffer in bytes used by :meth:`read` when reading until EOF. :arg drop_handles: Has no effect if an open ``fid`` is specified, rather than a ``filename``. If ``True`` (the default), a handle to the file is opened and closed on every access. Otherwise the file is opened at ``__cinit__``, and kept open until this ``_IndexedGzipFile`` is destroyed. :arg index_file: Pre-generated index for this ``gz`` file - if provided, passed through to :meth:`import_index`. :arg buffer_size: Optional, must be passed as a keyword argument. Passed through to ``io.BufferedReader.__init__``. If not provided, a default value of 1048576 is used. :arg line2seekpoint: Optional, must be passed as a keyword argument. If not passed, this will automatically be created. \"\"\" filename = kwargs.get(\"filename\") if args and not filename: filename = args[0] need_export_index = False if filename: if not os.path.exists(filename+\"_idx\"): need_export_index = True os.makedirs(filename+\"_idx\") if not os.path.exists(filename+\"_idx/igzip.pickle\"): need_export_index = True else: kwargs['index_file'] = kwargs.pop('index_file', filename+\"_idx/igzip.pickle\") if 'file_size' in kwargs: file_size = self.file_size = kwargs.pop('file_size', None) need_export_index = False self.line2seekpoint = kwargs.pop('line2seekpoint', None) if need_export_index and 'auto_build' not in kwargs: kwargs['auto_build'] = True super(GzipByLineIdx, self).__init__(*args, **kwargs) if not hasattr(self, 'file_size'): self.build_full_index() pos = self.tell() self.seek(0, os.SEEK_END) self.file_size = file_size = self.tell() if self.line2seekpoint is None: def reader(fobj, rng, max_rng, ret): fobj.seek(rng,0) pos = fobj.tell() while rng < max_rng: fobj.readline() pos = fobj.tell() if pos < max_rng: ret.append(pos) else: break rng = pos workers=[] line_nums = [] for rng in range(0, file_size, 10000000): max_rng = min(rng + 10000000, file_size) line_nums.append([]) worker = threading.Thread(target=reader, args=(copy.copy(self), rng, max_rng, line_nums[-1])) workers.append(worker) worker.start() for worker in workers: worker.join() self.line2seekpoint = [0]+list(itertools.chain(*line_nums)) if filename and need_export_index: self.export_index(filename+\"_idx/igzip.pickle\")","title":"__init__()"},{"location":"reference/utils/#utils.GzipByLineIdx.__reduce__","text":"Used to pickle an GzipByLineIdx . Returns a tuple containing a reference to the unpickle function a tuple containing a \"state\" object, which can be passed to unpickle . Source code in src/utils.py def __reduce__(self): \"\"\"Used to pickle an ``GzipByLineIdx``. Returns a tuple containing: - a reference to the ``unpickle`` function - a tuple containing a \"state\" object, which can be passed to ``unpickle``. \"\"\" fobj = self._IndexedGzipFile__igz_fobj if (not fobj.drop_handles) or (not fobj.own_file): raise pickle.PicklingError( 'Cannot pickle GzipByLineIdx that has been created ' 'with an open file object, or that has been created ' 'with drop_handles=False') # export and serialise the index if # any index points have been created. # The index data is serialised as a # bytes object. if fobj.npoints == 0: index = None else: index = io.BytesIO() self.export_index(fileobj=index) index = index.getvalue() state = { 'filename' : fobj.filename, 'auto_build' : fobj.auto_build, 'spacing' : fobj.spacing, 'window_size' : fobj.window_size, 'readbuf_size' : fobj.readbuf_size, 'readall_buf_size' : fobj.readall_buf_size, 'buffer_size' : self._IndexedGzipFile__buffer_size, 'line2seekpoint' : self.line2seekpoint, 'file_size' : self.file_size, 'tell' : self.tell(), 'index' : index} return (_unpickle_gzip_by_line, (state, ))","title":"__reduce__()"},{"location":"reference/utils/#utils.create_hiearchical_clusters","text":"Incremental hiearchical clustering from embeddings stored in a mmap file. Can also used to create an index for searching. with a max of 4 levels, and each node containing 200 items, we can have up to 1.6B items approximately span2cluster_label maps a child span to a parent span. spans can be of the form int|(int,int). leaf nodes are ints. non-leaf nodes are (int,int) tuples clusters maps cluster_label => list of spans when we use the term 'idx', we normally refer to the index in an embedding file. :arg clusters: the dict mapping parent span to list of child span :arg span2cluster_label: the inverse of the above. :arg mmap_file: the name of the mmap file. :arg mmap_len: the current length of the mmap file. :arg embed_dim: the dimension of an embedding. :arg dtype: the numpy dtype. :arg skip_idxs: Optioal. the idx into the embeddings that will not be clustered or searched for. :arg idxs: Optioal. if provided, the particular embedding idx that will be clustered in this call. :arg max_level: the maximum level of the cluster hiearchy. :arg max_cluster_size:the maximum size of any particular cluster. :arg min_overlap_merge_cluster. When incremental clustering, the minimum overlap between one cluster and another before merging them. :arg kmeans_batch_size: the size of each batch of embeddings that are kmean batched. :arg use_tqdm: whether to report the progress of the clustering. :arg grouping_fn: Optional. a function that takes in a grouping_fn_callback_data, embeddings, and a list of spans, will return a hash of form {'group_X': [...], 'group_Y': [...], etc.} :arg grouping_fn_callback_data: Optional. arbitrary data to pass to the grouping_fn Source code in src/utils.py def create_hiearchical_clusters(clusters, span2cluster_label, mmap_file, mmap_len=0, embed_dim=25, dtype=np.float16, skip_idxs=None, idxs=None, max_level=4, \\ max_cluster_size=200, min_overlap_merge_cluster=2, prefered_leaf_node_size=None, kmeans_batch_size=250000, \\ recluster_start_iter=0.85, max_decluster_iter=0.95, use_tqdm=True, grouping_fn=None, grouping_fn_callback_data=None): \"\"\" Incremental hiearchical clustering from embeddings stored in a mmap file. Can also used to create an index for searching. with a max of 4 levels, and each node containing 200 items, we can have up to 1.6B items approximately span2cluster_label maps a child span to a parent span. spans can be of the form int|(int,int). leaf nodes are ints. non-leaf nodes are (int,int) tuples clusters maps cluster_label => list of spans when we use the term 'idx', we normally refer to the index in an embedding file. :arg clusters: the dict mapping parent span to list of child span :arg span2cluster_label: the inverse of the above. :arg mmap_file: the name of the mmap file. :arg mmap_len: the current length of the mmap file. :arg embed_dim: the dimension of an embedding. :arg dtype: the numpy dtype. :arg skip_idxs: Optioal. the idx into the embeddings that will not be clustered or searched for. :arg idxs: Optioal. if provided, the particular embedding idx that will be clustered in this call. :arg max_level: the maximum level of the cluster hiearchy. :arg max_cluster_size:the maximum size of any particular cluster. :arg min_overlap_merge_cluster. When incremental clustering, the minimum overlap between one cluster and another before merging them. :arg kmeans_batch_size: the size of each batch of embeddings that are kmean batched. :arg use_tqdm: whether to report the progress of the clustering. :arg grouping_fn: Optional. a function that takes in a grouping_fn_callback_data, embeddings, and a list of spans, will return a hash of form {'group_X': [...], 'group_Y': [...], etc.} :arg grouping_fn_callback_data: Optional. arbitrary data to pass to the grouping_fn \"\"\" global device if skip_idxs is None: skip_idxs = set() else: skip_idxs = set(skip_idxs) if clusters is None: clusters = {} if span2cluster_label is None: span2cluster_label = {} for label, a_cluster in clusters: for span in a_cluster: span2cluster_label[span] = label else: #make sure clusters have the same data as span2cluster_label for span, label in span2cluster_label.items(): if span not in clusters.get(label,[]): clusters[label] = clusters.get(label,[]) + [span] #we are not going to cluster idxs that should be skipped if idxs: idxs = [idx for idx in idxs if idx not in skip_idxs] #remove some idx from the clusters so we can re-compute the clusters remove_idxs = list(skip_idxs) + ([] if idxs is None else idxs) if remove_idxs is not None: need_recompute_clusters=False for idx in remove_idxs: label = span2cluster_label.get(idx) if label is not None: clusters[label].remove(idx) a_cluster = clusters[label] del span2cluster_label[idx] need_recompute_clusters=True # now re-create the label if the idx is the proto index. if idx == label[1]: new_idx = a_cluster[0] for level in range(0, max_level): new_label2 = (level, new_idx) old_label2 = (level, idx) if old_label2 in span2cluster_label: #rename the label for the children clusters[new_label2] = clusters[old_label2] del clusters[old_label2] for span in clusters[new_label2]: span2cluster_label[span] = new_label2 #make the parent refer to the new label parent_label = span2cluster_label[old_label2] clusters[parent_label].remove(old_label2) clusters[parent_label].append(new_label2) span2cluster_label[new_label2] = parent_label #belt and suspenders, let's just recreate the clusters if need_recompute_clusters: clusters.clear() for span, label in span2cluster_label.items(): clusters[label] = clusters.get(label, []) + [span] #print (mmap_len, clusters, span2cluster_label) if prefered_leaf_node_size is None: prefered_leaf_node_size = max_cluster_size cluster_embeddings = np_memmap(mmap_file, shape=[mmap_len, embed_dim], dtype=dtype) # at level 0, the spans are the indexes themselves, so no need to map using all_spans all_spans = None for level in range(max_level): assert level == 0 or (all_spans is not None and idxs is not None) #print (\"got here\") if idxs is None: len_spans = mmap_len else: len_spans = len(idxs) # we are going to do a minimum of 6 times in case there are not already clustered items # from previous iterations. num_times = max(6,math.ceil(len_spans/int(.7*kmeans_batch_size))) recluster_at = max(0,num_times*recluster_start_iter) rng = 0 if use_tqdm: num_times2 = tqdm.tqdm(range(num_times)) else: num_times2 = range(num_times) for times in num_times2: max_rng = min(len_spans, rng+int(.7*kmeans_batch_size)) #create the next batch to cluster if idxs is None: spans = list(range(rng, max_rng)) not_already_clustered = [idx for idx in range(rng) if (all_spans is not None and all_spans[idx] not in span2cluster_label) or \\ (all_spans is None and idx not in span2cluster_label)] else: spans = idxs[rng: max_rng] not_already_clustered = [idx for idx in range(rng) if idxs[:rng] if (all_spans is not None and all_spans[idx] not in span2cluster_label) or \\ (all_spans is None and idx not in span2cluster_label)] num_itmes_left = kmeans_batch_size - len(spans) if len(not_already_clustered) > int(.5*num_itmes_left): spans.extend(random.sample(not_already_clustered, int(.5*num_itmes_left))) else: spans.extend(not_already_clustered) if len(spans) == 0: continue if level == 0: already_clustered = [idx for idx in range(mmap_len) if idx in span2cluster_label] else: already_clustered = [idx for idx, span in enumerate(all_spans) if span in span2cluster_label] if len(already_clustered) > int(.5*num_itmes_left): spans.extend(random.sample(already_clustered, int(.5*num_itmes_left))) else: spans.extend(already_clustered) # get the embedding indexs for the cluster if level == 0: spans = [span for span in spans if span not in skip_idxs] embedding_idxs = spans else: spans = [all_spans[idx] for idx in spans] spans = [span for span in spans if span[1] not in skip_idxs] embedding_idxs = [span[1] for span in spans] #print (spans) #do kmeans clustering in batches with the embedding indexes if level == 0: true_k = int(len(embedding_idxs)/prefered_leaf_node_size) else: true_k = int(len(embedding_idxs)/max_cluster_size) _cluster_one_batch(true_k, spans, embedding_idxs, clusters, span2cluster_label, level, cluster_embeddings, min_overlap_merge_cluster, grouping_fn, grouping_fn_callback_data) # re-cluster any small clusters or break up large clusters if times >= recluster_at: need_recompute_clusters = False for parent, spans in list(clusters.items()): if times < max(0, num_times*max_decluster_iter) and \\ ((level == 0 and len(spans) < prefered_leaf_node_size*.5) or (level != 0 and len(spans) < max_cluster_size*.5)): need_recompute_clusters = True for span in spans: del span2cluster_label[span] elif len(spans) > max_cluster_size: need_recompute_clusters = True for token in spans: del span2cluster_label[token] embedding_idxs = [span if type(span) is int else span[1] for span in spans] if level == 0: true_k = int(len(embedding_idxs)/prefered_leaf_node_size) else: true_k = int(len(embedding_idxs)/max_cluster_size) _cluster_one_batch(true_k, spans, embedding_idxs, clusters, span2cluster_label, level, cluster_embeddings, min_overlap_merge_cluster, grouping_fn, grouping_fn_callback_data) if need_recompute_clusters: clusters.clear() for span, label in span2cluster_label.items(): clusters[label] = clusters.get(label, []) + [span] rng = max_rng # prepare data for next level clustering all_spans = [label for label in clusters.keys() if label[0] == level] if len(all_spans) < max_cluster_size: break idxs = [idx for idx, label in enumerate(all_spans) if label not in span2cluster_label] return clusters, span2cluster_label","title":"create_hiearchical_clusters()"}]}